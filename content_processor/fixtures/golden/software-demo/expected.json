{
  "modules": [
    {
      "slug": "demo",
      "title": "Introduction to AI Safety",
      "contentId": "f97b4ad0-7fcf-408c-835a-a0aca0be9e9b",
      "sections": [
        {
          "type": "page",
          "contentId": "d1e2f3a4-b5c6-7890-d1e2-f3a4b5c67890",
          "meta": {
            "title": "Welcome to AI Safety"
          },
          "segments": [
            {
              "type": "text",
              "content": "We begin by examining the potential of AI and the risks and opportunities\nthat the characteristics of this technology present to humanity."
            }
          ]
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Speculations concerning the first ultraintelligent machine",
            "author": "Irving John Good",
            "sourceUrl": "https://flyingpenguin.com/wp-content/uploads/2022/04/good-1964-.pdf"
          },
          "segments": [
            {
              "type": "text",
              "content": "The issue with self amplifying loops is that plans become obsolete very quickly. The system a few steps down the line will be dominated by effects that are almost impossible to foresee from the starting line. I.J. Good recognised this when he wrote the following:"
            },
            {
              "type": "article-excerpt",
              "content": "The survival of man depends on the early construction of an ultra-intelligent machine. In order to design an ultraintelligent machine we need to understand more about the human brain or human thought or both. In the following pages an attempt is made to take more of the magic out of the brain by means of a \"subassembly\" theory, which is a modification of Hebb\u2019s famous speculative cell-assembly theory. My belief is that the first ultraintelligent machine is most likely to incorporate vast artificial neural circuitry, and that its behavior will be partly explicable in terms of the subassembly theory. Later machines will all be designed by ultraintelligent machines, and who am I to guess what principles they will devise? But probably Man will construct the deus ex machina in his own image.",
              "collapsed_before": "## 1. Introduction",
              "collapsed_after": null
            },
            {
              "type": "article-excerpt",
              "content": "Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultra- intelligent machine could design even better machines; there would then unquestionably be an \"intelligence explosion,\" and the intelligence of man would be left far behind (see for example refs. [22], [34], [44]). Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously. In one science fiction story a machine refused to design a better one since it did not wish to be put out of a job. This would not be an insuperable difficulty, even if machines can be egotistical, since the machine could gradually improve itself out of all recognition, by acquiring new equipment. B. V. Bowden stated on British television (August 1962) that there is no point in building a machine with the intelligence of a man, since it is easier to construct human brains by the usual method. A similar point was made by a speaker during the meetings reported in a recent IEEE publication [1], but I do not know whether this point appeared in the published report. This shows that highly intelligent people can overlook the \"intelligence explosion.\" It is true that it would be uneconomical to build a machine capable only of ordinary intellectual attainments, but it seems fairly probable that if this could be done then, at double the cost, the machine could exhibit ultraintelligence. Since we are concerned with the economical construction of an ultra-intelligent machine, it is necessary to consider first what such a machine would be worth. Carter [11] estimated the value, to the world, of J. M. Keynes, as at least 100,000 million pounds sterling. By definition, an ultraintelligent machine is worth far more, although the sign is uncertain, but since it will give the human race a good chance of surviving indefinitely, it might not be extravagant to put the value at a megakeynes.",
              "collapsed_before": "The subassembly theory sheds light on the physical embodiment of memory and meaning, and there can be little doubt that both will need embodiment in an ultraintelligent machine. Even for the brain, we shall argue that physical embodiment of meaning must have originated for reasons of economy, at least if the metaphysical reasons can be ignored. Economy is important in any engineering venture, but especially so when the price is exceedingly high, as it most likely will be for the first ultraintelligent machine. Hence semantics is relevant to the design of such a machine. Yet a detailed knowledge of semantics might not be required, since the artificial neural network will largely take care of it, provided that the parameters are correctly chosen, and provided that the network is adequately integrated with its sensorium and motorium (input and output). For, if these conditions are met, the machine will be able to learn from experience, by means of positive and negative reinforcement, and the instruction of the machine will resemble that of a child. Hence it will be useful if the instructor knows something about semantics, but not necessarily more useful than for the instructor of a child. The correct choice of the parameters, and even of the design philosophy, will depend on the usual scientific method of successive approximation, using speculation, theory, and experiment. The percentage of speculation needs to be highest in the early stages of any endeavor. Therefore no apology is offered for the speculative nature of the present work. For we are certainly still in the early stages in the design of an ultraintelligent machine. In order that the arguments should be reasonably self-contained, it is necessary to discuss a variety of topics. We shall define an ultra-intelligent machine, and, since its cost will be very large, briefly consider its potential value. We say something about the physical embodiment of a word or statement, and defend the idea that the function of meaning is economy by describing it as a process of \"regeneration.\" In order to explain what this means, we devote a few pages to the nature of communication. (The brain is of course a complex communication and control system.) We shall need to discuss the process of recall, partly because its understanding is very closely related to the understanding of understanding. The process of recall in its turn is a special case of statistical information retrieval. This subject will be discussed in Section 5. One of the main difficulties in this subject is how to estimate the probabilities of events that have never occurred. That such probabilities are relevant to intelligence is to be expected, since intelligence is sometimes defined as the ability to adapt to new circumstances. The difficulty of estimating probabilities is sometimes overlooked in the literature of artificial intelligence, but this article would be too long if the subject were surveyed here. A separate monograph has been written on this subject [48]. Some of the ideas of Section 5 are adapted, in Section 6, to the problem of recall, which is discussed and to some extent explained in terms of the subassembly theory. The paper concludes with some brief suggestions concerning the physical representation of \"meaning.\" This paper will, as we said, be speculative: no blueprint will be suggested for the construction of an ultraintelligent machine, and there will be no reference to transistors, diodes, and cryogenics. (Note, however, that cryogenics have the important merit of low power consumption. This feature will be valuable in an ultraintelligent machine.) One of our aims is to pinpoint some of the difficulties. The machine will not be on the drawing board until many people have talked big, and others have built small, conceivably using deoxyribonucleic acid (DNA). Throughout the paper there are suggestions for new research. Some further summarizing remarks are to be found in the Conclusions.\n## 2. Ultraintelligent Machines and Their Value",
              "collapsed_after": "There is the opposite possibility, that the human race will become redundant, and there are other ethical problems, such as whether a machine could feel pain especially if it contains chemical artificial neurons, and whether an ultraintelligent machine should be dismantled when it becomes obsolete [43, 84]. The machines will create social problems, but they might also be able to solve them in addition to those that have been created by microbes and men. Such machines will be feared and respected, and perhaps even loved. These remarks might appear fanciful to some readers, but to the writer they seem very real and urgent, and worthy of emphasis outside of science fiction. If we could raise say a hundred billion dollars we might be able to simulate all the neurons of a brain, and of a whole man, at a cost of ten dollars per artificial neuron. But it seems unlikely that more than say a millikeynes would actually be forthcoming, and even this amount might be difficult to obtain without first building the machine! It would be justified if, with this expenditure, the chance of success were about 108.\n\nUntil an ultraintelligent machine is built perhaps the best intellectual feats will be performed by men and machines in very close, sometimes called \"symbiotic,\" relationship, although the term \"biomechanical\" would be more appropriate. As M. H. A. Newman said in a private communication in 1946, an electronic computer might be used as \"rough paper\" by a mathematician. It could already be used in this manner by a chess player quite effectively, although the effectiveness would be much increased if the chess-playing programs were written with extremely close man-machine interaction in mind from the start. The reason for this effectiveness is that the machine has the advantage in speed and accuracy for routine calculation, and man has the advantage in imagination. Moreover, a large part of imagination in chess can be reduced to routine. Many of the ideas that require imagination in the amateur are routine for the master. Consequently the machine might appear imaginative to many observers and even to the programmer. Similar comments apply to other thought processes. The justification for chess-playing programs is that they shed light on the problem of artificial intelligence without being too difficult to write. Their interest would be increased if chess were replaced by so-called \"randomized chess,\" in which the positions of the white pieces on the first rank are permuted at random before the game begins (but with the two bishops on squares of opposite colors), and then the initial positions of the black pieces are determined by mirror symmetry. This gives rise to 1440 essentially distinct initial positions and effectively removes from the game the effect of mere parrot learning of the openings, while not changing any of the general principles of chess. In ordinary chess the machine would sometimes beat an international Grandmaster merely by means of a stored opening trap, and this would be a hollow victory. Furthermore a program for randomized chess would have the advantage that it would not be necessary to store a great number of opening variations on magnetic tape. The feats performed by very close man-machine interaction by say 1980 are likely to encourage the donation of large grants for further development. By that time, there will have been great advances in microminiaturization, and pulse repetition frequencies of one billion pulses per second will surely have been attained in large computers (for example see Shoulders [91]). On the other hand, the cerebral cortex of a man has about five billion neurons, each with between about twenty and eighty dendrites ([90], pp. 35 and 51), and thousands of synapses. (At the recent IEEE meetings, P. Mueller offered the estimate 300,000 orally. It would be very interesting to know the corresponding figure for the brain of a whale, which, according to Tower [99], has about three times as many neurons as a human brain. Perhaps some whales are ultraintelligent! [49].) Moreover, the brain is a parallel-working device to an extent out of all proportion to any existing computer. Although computers are likely to attain a pulse repetition speed advantage of say a million over the brain, it seems fairly probable, on the basis of this quantitative argument, that an ultraintelligent machine will need to be ultraparallel. In order to achieve the requisite degree of ultraparallel working it might be useful for many of the elements of the machine to contain a very short-range microminiature radio transmitter and receiver. The range should be small compared with the dimensions of the whole machine. A \"connection\" between two close artificial neurons could be made by having their transmitter and receiver on the same or close frequencies. The strength of the connection could be represented by the accuracy of the tuning. The receivers would need numerous filters so as to be capable of receiving on many different frequencies. \"Positive reinforcement\" would correspond to improved tuning of these filters. It cannot be regarded as entirely certain that an ultraintelligent machine would need to be ultraparallel, since the number of binary operations per second performed by the brain might be far greater than is necessary for a computer made of reliable components. Neurons are not fully reliable; for example, they do not all last a lifetime; yet the brain is extremely efficient. This efficiency must depend partly on \u2018\u2018redundancy\u2019\u2019 in the sense in which the term is used in information theory. A machine made of reliable components would have an advantage, and it seems just possible that ultraparallel working will not be essential. But there is a great waste in having only a small proportion of the components of a machine active at any one time. Whether a machine of classical or ultraparallel design is to be the first ultraintelligent machine, it will need to be able to handle or to learn to handle ordinary language with great facility. This will be important in order that its instructor should be able to teach it rapidly, and so that later the machine will be able to teach the instructor rapidly. It is very possible also that natural languages, or something analogous to them rather than to formal logic, are an essential ingredient of scientific imagination. Also the machine will be called upon to translate languages, and perhaps to generate fine prose and poetry at high speed, so that, all in all, linguistic facility is at a high premium. A man cannot learn more than ten million statements in a lifetime. A machine could already store this amount of information without much difficulty, even if it were not ultraparallel, but it seems likely that it would need to be ultraparallel in order to be able to retrieve the information with facility. It is in recall rather than in retention that the ordinary human memory reveals its near magic. The greatest mental achievements depend on more than memory, but it would be a big step toward ultraintelligence if human methods of recall could be simulated.\n\nFor the above reasons, it will be assumed here that the first ultra-intelligent machine will be ultraparallel, perhaps by making use of radio, as suggested. For definiteness, the machine will be assumed to incorporate an artificial neural net. This might be in exceedingly close relationship with an ordinary electronic computer, the latter being used for the more formalizable operations [33]. In any event the ultra-intelligent machine might as well have a large electronic computer at its beck and call, and also a multimillion dollar information-retrieval installation of large capacity but of comparatively slow speed, since these would add little to the total cost. It is unlikely that facility in the use of language will be possible if semantic questions are ignored in the design. When we have read or listened to some exposition we sometimes remember for a long time what it meant, but seldom how it was worded. It will be argued below that, for men, meaning serves a function of economy in long-term retention and in information handling, and this is the basis for our contention that semantics are relevant to the design of an ultraintelligent machine. Since language is an example of communication, and since an ultra-intelligent machine will be largely a complicated communication system, we shall briefly consider the nature of communication. It will be argued that in communication a process of \"generalized regeneration\" always occurs, and that it serves a function of economy. It will also be argued that the meanings of statements are examples of generalized regeneration.\n## 3. Communication as Regeneration1\nIn a communication system, a source, usually a time series denoted here by S(t) or S for short, undergoes a sequence of transformations. The first transformation is often a deterministic encoding, which transforms the source into a new time series, T0S(t). This is noisily (indeterministically) transmitted, i.e., it undergoes a transformation T1 which is a random member of some class of transformations. If the possible sources are, in some sense, far enough apart, and if the noise is not too great, then the waveforms T1T0S will, also in some sense, tend to form clumps, and it will be possible with high probability to reconstruct the encoded sources at the receiving end of the channel. This reconstruction is called here (generalized) regeneration, a term that is most familiar in connection with the reshaping of square pulses. When dealing with groups of consecutive pulses, the term error correction is more usual, especially when it is assumed that the individual pulses have themselves been first regenerated. Another way of saying that the source signals must be far enough apart is to say that they must have enough redundancy. In a complicated network, it is often convenient to regard signals as sources at numerous places in the network and not merely at the input to the network. The redundancy might then be represented, for example, by mere serial or parallel repetition. 1 For a short survey of the nature of communication, see for example Pierce [80a]. A compromise between pure regeneration and the use of the whole garbled waveform T1T0S(t)is probabilistic regeneration, in which the garbled waveform is replaced by the set of probabilities that it has arisen from various sources [42]. In probabilistic regeneration less information is thrown away than in pure regeneration, and the later data-handling costs more, but less than it would cost if there were no regeneration at all. The hierarchical use of probabilistic regeneration would add much flexibility to complicated communication networks. An example of generalized and hierarchical regeneration is in the use of words in a language. A word in a spoken language could well be defined as a clump of short time series, that is, a class of time series having various properties in common. (The class depends on the speaker, the listener, and the context; and membership of the class is probabilistic since there are marginal cases.) If any sound (acoustic time series) belonging to the clump is heard, then the listener mentally regenerates the sound and replaces it by some representation of the word. He will tend to remember, or to write down, the word and not its precise sound,\n\nalthough if any other significant property of the sound is noticed it might also be remembered. The advantage of remembering the word rather than the precise sound is that there is then less to remember and a smaller amount of information handling to do. This process of regeneration occurs to some extent at each of the levels of phonemes, words, sentences, and longer linguistic stretches, and even at the semantic level, and wherever it occurs it serves a function of economy. But the economy is not parsimonious: \"redundancy\" often remains in the coding in order that the encoded message should continue to have useful error-correcting features. The redundancy often decreases with the passage of time, perhaps leading eventually to the extinction of a memory. That communication theory has a bearing on the philosophy of meaning has been suggested before (see for example, Weaver [89], pp. 114\u2013117, and Lord Brain [8]). Note also the definition of the amount of subjective information in a proposition, as \u2013 log2 p where p is the initial subjective probability that the proposition is true ([21], p. 75). This could also be described as subjective semantic information: when the probabilities are credibilities (logical probabilities) we obtain what might be called objective semantic information [5, 10], the existence of which is, in my opinion, slightly more controversial. That subjective probability is just as basic as communication theory to problems of meaning and recognition, if not more so, is a necessary tenet for any of us who define reasoning as logic plus probability ([21], pp. 3 and 88; see also Colin Cherry [12], pp. 200 and 274, Woodward [105], and Tompkins [98]). The implication is that both the initial (prior) probabilities and the likelihoods or \"weights of evidence\" [21] should be taken into account in every practical inference by a rational man, and in fact nearly always are taken into account to some extent, at least implicitly, even by actual men. (In case this thesis should appear as obvious to some readers as it does to the writer, it should be mentioned that in 1950 very few statisticians appeared to accept the thesis; and even now they are in a minority.) There is conclusive experimental evidence that the recognition of words depends on the initial probabilities [94]: a well-known method of deception when trying to sell a speech-synthesis system is to tell the listeners in advance what will be said on it, and thereby to make it easily intelligible when it is repeated. There is a similar effect in the perception of color [9]. The rational procedure in perception would be to estimate the final (a posteriori) probabilities by means of Bayes\u2019 theorem, and then perhaps to select one or more hypotheses for future consideration or action, by allowing also for the utilities. (Compare refs. [24], [12], p. 206, and Middleton [68].) In fact the \"principle of rationality\" has been defined as the recommendation to maximize expected utility. But it is necessary to allow also for the expected cost of information handling including theorizing [23, 40], and this is why regeneration and probabilistic regeneration are useful. We pointed out above that the organization of regeneration is often hierarchical, but it is not purely so. For example, we often delay the regeneration of a phoneme until the word to which the phoneme belongs has been regenerated with the help of the surrounding context. Likewise if a machine is to be able to \"understand\" ordinary spoken language in any reasonable sense, it seems certain that its corresponding regeneration structure must not be purely hierarchical unless it is also probabilistic. For each process of nought-one or pure regeneration (each definite \"decision\") loses information, and the total loss would certainly be too great unless the speech were enunciated with priggish accuracy. The probabilistic regeneration structure that will be required will be much more complex than a \"pure\" regeneration structure. (Historical note: the hierarchical structure of mental processes was emphasized by Gall [20], McDougall [66], and many times since\u2013see for example MacKay [63], Hayek [53], and others [30], [34], [87], [41].) It seems reasonable to the present writer that probabilistic regeneration will, for most purposes, lose only a small amount of information, and that, rather than to use anything more elaborate, it is likely to be better to compromise between pure and probabilistic regeneration for most purposes. The applications of regeneration in the present paper will be to assemblies, subassemblies, and meaning.\n\nWhen a person recalls a proposition he could be said to regenerate its meaning; when he understands a statement made by another person the term \"transgeneration\" would be more appropriate; and when he thinks of a new proposition, the process would be better called \"generation,\" but we shall use the word \"regeneration\" to cover all three processes. For example, when listening to speech, the production of meaning can be regarded as the last regeneration stage in the hierarchy mentioned before, and it performs a function of economy just as all the other stages do. It is possible that this has been frequently overlooked because meaning is associated with the metaphysical nature of consciousness, and one does not readily associate metaphysics with questions of economy. Perhaps there is nothing more important than metaphysics, but, for the construction of an artificial intelligence, it will be necessary to represent meaning in some physical form.\n## 4. Some Representations of \"Meaning\" and Their Relevance to Intelligent\nMachines Semantics is not relevant to all problems of mechanical language processing. Up to a point, mechanical translation can be performed by formal processes, such as dictionary look-up and some parsing. Many lexical ambiguities can be resolved statistically in terms of the context, and some as a consequence of the parsing. Sometimes one can go further by using an iterative process, in which the lexical ambiguities are resolved by the parsing, and the parsing in its turn requires the resolution of lexical ambiguities. But even with this iterative process it seems likely that perfect translation will depend on semantic questions [14, 89]. Even if this is wrong, the design of an ultraintelligent machine will still be very likely to depend on semantics [31, 50]. {Verify Ref is 31} What then is meant by semantics? When we ask for the meaning of a statement we are talking about language, and are using a metalanguage; and when we ask for the meaning of \"meaning\" we are using a metametalanguage, so it is not surprising that the question is difficult to answer. A recent survey chapter was entitled \"The Unsolved Problem of Meaning\" [3]. Here we shall touch on only a few aspects of the problem, some of which were not mentioned in that survey (see also Black [7]). It is interesting to recall the thought-word-thing triangle of Charles Pierce and of Ogden and Richards. (See, for example Cherry [12], p. 110. Max Black ascribed a similar \"triangle\" to the chemist Lavoisier in a recent lecture.) It will help to emphasize the requirement for a physical embodiment of meaning if it is here pointed out that the triangle could be extended to a thought-word-thing-engram tetrahedron, where the fourth vertex represents the physical embodiment of the word in the brain, and will be assumed here usually to be a cell assembly. Given a class of linguistic transformations that transform statements into equivalent statements, it would be plausible to represent the meaning of the statement, or the proposition expressed by the statement, by the class of all equivalent statements. (This would be analogous to a modified form of the Frege-Russell definition of a cardinal integer, for example, \"3\" can be defined as the class of all classes \"similar\" to the class consisting of the words \"Newton,\" \"Gauss,\" and \"Bardot.\") The point of this representation is that it makes reference to linguistic operations alone, and not to the \"outside world.\" It might therefore be appropriate for a reasoning machine that had few robotic properties. Unfortunately, linguistic transformations having a strictly transitive property are rare in languages. There are also other logical difficulties analogous to those in the Frege-Russell definition of a cardinal integer. Moreover, this representation of meaning would be excessively unwieldy for mechanical use. Another possible representation depending on linguistic transformations is a single representative of the class of all equivalent statements. This is analogous to another \"definition\" or, better, \"representation,\" of a cardinal integer (see for example Halmos [51], p. 99). This representation is certainly an improvement on the previous one. If this representation were to be used in the construction of an ultraintelligent machine, it\n\nwould be necessary to invent a language in which each statement could be reduced to a canonical form. Such an achievement would go most of the way to the production of perfect mechanical translation of technical literature, as has often been recognized, and it would also be of fundamental importance for the foundations of intuitive or logical probability ([21], pp. 4 and 48). The design of such a \"canonical language\" would be extremely difficult, perhaps even logically impossible, or perhaps it would require an ultraintelligent machine to do it! For human beings, meaning is concerned with the outside world or with an imaginary world, so that representations of meaning that are not entirely linguistic in content might be more useful for our purpose. The behaviorist regards a statement as a stimulus, and interprets its meaning in terms of the class of its effects (responses) in overt behavior. The realism of this approach was shown when \"Jacobson... made the significant discovery that action potentials arise in muscles simultaneously with the meaning processes with which the activity of the muscle, if overtly carried out, would correspond\" ([3], p. 567). Thus the behavioral interpretation of meaning might be relevant for the understanding of the behavior and education of people and robots, especially androids. But, for the design of ultraintelligent machines, the internal representation of meaning (inside the machine) can hardly be ignored, so that the behavioral interpretation is hardly enough. So far we have been discussing the interpretation and representation of the meaning of a statement, but even the meaning of a word is much less tangible and clear-cut than is sometimes supposed. This fact was emphasized, for example, by the philosopher G. E. Moore. Later John Wisdom (not J. O. Wisdom) emphasized that we call an object a cow if it has enough of the properties of a cow, with perhaps no single property being essential. The need to make this interpretation of meaning more quantitative and probabilistic has been emphasized in various places by the present writer, who has insisted that this \"probabilistic definition\" is of basic important for future elaborate information-retrieval systems [29, 35, 31, 43, 41].\"An object is said to belong to class C (such as the class of cows) if some function f(p1, p2 . . ., pm) is positive, where the p\u2019s are the credibilities (logical probabilities) that the object has qualities Q1, Q2, . . ., Qm.These probabilities depend on further functions related to other qualities, on the whole more elementary, and so on. A certain amount of circulatory is typical. For example, a connected brown patch on the retina is more likely to be caused by the presence of a cow if it has four protuberances that look like biological legs than if it has six; but each protuberance is more likely to be a biological leg if it is connected to something that resembles a cow rather than a table. In view of the circularity in this interpretation of \"definition,\" the stratification in the structure of the cerbral cortex can be only a first approximation to the truth\" ([41], pp. 124\u2013125; see also Hayek [53], p. 70). The slight confusion in this passage, between the definition of a cow and the recognition of one, was deliberate, and especially appropriate in an anthology of partly baked ideas. It can be resolved by drawing the distinction between a logical probability and a subjective probability (see for example [36]), and also the distinction between subjective and objective information that we made in the previous section. If we abandon interpretations of meaning in terms of linguistic transformations, such as dictionary definitions, or, in the case of statements, the two interpretations mentioned before; and if also we do not regard the behavioral interpretations as sufficient, we shall be forced to consider interpretations in terms of internal workings. Since this article is written mainly on the assumption that an ultraintelligent machine will consist largely of an artificial neural net, we need in effect a neurophysiological representation of meaning. The behavioral interpretation will be relevant to the education of the machine, but not so much to its design. It does not require much imagination to appreciate that the probabilistic and iterative interpretation of the definition of a word, as described above, is liable to fit well into models of the central nervous system. It has been difficult for the writer to decide how much neurophysiology should be discussed, and hopefully an appropriate balance is made in what follows between actual neurophysiology and the logic of artificial neural networks. The discussion will be based on the speculative cell-assembly theory of Hebb [54] (see also [53] and [71]), or rather on a modification of it in which \"subassemblies\" are emphasized and a central control is assumed. If the present discussion contains inconsistencies, the present writer should be blamed. (For a very good survey of the relevant literature of neurophysiology and psychology, see Rosenblatt [82], pp. 9\u201378.)\n## 5. Recall and Information Retrieval\nWhatever might be the physical embodiment of meaning, it is certainly closely related to that of long-term recall. Immediate recall is not strongly related to semantics, at any rate for linguistic texts. In fact, experiments show that immediate and exact recall of sequences of up to fifty words is about as good for meaningless texts as it is for meaningful texts, provided that the meaningless ones are at least \"fifth order\" approximations to English, that is to say that the probability of each word, given the previous five, is high [70]. The process of recall is a special case of information retrieval, so that one would expect there to be a strong analogy between the recall of a memory and the retrieval of documents by means of index terms. An immediate recall is analogous to the trivial problem of the retrieval of a document that is already in our hands. The problem of the retrieval of documents that are not immediately to hand is logically a very different matter, and so it is not surprising that the processes of immediate and long-term recall should also differ greatly. The problem of what the physical representation is for immediate recall is of course not trivial, but for the moment we wish to discuss long-term recall since it is more related to the subject of semantics. The usual method for attacking the problem of document retrieval, when there are many documents (say several thousand), is to index each document by means of several index terms. We imagine a library customer, in need of some information, to list some index terms without assuming that he uses any syntax, at least for the present. In a simple retrieval system, the customer\u2019s index terms can be used to extract documents by means of a sort, as of punched cards. The process can be made more useful, not allowing for the work in its implementation, if the terms of the documents, and also those of the customer, are given various weights, serving in some degree the function of probabilities. We then have a weighted or statistical system of information retrieval. One could conceive of a more complicated information-retrieval system in which each document bad associated with it a set of resonating filters forming a circuit C. All documents would be queried in parallel: the \"is-there-a-doctor-in-the-house\" principle [86]. The amount of energy generated in the circuit C would be fed back to a master control circuit. (In the brain, the corresponding control system might be the \"centrencephalic system\" [79].) Whichever circuit C fed back the maximum power, the corresponding document would be extracted first. If this document alone failed to satisfy the customer completely, then the circuit C would be provisionally disconnected, and the process repeated, and so on. Ideally, this search would be probabilistic, in the sense- that the documents would be retrieved in order of descending a posteriori probability, and the latter would be registered also. If these were p1, p2, . . ., then the process would stop at the nth document, where there would be a threshold on n, and on p1 + p2 + . . . + pn. For example, the process might stop when n  = 10, or when p1 + p2 + . . . + pn > 0.95, whichever occurred first. The thresholds would be parameters, depending on the importance of the search. (For the estimation of probabilities, see [48].) When we wish to recall a memory, such as a person\u2019s name, we consciously or unconsciously use clues, analogous to index terms. These clues are analogous to weighted index terms, and it seems virtually certain that they lead to the retrieval of the appropriate memory by means of a parallel search, just as in the above hypothetical document-retrieval system. The speed of neural conduction is much too slow for a primarily\n\nserial search to be made. The search might very well be partly serial: the less familiar memories take longer to recall and require more effort. This might be because the physical embodiment of the less familiar memory requires a greater weight of clues before it will \u2018\u2018resonate\u2019\u2019 strongly enough. Further evidence that the search is, on the whole, more parallel than serial can be derived from Mandelbrot\u2019s explanation of the Zipf \"law\" of distribution of words [28]. The explanation requires that the effort of extracting the rth commonest word from memory is roughly proportional to log r. This is reasonable for a parallel search, whereas the effort would be roughly proportional to r for a serial search. When the clues do spark off the sought memory, this memory in its turn reminds us of other clues that we might have used in advance if we had thought of doing so. These \"retrieved clues\" often provide an enormous factor in favor of the hypothesis that the memory retrieved is the one that was sought: consequently we are often morally certain that the memory is the right one once it is recalled, even though its recall might have been very difficult. There is again a strong resemblance to document retrieval. When we extract a wrong memory, it causes incorrect clues to come to mind, and these are liable to block the correct memory for a number of seconds, or for longer if we panic. This is another reason why the less familiar memories take longer to recall. When we wish to recall incidents from memory, pertaining to a particular subject, the method used is to bring to mind various relevant facts and images in the hope that they are relevant enough, numerous enough, vivid enough, independent enough, and specific enough to activate the appropriate memory. (If specificity is lacking, then the wrong memory is liable to be recalled.) There is a clear analogy with the probabilistic definition of a word and probabilistic recognition of an object quoted in Section 4. A corresponding method of information retrieval is to list index terms that are relevant enough, numerous enough, independent enough, and specific enough, and (if the process is not entirely mechanized) vivid enough. This attitude towards index terms leads to forms of probabilistic or statistical indexing, as suggested independently by the writer ([35], [31], p. 12) and by Maron and Kuhns [64] who treated the matter in more detail. The present writer regards subjective and logical probabilities as partially ordered only [21], but does not consider that the fact of only partial ordering is the main source of the difficulties in probabilistic indexing. We have said enough to bring out the analogy between the process of recall and the techniques of document retrieval, and to indicate that, if it is possible to develop a comprehensive theory of either of these subjects, it should be a probabilistic theory. The need for a probabilistic theory is further brought out by means of a short discussion of what might be called \"statistical semantics.\" A complete discussion of statistical semantics would lean heavily on (i) the very intricate subject of non- statistical semantics, and on (ii) some statistical theory concerning language, without any deep discussion of semantic problems. But our purpose in this section is only to make clear that a complete treatment of statistical semantics would be somewhat more general than recall and document retrieval. If we wish to teach a language to a baby who starts in a state of comparative ignorance, we simultaneously allow him to become familiar with some part of the world of nonlinguistic objects and also with linguistic sounds, especially phonemes. The primitive ability of the baby to achieve this familiarity, although not much more remarkable than the achievements of lower animals, is still very remarkable indeed, and more so, in the writer\u2019s opinion, than anything that comes later in his intellectual development. If this opinion is correct, then most of the struggle in constructing an ultraintelligent machine will be the construction of a machine with the intelligence of an ape. The child later associates words with objects and activities, by implicit statistical inference: in fact the first words learned are surely regarded by the child as properties of an object in much the same sense as the visual, olfactory, and tactual properties of the object. For example, if the child succeeds in pronouncing a word to an adequate approximation, and perhaps points in approximately the right direction or otherwise makes approximately the right gesture, then, statistically speaking, events are more likely to occur involving the object or activity in question; and, if the environment is not hostile, the events are likely to be pleasurable. Thus the words and gestures act as statistical index terms for the retrieval of objects, and the activation of processes. At a later stage of linguistic development, similar statistical associations are developed between linguistic elements themselves. The subject of statistical semantics would be concerned with all such statistical associations, between linguistic elements, between nonlinguistic and linguistic elements, and sometimes even between nonlinguistic elements alone. A basic problem in statistical semantics would be the estimation of probabilities P(Wi | Oj) and P(Oj | Wi),where Wi represents a word (a clump of acoustic time series defined in a suitable abstract space, or, in printed texts, a sequence of letters of the alphabet with a space at both ends: admittedly not an entirely satisfactory definition), and Oj represents an object or an activity. P(Wi | Oj) denotes the probability that a person, speaking a given language, will use the word Wi to designate the object Oj, and P(Oj | Wi) is the probability that the object Oj is intended when the word Wi is used. Strictly, the estimation of probabilities is nearly always interval estimation, but, for the sake of simplicity, we here talk as if point estimation is to be used. The ranges of values of both i and j are great; the vocabulary of an educated man, in his native tongue, is of the order of 30,000 words and their simple derivatives; whereas the range of values of j is far far greater. The enormity of the class of objects is of course reducible by means of classification, which, in recognition, again involves a process of regeneration, just as does the recognition of a word. An ideal statistical dictionary would, among other things, present the two probability matrices, [P(Wi | Oj)] and [P(Oj | Wi)] (Compare Sp\u00e4rck Jones [95] and the discussion.) Such a dictionary would, apart from interdependences between three or more entities, give all the information that could be given, by a dictionary, for naming an object and for interpreting a word. Existing dictionaries sometimes indicate the values of the probabilities P(Wi | Oj) to the extent of writing \"rare\"; and also the variations between subcultures are indicated (\"archaic,\" \"dialect,\" \"slang,\" \"vulgar,\" and so on). But let us, somewhat unrealistically, imagine a statistical-dictionary maker who is concerned with a fixed subculture, so that the two probability transition matrices are fixed. One method he can use is to take linguistic texts, understand them, and thus build up a sample (fij),where fij is the frequency with which object Oj is designated by word Wi. Within the hypothetically infinite population from which the text is extracted, there would be definable probabilities P(Wi)and P(Oj) for the words and objects, and a joint probability P(Wi \u2022 Oj) crudely estimated by fij / Sij fij these joint probabilities could be estimated, then the two probability matrices could be readily deduced. We have now said enough to indicate the very close relationship that exists between statistical semantics, recall, and the retrieval of documents. In the remaining discussion in this section we shall restrict our attention to the question of retrieval of documents, including abstracts. This is a particular case of the retrieval of objects and the inauguration of processes, and the discussion brings out some of the theoretical difficulties of statistical semantics in a concrete manner. A basic problem, bordering on semantics, is the estimation of the probability P(Dj | Wi), where Wi represents a word, or index term, and Dj represents a document, or other object, and P(Dj | Wi) denotes the probability that Dj represents a sought document, when Wi is an index term, and when it is not known what the other index terms are. Strictly speaking, the probability depends on the customer, but, for the sake of simplicity, it will be assumed here that the indexer of the documents, and all the customers, speak the same\n\nindexing language. The problem of estimating P(Dj | Wi) is nontrivial to say the least [48], but let us imagine it solved for the present. Next suppose that index terms W1, W2, . . ., Wm have been specified. Then we should like to be able to compute the probabilities P(Dj | W1 \u2022 W2 \u2022 . . . \u2022 Wm), where the periods denote logical conjunction. One could imagine this probability to be estimated by means of a virtually infinite sample. Reasonable recall systems would be those for which (i) the probability that the document Dj will be recalled is equal to the above probability; (ii) the document Dj that maximizes the probability is selected; or (iii) the documents of highest (conditional) probability are listed in order, together with their probabilities. (Compare, for example [35], [31], p. 12, [41].) In one of the notations of information theory [26, 67], log P(Dj | W1 \u2022 W2 \u2022 . . . \u2022 Wm) = log P(Dj) + I(Dj : W1 \u2022 W2 \u2022 . . . \u2022 Wm) (5.1) where I(E : F) denotes the amount of information concerning E provided by F, and is defined (for example [26, 16a]) as the logarithm of the \"association factor\" (5.2) (The \"association factor\" as defined in refs [27], [31], and [41] is the factor by which the probability of one proposition is multiplied in the light of the other. It is used in a different sense, not as a population parameter, in Stiles [96].) The amount of information concerning E provided by F is a symmetrical function of E and F and is also called the \"mutual information\" between E and F, and is denoted by I(E, F) when we wish to emphasize the symmetry. Notice that our \"mutual information\" is not an expected value as is, for example, the \"relatedness\" of McGill and Quastler [67]. Shannon [89] always used expected values. If the index terms W1, W2, . . ., Wm provide statistically independent information concerning Dj (i.e., if W1, W2, . . ., Wm are statistically independent, and are also statistically independent given Dj), then log P(Dj | W1 \u2022 W2 \u2022 . . . \u2022 Wm) = log P(Dj) + (5.3) The expected rate at which the individual index terms provide information concerning documents is conveniently denoted by I(D : W) (compare [89], p. 90), but this does not allow for the expectation of the mutual information when several index terms are used. A knowledge of the expectations, for various values of m, would be relevant to the design of information-retrieval systems, since its antilogarithm would give some idea of the \"cut-down factor\" of an in-term request. When one wishes to choose between only two documents, then the final log-odds are equal to the initial log-odds plus the sum of the \"weights of evidence\" or \"log factors\" (see [21] for the terminology here and ef. Minsky [73]). It should be noted that Eq. (5.1) and (5.3) are just ways of writing Bayes\u2019 theorem, but this is not a stricture, since Bayes\u2019 theorem is likewise just a way of writing the product axiom of probability theory. It is suggestive to think of Bayes\u2019 theorem in a form that is expressible in one of the notations of information theory, since the various terms in Eqs. (5.1) and (5.3) might correspond to physical mechanisms, associative bonds between memory traces (see Section 6). The use of Eq. (5.3) might be described as the \"crude\" use of Bayes\u2019 theorem, or as a first approximation to the ideal procedure. It was used, for example, in [73], and I think also in [64]. It is a special case of discrimination by means of linear discriminants of the form (5.4) which have been used, for example, in the simplest class of perceptra [82], and in suggestions or experiments related to mechanical chess and chequers (draughts) (for example [33, 83, 18, 83a]). One can write (5.4) in the form (5.5) where now the summation is over all words in the language, not just those that occur in the context, and epsilonmu is defined as 1 if Wr does occur and as 0 otherwise. It is because of this mode of writing (5.4) that we call it a linear discriminant. It has been claimed [104] that the more general form, (5.4) or (5.5), is often much better than the crude use of Bayes\u2019 theorem, i.e., Eq. (5.3). In order to estimate the second term on the right of Eq. (5.1), a very large sample would usually be required, and this is why it is necessary to make approximations. Successively better approximations can presumably be obtained by truncating the following series: (5.6) (r, s, t, . . . = 1, 2, . . ., m; r < s < t < ...), where the I's are \"interactions of the first kind\" as defined in the Appendix. If, for example, we were to truncate the series after the interactions of the second order (the J2's),we would obtain a special case of the quadratic discriminant (5.7) which, with optimal coefficients, would of course give a better approximation. (An example of the relevance of the quadratic terms, in analogous problems in learning machines, is in the evaluation of material advantage in chess: the advantage of two bishops [33].) If we truncate Eq. (5.6) after the third-order interactions, we of course obtain a special case of a cubic discriminant, and so on.\n\nAn interesting class of problems arises when we ask: What are the optimal linear, quadratic, cubic, etc., discriminants, and how do we set about finding them? There is some discussion of this problem in [19] and in [85]. Here we merely make the obvious comment that, if the number of words is large, the number of coefficients increases rapidly with the degree, and optimization problems might be exceedingly difficult even for the cubic discriminant. Even without optimization, the work of estimating the interactions I(Dj, Wr, Ws, Wt) would be enormous. It will be suggested, in Section 6, that the subassembly theory of the brain is capable of explaining, or at least of explaining away, how the brain can in effect embody these higher interactions as association bonds between sets of subassemblies. But in the present section we shall not consider biological processes. Let us consider how, in principle, the various terms in Eq. (5.6) could be obtained. We should begin by taking a sample of successful library applications, each being of the form (W1, W2, . . ., Wm;Dj),meaning that the index terms W1, W2, . . ., Wm were used by the customer, and he was satisfied with the document Dj. If on a single occasion he was satisfied by more than one document, then, in this notation, that occasion would correspond to more than one successful library application. It should be remembered that we are assuming that all customers speak the same language. This assumption is in flagrant contradiction with the facts of life, but we assume it as an approximation in order to avoid complication. It should be noted that a sample of the kind mentioned here would form a useful part of any practical operational research on document retrieval. We can now imagine the raw statistical data to be entered in a contingency table in w + 1 dimensions, where w is the number of index terms in use (the \"size of the vocabulary\"); w of the sides of the contingency table would be of length 2, whereas the remaining side would be of length d, the number of documents. It might be suggested that the way to enter the data in the table would be to regard each successful library application as a long vector (e1, e2, . . . ew, j) (5.8) where e1 is 1 or 0 depending on whether the ith index term in a dictionary of index terms is one of the index terms, W1, . . ., Wm that was used in the application; and so to put a tick in the cell (5.8) of the contingency table. This method of constructing the contingency table would be very misleading, since there is a world of difference between missing out an index term and explicitly saying that the term is irrelevant to the sought document. This method of construction would be appropriate only if the entire vocabulary of index terms were presented to the customer to be used as a yes-no tick-off list. As R. A. Fairthorne has often pointed out, the idea of negation is not usually a natural one in indexing. Instead, the above \"successful library application\" is better regarded as contributing to the \"marginal total,\" denoted [47] by N ' ' ' . . . ' ' ' 1' ' ' . . . ' ' ' 1' ' ' . . . . . . ' ' ' 1 ' ' ' . . . ' ' ' j The meaning of this notation is this. Let be the hypothetical entry in the contingency table in the cell given by (5.8); \"hypothetical\" since tick-off lists are not in fact used. In the above notation, each of the l\u2019s, of which there are m, corresponds to the specification of an index term, and the acute accents indicate summations of over all the ei's that do not correspond to one of these m terms. After a large amount of sampling, one would have good estimates for the values of many of the marginal totals of the \"population contingency table,\" that is, the (w + 1)-dimensional array of population probabilities. The \"principle of maximum entropy\" [47, 56, 48] could then be used in principle for estimating all the 2Wd probabilities. The amount of calculation would be liable to be prohibitive, even if it were ultra-parallel, although it might be practicable in analogous small problems such as the recognition of printed characters or phonemes. It should be possible in principle to cut down the size of both the sample and the calculation by making use of the theory of clumps (\"botryology\") or of clusters. One of the benefits of such a theory would be that, by lumping together words into clumps, the dimensionality of the contingency table would be reduced. The theory of clumps is still in its infancy (see for example [30, 35, 31, 41, 78, 75, 76]), and is necessarily as much an experimental science as a theory: this is why we prefer to call it \"botryology.\" Research workers who use the term \"cluster\" rather than \"clump\" (see for example [77, 97, 81, 93, 78a]2) seem to be concerned with the grouping of points that lie in a Euclidean space, and their methods tend to be fairly orthodox from the point of view of statistical methodology. In botryology the methods tend to be less orthodox, and it is sometimes actually desirable that the clumps should overlap, both in applications to information retrieval and in potential application to neural nets. Nevertheless the two theories might be expected eventually to merge together. 2The Editor mentions [78a], which I have not seen. Let us consider how botryology might be applied for finding clumps of associated index terms and \"conjugate\" clumps of documents. (The method could also be applied to the categorization of diseases, by replacing the index terms by symptoms and the documents by people.) Let there be w index terms, and d documents. Let fij be the frequency with which index term i occurs in document j, and consider the w by d matrix F = (fij) Various botryological computations with F have been suggested in the references: the one proposed here is closest to that of Needham [76], who, however, was concerned with a square symmetric matrix of frequencies of co-occurrence of index terms, and who did not use logarithms or \"balancing,\" as described below. First replace the matrix F by the matrix [log (fij + k)], where k is a small constant (less than unity). A reason for using the logarithm is that we are proposing to use additive methods and a sum of log-frequencies is a log-likelihood. The addition of the small constant k to the frequencies is necessary to prevent zeros from going to minus \u00a5, and can be roughly justified for other reasons (see for example [58], [25], p. 241, or [48]). This modified matrix is now \"balanced\" in the following sense. By balancing an arbitrary matrix we mean adding ai + bj to cell (i,j) (i,j = 1, 2, . . .) in such a manner that each row and each column adds up to zero. It is easy to show that the balanced matrix is unique, and that the balancing constants can be found by first selecting the ai's to make the rows add up to zero, and then selecting the bj's to make the columns add up to zero. The column balancing does not upset the row balancing. For a symmetric matrix the row-balancing constants are equal to the column-balancing constants. In what follows, instead of balancing the matrix it might be adequate to subtract the mean of all the entries from each of them.\n\nLet B be the result of balancing the matrix [log(fij \u00b1 k)]. Consider the bilinear form b = x\u2019By, where x is a column vector consisting of +1\u2019s and \u20131\u2019s, and the prime denotes transposition. We now look for local maxima of b in the obvious manner of first fixing x, perhaps randomly, and finding y to maximize b (i.e., taking y = sgn B'x), and then fixing y and finding x to maximize b (i.e., taking x sgn By), and so on iteratively. The process terminates when the bilinear form takes the same value twice running. The process would lead to the separation of the words into two classes or large clumps, and two conjugate clumps of documents. Consider one of the two smaller matrices obtained by extracting the rows and columns from B, corresponding to a clump and its conjugate. Balance this smaller matrix, and find a local maximum of its bilinear form. This procedure will split our clump into two smaller clumps, and will simultaneously split the conjugate clump. In this manner we can continue to dichotomize our clumps until they are of approximately any desired size. The whole collection of clumps would form a tree. Actually, it is desirable that clumps should overlap in some applications to information retrieval, and this can be achieved by means of a slight modification of the above procedure, in which the \"large\" clumps are made larger still. That is, in place of taking all the + 1's in w as a clump, one could take all the components in B\u2019x algebraically greater than some negative threshold; and, in the conjugate clump, all the components in By above some negative threshold. The effect of this botryological procedure is to induce a partially ordered structure each of whose elements is a clump of index terms together with its conjugate clump of documents. Having obtained the partially ordered set of clumps, one could apply the methods described in [48], which, however, have not been completely worked out, in order to make estimates of I(i, j) when fij is too small for the estimate log fij \u2013 log f\u00cd \u2013 log f'j to be usable (for example when fij = 0 or 1). (We have written f\u00ed and f'j for the total frequencies of Wi and Dj.) Hopefully, the higher-order mutual information (interaction) I(W1, W2, . . ., Wm | Dj) could be estimated in a similar manner. Another conceivable method for associating documents with index terms would be in terms of the eigenvectors of BB\u2019 and of B\u2019B, where the primes still indicate transposition. By a theorem of Sylvester, the eigenvalues of B\u2019B are the same as those of BB', together with d \u2013 w zeroes, if d \u00b3 w. We can use the nonzero eigenvalues in order to pair off the two sets of eigenvectors, and we could order each of the two sets of eigenvectors in the order of the magnitudes of the eigenvalues. Then we could associate with the ith index term the ith component of the normalized eigenvectors of BB', and with the jth document the jth component of the corresponding w eigenvectors of B\u2019B. This would associate a w-dimensional vector with each index term and with each document. The relevance of index term i to document j could now be defined as the correlation coefficient between the two associated vectors. An approximate relationship between relevance and mutual information could then be found experimentally, and we could then apply Eq. (5.1) for document retrieval. The amount of calculation required for the application of this method would be exceedingly great, whereas the clumping algorithm mentioned before could perhaps be carried out on a computer of the next generation.\n## 6. Cell Assemblies and Subassemblies\nSuppose that one wishes to simulate psychological association and recall on a machine. We restrict our attention to the recall of one word when m other words are presented, but most of the discussion can be adapted, in an obvious manner, to the recall of a concept given various attributes, or to the retrieval of a document, given various index terms. The discussion could be modified in order to cover the case when the words are presented serially and form a Markov chain, this being a well-known approximate model for the prediction of words in a language text (ef. [88]). For the sake of simplicity, we shall ignore problems of syntax, so that our discussion will be in this respect more pertinent to methods of information retrieval based only on index terms than to the full problem of recall. This limited problem is difficult enough for the present, and is I think a necessary preliminary to any more ambitious discussion of recall in general. If there are w words in the vocabulary, there are potentially w(w \u2013 1)/2 associations of various strengths between pairs of words. (Kinds of association are here being ignored.) The process of recall, in this example, is that of selecting the word, A, that is in some sense most associated with the m words A1, A2, . . ., Am which have been recently inserted at the input of the computer. In the usual problem of information retrieval A would be a document and A1, A2, . . ., Am would be index terms, and the discussion of the previous section is all relevant to the present problem. The difficulty of making the probability estimates [48] provides some of the explanation of why men are not entirely rational in their probability estimates and in their recall. It is possible, for example, that, for men, the probability of retrieval of a word is approximated by only a few terms of Eq. (5.6) of the previous section. An ultraintelligent machine might be able to use more terms of the equation, since it might be able to speed up the calculations by invoking the electronic computer with which it would be in close relationship (cf. [33]). Russell and Uttley [102] suggested that a time delay might be the neural embodiment of the amount of information in a proposition, I(H) = \u2013 log P(H), and that this would make conditional probabilities easily embodiable, since the difference between two time delays is itself a time delay. As point out in [38], this idea extends at once to mutual information, log-odds, weights of evidence, and tendency to cause. But of course time delay is only one of many possible physical representations of a real variable, and others could be suggested in terms of synaptic facilitation. In view of the complexity of the brain, it is quite probable that more than one representation is used, and this would give greater scope for adaptability. One must not be overready to apply Ockham\u2019s lobotomy. As in other complex systems, many theories can contain elements of the truth. Economists are familiar with this principle. We return now to our problem of information retrieval. Suppose that w = 30,000 and that some acceptable method were found for estimating the mutual information between each pair of the 30,000 words. Then it will still be hardly practicable to list the 450 million answers in immediately accessible form in a machine that is not ultraparallel. Instead it would be necessary to put the words that have appreciable association with a given word, A, into a list of memory locations, called say the A list. Each word in each list must have the strength of the association (the logarithm of the association factor) tagged to it. Many of the lists would be very long. The process of recall involves the collation of the words in the lists corresponding to recent input words, together with some further arithmetic. Collation is a slow process, and it is tempting to ask whether it would be more economical to simulate the process of recall by means of an artificial neural network, or at any rate by means of ultraparallelism. The use of artificial associative memories is a step in this direction, but so far only a small one (for example [60, 65]). For purposes of information retrieval, which in effect is what we are discussing, it might be worth while to design computers that are not ultraparallel but have extremely rapid collation as a special feature. Such computers would be very useful for information retrieval by means of index terms, but when the words are strongly interdependent statistically, as in ordinary language, a machine using artificial neural nets seems intuitively to hold out more promise of flexibility. (See also the discussions of \"higher-order interactions\" later in this section). If each word were represented by an artificial neuron, or otherwise highly localized, it would take too long to set up the associations, unless there were w(w \u2013 1) association fibers built in, and this would be very expensive in equipment. Moreover, it is not easy to see how more than a small fraction of such a machine could be in operation at any one time, so that there would be a great wastage of potential computation power. For these reasons, a machine with \"distributed memory\" seems more promising. As Eccles says ([16], p. 266), \"Lashley argues convincingly that millions of neurons are involved in any memory recall, that any memory trace or engram has multiple representation; that each neuron or even each synaptic joint is built into many engrams\" [61]. A further relevant quotation, from [34], is: \"An interesting analogy is with the method of superimposed coding, of which Zatocoding is an example. This is a method of coding of information for information-retrieval purposes. Suppose we wish to identify a document by means of m index terms. Each term is represented by means of v punched holes in a card containing N locations each of which can be punched or not punched. [For each of the index terms] we may select locations out of the N at random [to punch]. The representation of the joint occurrence of m index terms is then simply the Boolean sum of the m individual punchings of v locations each. . . . In the application to information retrieval if we extract all the cards punched in the v locations corresponding to any given term, we may get some cards that are irrelevant by chance. If N is large, and v is suitably selected, mistakes need seldom occur. In fact it is natural to arrange that (1 \u2013 v/N)m ? ! i.e., v ? (1 - 2-1/m)N This must be the best value of v since to have half the holes punched gives the largest variety of possible punchings. \"By analogy, Nature\u2019s most economical usage of the brain would be for a reasonable proportion of it to be in operation at any one time, rather than having one concept, one neuron.\" Instead, each neuron would occur in a great many distinct circuits, and would not be indispensable for any of them. Such an analogy can at best give only a very rough idea of what goes on in the brain, which is an ultradynamic system as contrasted with a collection of punched cards. (The analogy would seem a little better if, instead of taking the Boolean sum, a threshold were used at each location.) But if we take m 20, on the grounds that the game of \"twenty questions\" is a reasonably fair game, we find that the representation of a word occupies say a thirtieth of the neurons in the cortex. It must be emphasized that this is not much better than a guess, partly because it is based on a very crude optimality principle. But it is not contradicted by the experiments of Penfield and others (for example [80], p. 117) who found that the electrical stimulation of a small area on the surface of the cortex could inhibit the recall of a fraction of the subject\u2019s vocabulary. (For further references, see Zangwill [108].) For it is entirely possible that a large subnetwork of neurons could be inhibited, and perhaps even sparked off, by stimulation at special points. Among the theories of distributed memory, the \"cell assembly\" theory is prominent, and, as stated in the previous section, a modified form of this theory will be adopted here. The meaning and point of the theory can be explained in terms of its applications to the linguistic activities of the brain, although the theory is usually discussed in a more general context. There are some advantages in discussing a special case, and some generalizations will be obvious enough. A cell assembly is assumed to consist of a great number of neurons, which can all be active at least once within the same interval of about a quarter to half a second. For simplicity we shall generally take the half-second estimate for granted. An assembly reverberates approximately as a unit, and, while reverberating, it tends to inhibit the remainder of the cortex, not neuron by neuron, but enough so that no other assembly can be very active during the same time interval. A word, or a familiar phrase, is often\n\nrepresented by an assembly, and, more generally, an assembly usually corresponds in Hebb \u2018s words, to a \"single element of consciousness.\" But the consciousness might habituate to assemblies that occur very frequently. It will be assumed in this paper that there are also subassemblies that can be active without dominating the whole cortex, and also that when an assembly becomes fatigued and breaks up it leaves several of its own subassemblies active for various lengths of time, from a second to several minutes, and typically about ten seconds. Each subassembly would consist of a smaller group of neurons than an assembly, but with greater relative interconnectivity. The subassemblies might in their turn break up into still smaller groups of still greater relative inter-connectivity and of greater \"half-lives.\" These could be called subsubassemblies, etc., but we shall usually use the term \"subassembly\" generically to include subsubassemblies, etc. When an assembly gains dominance for a moment it is approximately completely active, when the subject is wide awake. The process is assumed to be one of approximate regeneration. It is not exact regeneration for if it were there would be no learning. Probabilistic regeneration might often be represented by the degree of activity of an assembly. This degree of activity will be carried forward by the subassemblies, so that the benefits of probabilistic regeneration, as described in a previous section, will be available. Also the activity is less, and the assembly is somewhat smaller, when the subject is sleepy or dreaming, but the activity is assumed to be nearly always enough for the assembly to have a definite identity, except perhaps in dreamless sleep. When the subject is nearly asleep, there might be frequent intervals of time when there is no active assembly. The association between two assemblies could be largely embodied in the subassemblies that they have in common. When a man is in a sleepy condition, an assembly need not be followed by another consciousness- provoking assembly for a short time. In that case, the assembly A might recover from fatigue and be reactivated by the subassemblies that it itself had left in its wake when it last fired. This would account for the occasional repetitivity of thought when one is sleepy. The hypothesis is not that the assembly reverberates for longer than usual, but that it is liable to reactivate because there has not been enough activity to disperse its subassemblies. The subassemblies themselves, both in sleepiness and in dreams, have lower activity than in wakefulness, so that, when one wakes up, the memory and atmosphere of dreams would be easily erased. When dreaming there is perhaps not enough energy in the cortex to sustain many full assemblies so that the subassemblies would be less inhibited than in wakefulness. It might well be that there are far more subassemblies active during sleep, and they would form arrangements having less logical cohesion and higher entropy. This would explain the remarkable rate at which visual information can be internally generated during dreams; and the incomplete regeneration of full assemblies would explain the non sequitur and imaginative nature of dreams. In the same spirit, if assemblies correspond to conscious thoughts, it might well be that subassemblies correspond to unconscious and especially to preconscious thoughts, in the wakeful state as well as in sleep. What gives the assemblies their semipermanent static structures, corresponding to long-term memory, is assumed, following Hebb, to be the pattern of strengths of synaptic joints throughout the cortex. The physical counterpart of learning is the variation of these strengths. We have already conjectured that the number of possible states of any synaptic joint is small enough to justify calling the strength a \"discrete variable.\" This assumption makes it easier to understand how memories can be retained for long periods, and how the identities of assemblies can be preserved. We assume that the strength of a synapse, when not in use, occasionally mutates in the direction of some standard value. This mechanism would explain the gradual erosion of memories that have not been recalled, and would also help to prevent all synapses from reaching equal maximal strength, which would of course be disastrous. Equally, the increase in strength of a synapse when its activation leads to the firing of a neuron can reasonably be assumed to be a mutation and only probabilistic. The number of synapses is so large that it might well be sufficient for only a small fraction of them to mutate when they contribute to the firing of a neuron. This hypothesis would also help to explain why all synapses do not reach maximum strength. Even when an assembly sequence is frequently recalled, some of the strengths of the relevant synapses would nevertheless have mutated downwards, so that some of the many weaker subassemblies involved in the assembly sequence would have become detached from the structure of the assembly sequence. Thus the structure of a frequently used assembly sequence, used only for recall and not for building into fantasy or fiction, would tend to become simplified. In other words, detail would be lost even though what is left might be deeply etched. Thus the corresponding memory would tend to become stereotyped, even in respect of embellishments made to it after the first recording. It is interesting to consider what enables us to judge the time elapsed since a memory was first inscribed. Elapsed time seems introspectively to be recorded with roughly logarithmic accuracy: the least discernible difference of a backward time estimate is perhaps roughly proportional to the time elapsed, not allowing for the \"cogency\" of the recall, that is, not allowing for the interconnections and cross-checks in the recall. This conjecture, which is analogous to the Weber-Fechner law, could be tested experimentally. An aging memory suffers from a gradual loss of \"unimportant\" detail. If, on the other hand, we recall an item repeatedly, we preserve more of the detail than otherwise, but we also overlay the memory with additional associations to assemblies high up in the hierarchy. We can distinguish between \"reality\" and imagination because a memory of a real event is strongly connected to the immediate low-order sensory and motor assemblies. As a memory ages it begins to resemble imagination more and more, and the memories of our childhood are liable to resemble those of a work of fiction. One of the advantages that an ultraintelligent machine would have over most men, with the possible exception of millionaires, would be that it could record all its experiences in detail, on photographic film or otherwise, together with an accurate time-track. This film would then be available in addition to any brain- like recordings. Perfect recall would be possible without hypnotism! As pointed out by Rosenblatt ([82], p. 55), a permanent lowering of neural firing thresholds would be liable to lead to all thresholds becoming minimal, unless there were a \"recovery mechanism.\" He therefore prefers the more popular theory of synaptic facilitation, which we are using here [15, 54]. Although there are far more synapses than neurons, a similar objection can be raised against this theory, namely, too many synapses might reach maximal facilitation, especially if we assume a cell assembly theory. This is why we have assumed a mutation theory for synaptic strengths. In fact, we assume both that a synapse, when not in use, mutates downwards, with some probability, and also, that when it has just been used, it mutates upwards, with some probability. The higher the strength at any time, the greater the probability of mutating downwards when not used, and the smaller the probability of mutating upwards when used. It is neither necessary nor desirable that every synapse should increase its strength whenever it is used. The enormous number of neurons in an assembly make it unnecessary, and the frequent uses of the synapses make it undesirable. After a certain number of uses, an assembly does not need any further strengthening. A sentence lasting ten seconds would correspond to an assembly sequence of about twenty assemblies. Hebb ([54], p. 143) says that the apparent duration of a \"conceptual process\" in man is from one to five or ten seconds. The expression \"conceptual process\" is of course vague, and the discussion is here made somewhat more concrete by framing it in terms of linguistic activity. A phoneme, when it is part of a word, perhaps corresponds to a subassembly, and there will be many other subassemblies corresponding to other properties of the word, but only a fraction of these will remain active when the assembly breaks up.\n\nWhich assembly becomes active at the next moment must depend on the current sensory input, the current dominant assembly, and the currently active subassemblies. Indirectly, therefore, it depends on the recent assembly sequence, wherein the most recent assemblies will have the greatest influence. It also depends of course on the semipermanent static storage, the \"past history.\" Well-formed assemblies will tend to be activated by a small fraction of their subassemblies; this is why it is possible to read fast with practice: it is not necessary to observe all the print. Memory abbreviates. An example that shows how the activation of an assembly can depend on the previous assembly sequence is the recall of a long sequence of digits, such as those of p. A. C. Aitken and Tom Lehrer, for example, can repeat several hundred digits of pi correctly. If we assume that there is one assembly for each of the ten digits 0, 1, \u2026 9, then it is clear that the next assembly to be activated must depend on more than just the previously active assembly. If there is no hexanome (sequence of six digits) that is repeated in the first 500 digits of p, then one method of remembering the 500 digits in order is to memorize a function of hexanomes to mononomes. Then any six consecutive digits would uniquely determine the next digit in this piece of p ; for example, the digit 5 is determined by the hexanome 415926. Let us consider how the subassembly theory would account for this, For the sake of argument, we shall ignore the strong possibility that a calculating prodigy has an assembly for say each of the hundred distinct dinomes, and continue to assume one assembly for each of the ten digits. (The argument could be modified to allow for other possibilities.) We take it for granted that the subject (Aitken) is in the psychological \"set\" corresponding to the recitation of the digits of pi. Suppose that the assembly corresponding to the digit i has subassemblies s(i, 1), s(i, 2), and that these symbols correspond to subassemblies of successively shorter \"half-lives.\" Then, provided that the digits are recited by Aitken at a constant rate, one set of subassemblies that would activate the assembly corresponding to 5 would be of the form s(4, 1), s(4, 2), . . ., s(4, n4,6); . . . ; s(6, 1), s(6, 2), . . ., s(6, n6,1), where s(i, ni,j) is the next subassembly (belonging to assembly i) to become extinguished after j \"moments of time.\" If at least one subassembly of each assembly is extinguished at each moment within the first six moments after the assembly is extinguished, then this theory could account for the possibility of the recitation. For, at any given moment, the active sub-assemblies would uniquely determine the next assembly to be activated. If the recitation were slowed down by a moderate factor, then there would still be enough clues for the unique determination of the successive digits. In fact a knowledge of the maximum slow-down factor would give quantitative information concerning the numbers and durations of activation of the subassemblies. There is an analogy between cell assemblies and the gel that can form in a polymerization reaction. (See Flery [17]{Sp.?} for a comprehensive discussion of polymerization, or [45] for a short self-contained description of some mathematical theory that might also be relevant to cell assemblies.) The gel is often regarded as a molecule of infinite size, but there can be other largish molecules present simultaneously, analogous to the subassemblies. Polymerization is not as dynamic as cerebral activity, so the analogy is imperfect, but it is instructive since it shows the plausibility of subassemblies. A theory that does some of the work of the subassembly theory is the theory of \"primed neurons\" ([43], p. 506 and [71]). We quote (from the former reference): \"After an assembly has just been extinguished, many of its neurons will have received subthreshold activation without having fired. Milner calls them \u2018primed neurons\u2019. . .. A primed neuron may be regarded as the opposite of a refractory one. Therefore, in virtue of \u2018temporal summation\u2019 for neurons, parts of a recently extinguished assembly will be primed, so that it will be easily reactivated during the next few seconds. This is an explanation of short-term memory different from that of reverberatory circuits; but an activated assembly must itself reverberate. Milner assumes that the effect of priming dies away after a few seconds. But I think it would be useful to assume that the time constant can vary greatly from neuron to neuron since this may help to explain our sense of duration, and also medium-term memory. Here, as elsewhere, other explanations are possible, such as the gradual extinction of small reverberating circuits within assembles.\" (The last remark is a reference to subassemblies;\n\nsee also [41].) The subassembly theory seems to be a more natural tool than that of primed neurons, for the purpose of explaining the sequence of firing of assemblies although both might be features of the brain. One would expect subassemblies to exist, since the density of connectivity in an assembly would be expected to vary from place to place in the cortex. Subelumps of high connectivity in a network would be expected to reverberate longer than those of low connectivity. Although it could be argued that highly connected subelumps should become exhausted more quickly, it should be observed that the synapses in these sub- clumps will tend to be stronger than where the connectivity is low. It is therefore natural to assume that the subelumps correspond to sub-assemblies. It might turn out that the theory of primed neurons will be sufficient to explain the workings of the brain, without the assumption of sub-assemblies, but the latter theory gives the kind of discrete representation that fits in well with the notion of probabilistic regeneration. The theory of subassemblies is so natural for any large partly random-looking communication network (such as that of a human society) that it tempts one to believe, with Ashby ([4], p. 229), that a very wide class of machines might exhibit intelligent behavior, provided that they have enough interconnectivity and dynamic states. Machines certainly need some design, but it is reasonable to suppose that money and complication can be traded for ingenuity in design. For example, a well-designed machine of say 109 components might be educable to ultra-intelligence, but a much more carelessly designed machine of say 1013 components might be equally good. That some design is necessary can be seen from one of the objections to the cell assembly theory as originally propounded by Hebb. Hebb did not originally assume that it was necessary to assume inhibition, and Milner pointed out that, without inhibition, the assemblies would fill the whole cortex. Ultimately there could be only one assembly. Either inhibition must be assumed to exist, as well as excitation, or else the assemblies would have to be microscopically small in comparison with the cortex. The latter assumption would be inconsistent with \"distributed memory.\" Milner accordingly assumed that neurons tend to inhibit those near them. Therefore one may picture an assembly as a kind of three-dimensional fishing net, where the holes correspond to inhibited neurons. The simplest model would assume that each fishing net (assembly) spans the entire cortex, or perhaps only the entire association cortex, or perhaps also other parts of the brain [57]. In future, mainly for verbal simplicity, we use the word \"cortex\" unqualified. There is a need for some mathematical theorems to show that a very large number of distinct assemblies could exist under reasonable assumptions for the parameters that describe connectivity. It is reasonable to conjecture that the thinness of the cortex is a relevant parameter, or rather the \"topology\" that is encouraged by the thinness. The dimensions of the cortex, if straightened out, would be about 50 cm by 50 cm by 2 mm ([90], pp. 32 and 34). It is possible that the assembly theory would become impossible if the cortex were much \"thicker.\" If we cannot treat the problem mathematically, perhaps we should experiment with an artificial neural net of neural dimensions approximately 50 ! 10,000 ! 10,000, but smaller-scale experiments would naturally be tried first. There must surely be some advantage in having thin cortices, otherwise people would have thicker ones. It seems unlikely that the brain contains many useless residuals of evolutionary history. Hence the anatomy of the brain is very relevant to the design of the first ultraintelligent machine, but the designer has to guess which features have important operational functions, and which have merely biochemical functions. Since it is not known what values of the parameters are required for the intelligent operation of a neural net, it is possible only to guess which features of the cortex are most relevant for the design of an ultra-intelligent machine. The feature of a good short-term memory (\"attention span\"), of the order of 20t, where t is the active time of a single assembly, is certainly essential for intelligence. (In a machine t need not be approximately half a second.) It might even be possible to improve on the performance of a brain by making the average duration of the sequence somewhat greater than 20t. But there must be a limit to the useful average duration, for a given cost in equipment. This limit might be determined by the fact that the longer an assembly sequence the smaller must be the average size of the assemblies; but is more likely to be determined by the fact that the complexity of concepts can be roughly measured by the durations of the assembly sequences, and beyond a certain level of complexity the brain would not be large enough to handle the relationships between the concepts. (In a more precise discussion the duration would be interpreted as a kind of \"half-life.\") When guessing what biological features are most relevant to the construction of an ultraintelligent machine, it is necessary to allow for the body as a whole, and not just the brain: an ultraintelligent machine would need also an input (sensorium) and an output (motorium). Since much of the education of the first ultraintelligent machine would be performed by a human being, it would be advisable for the input and output to be intuitively tangible. For example, the input might contain a visual and a tactual field and the output might control artificial limbs. In short the machine could be something of a robot. The sensorium and motorium might be connected topographically to parts of the two surfaces of the disk that represents the cortex. Many other decisions would have to be made concerning the design, even before any really useful experiments could be performed. These decisions would concern qualitative details of structure and also the values of quantitative parameters. The need for further theory is great, since, without advances in theory, the amount of experimentation might be prohibitive. Even if the values of the parameters in the cerebral cortex were known [90], theory would be required in order to decide how to scale them to a model with fewer components. A very tentative example of some quantitative theory is given near the end of the present section. It has been argued [79] that the cortex seems to be under the control of a more centrally placed subeortical region, partly in the diencephalon, \"not in the new brain but in the old\" ([80], p. 21).3 Penfield calls the partly hypothetical controlling region the \"centrencephalic system.\" It seems that consciousness is likely to be associated with this system. A natural inference of the hypothesis that consciousness is associated with the old brain is that the lower animals have consciousness, and can experience \"real metaphysical pain,\" an inference natural to common sense but disliked by some experimentalists for obvious reasons: they therefore might call it meaningless. Sometimes Penfield's theory is considered to be inconsistent with Hebb\u2019s, but in the present writer\u2019s opinion, the assembly theory is made easier to accept by combining it with this hypothesis of a central control. For the following mechanism suggests itself. The greater the amount of activity in the cortex, the greater the number of inhibitory pulses sent to all currently inactive parts of the cortex by the centrencephalic system. This negative feedback mechanism would prevent an assembly from firing the whole cortex, and would also tend to make all assemblies of the same order of size, for a given state of wakefulness of the centrencephalic system. This in its turn would be largely determined by the condition of the human body as a whole. This \"assembly theory, MARK III,\" as we may call it (taking a leaf out of Milner [71]), has two merits. First, it would allow a vastly greater class of patterns of activity to assemblies: they would not all have to have the pattern of a three-dimensional fishing net, filling the cortex. This makes it much easier to accept the possibility that a vast variety of assemblies can exist in one brain, as is of course necessary if the assembly theory is to be acceptable. A second, and lesser, merit of the modified theory is that a single mechanism can explain both the control of the \"cerebral atomic reactor\" and degrees of wakefulness, and perhaps of psychological \"set\" also. Finally, the theory will shortly be seen to fit in well with a semiquantitative theory of causal interactions between assemblies.\n\n3 Zangwill gives earlier references in his interesting survey [108]. It is proposed therefore that our artificial neural net should be umbrella-shaped, with the spikes filling a cone. During wakefulness, most assemblies will have a very complicated structure, but, during dreamless sleep, the centrencephalic system will become almost exclusively responsible, directly and indirectly, for the activity in the cortex, taking for granted of course the long-term or \"static\" structure of the cortex. The input from the cortex to the centrencephalic system will, as it were, be \"reflected back\" to the cortex. The assumption is that the excitation put out by the centrencephalic system has the function of encouraging cortical activity when it is low, and discouraging it when it is high. Under a wide class of more detailed models, the amount of activity will then have approximately simple harmonic amplitude when other input into the cortex is negligible. Since we are assuming that the duration of a cell assembly is about half a second, following Hebb, it is to be expected that the period of this simple harmonic motion will also be about half a second. This would explain the delta rhythm ([103], p. 167) which occurs during sleep. Apparently, very rhythmic assemblies do not correspond to conscious thought. To some extent this applies to all assemblies that are very frequently used. Consciousness is probably at its height when assemblies grow. In order to explain the alpha rhythm, of about five cycles per second, when the eyes are closed and the visual imagination is inactive, along similar lines, we could assume that \"visual assemblies\" have a duration of only about a fifth of a second. This would be understandable on the assumption that they are on the whole restricted to the visual cortex, i.e., to a smaller region than most other assemblies (cf. Adrian and Matthews [2]). We have assumed that, when no assembly is active, the centrencephalic system encourages cortical activity, so that, at such times, the various current active subassemblies will become more active. This process will continue until the activity reaches a critical level, at which moment the neurons not already active are on the whole inhibited by those that are active, including those in the centrencephalic system. This is the moment at which, by definition, an assembly has begun to fire. If this happens to be a new assembly, then the interfacilitation between its subassemblies will establish it as an assembly belonging to the repertoire of the cortex. This will happen whenever we learn something new or when we create a new concept. The newborn child has certain built-in tendencies, such as the exercise of its vocal organs. We assume that there are pleasure centers in the brain, whose function is reinforcement, and that they are usually activated when there is a \"match\" between a sound recently heard and one generated by the vocal organs. The matching could be done by a correlation mechanism, which in any case is apparently required in order to recognize the direction of a sound. E. C. Cherry [13] points out the need for this, and also the possibility of its more general application (see also [57, 63]). Also the child is rewarded by attention from its parents when it pronounces new phonemes for the first time. Thus one would expect assemblies to form, corresponding to the simplest correctly pronounced phonemes. The phonemes in agricultural communities might be expected to be influenced by the farm animals. Assemblies corresponding to syllables and short words would form next, apart from the words that were negatively reinforced. Each assembly representing a word would share subassemblies with the assemblies that represent its phonemes. An assembly for a word would also have subassemblies shared with nonlinguistic assemblies, such as those representing the taste of milk, and, more generally, representing experiences of the senses, especially at the nine apertures, where the density of neurons is high for evolutionary reasons. And so, gradually, the largely hierarchical structure of assemblies would be formed, the lowest levels being mostly closely connected with the motorium and also with the sensorium, especially where the surface neural density is high.\n\nIt is interesting to speculate concerning the nature of the associations between cell assemblies. We shall suppose that there is some measure of the strength of the association from one cell assembly, F, to another one, A, or from an assembly sequence F to the assembly A. Assuming the subassembly theory, this association will be largely embodied in the strength of the association to A from the subassemblies left behind by F, and will depend on the degrees of activation of the subassemblies and on the current psychological \"set.\" A few distinct but related formulas suggest themselves, and will now be considered. In these formulas we shall take for granted the degrees of activation and the psychological set, and shall omit them from the notation. The first suggestion is that the strength of the association from F to A should be measured by J(A : F), as in the discussion of information retrieval in Section 5. If F is the assembly sequence A1, A2, . . ., Am, and if these assemblies supply statistically independent information, we have, by Eq. (5.3): It could then be suggested that the term log P(A) is represented by the strength of the connectivity from the centrencephalic system to A. Actually it is unlikely that the assemblies will supply statistically independent information, and it will be necessary to assume that there are interaction terms as in Eq. (5.6). We would then have an explanation of why the next assembly that fires, following an assembly sequence, is often the one that ought to have the largest probability of firing in a rational man. More precisely, the terms I(A : Ar) corresponding to the most recently active assemblies will be represented with larger weights. Consequently, when we wish to recall a memory, it pays to hold in mind all the best clues without the intervention of less powerful clues. An objection to the above suggestion is that it is necessary to add a constant to log P(A) to make it positive, and then the neurophysiological \"calculation\" of the strength of the association from the centrencephalic system would be ill-conditioned. Accordingly we now consider another suggestion. One of the distinctions between the action of the brain and document-retrieval systems is that the brain action is considerably more dynamic. The activity of the assemblies constitutes an exceedingly complicated causal network. It is natural to consider whether the causal calculus [39] might be applicable to it. Reference [39] contains two immediately relevant formulas, namely, the tendency of F to cause E ( denotes \"not F\"), also described as \"the weight of evidence against F if E does not occur\"; and the \"intrinsic\" tendency of F to cause E. In both formulas, the laws of nature, and the state of the world immediately before the occurrence of F, are taken for granted and omitted from the notation. Like the mutual information, both Q and K have the additive property\n\nQ(E : F \u2022 G) Q(E : F) + Q(E : G | F) K(E : F \u2022 G) = K(E : F) + K(E : G | F) Moreover Q(E : F \u2022 G) Q(E : F) \u00b1 Q(E : G) K(E : F \u2022 G) = K(E : F) + K(E : G) when F and G are \"independent causes\" of F. This means that F and G are statistically independent, and are also statistically independent given not E. This definition of independent causes, extracted from [39], was seen to be a natural one by the consideration of a firing squad: is the event that the victim is shot, F and C are the events of shooting by two marksmen; and part of the given information, taken for granted and omitted from the notation, is that the sergeant at arms gives the order to fire. We now take F as the firing of an assembly or assembly sequence, also denoted by F, and we take E as the firing of the assembly A. The suggestion is that Q or K is a reasonable measure of the strength of the association from F to A. We then have additivity in so far as the components of F, assemblies or subassemblies, have independent tendencies to cause A to fire. Otherwise various interaction terms can be added, and can be expressed in various ways, for example, K(E : F \u2022 G) = K(E : F) + K(E : G) + I(F : G) \u2013 I(F : G | ) The \"causal force,\" K(E : F), tends to activate A, but the assembly that is activated will not be the one that maximizes K(E : F), but rather the one that maximizes P(E | F). This can be achieved by assuming that the centrencephalic system applies a \"force\" \u2013log[1 \u2013 P(E)]. [This will always be well approximated simply by P(E).] The resultant force will be \u2013log[1 \u2013 P(E|F)] and increases with P(E | F) as it should. We see that K(E : F) appears to be more logical than Q(E : F) for our purpose, since it would be more difficult to see how the centrencephalic system could apply a \"force\" equal to \u2013log [1 \u2013 P(E| )] to A. If there exists no E for which \u2013log[1 \u2013 P(E |F)] exceeds some threshold, then a new assembly will be activated, or else the next thought that occurs will be very much of a non sequitur. It could be asked, what is the advantage of using K(E : F) rather than \u2013log[1 \u2013 P(E | F)], as a measure of the strength of the association from F to A? (In the latter case the centrencephalic system would not need to make a contribution.) Two answers can be given: first that, if P(E | F) = P(E), then F should have no tendency to cause A to fire. Second, that, when F and C have independent tendencies to cause E, we can easily see that \u2013log[1 \u2013 P(E | F \u2022 G)] = \u2013log[1 \u2013 P(E | F)] \u2013 log[1 \u2013 P(E | G)] + log[l \u2013 P(E)] and consequently the strengths would not be additive. Hopefully, these measures of strengths of association between assemblies will help to suggest some quantitative neural mechanisms that could be put to experimental test. In physical terms, the interaction between a pair of assemblies, A and B, will depend on the number and size of the subassemblies (including the subsubassemblies) that they have in common. This set of subassemblies could be called the \"intersection,\" A.B. (A more complete notation would be A.B(T), where T is the time since B fired. The intersection decreases to zero as T increases.) The second-order interaction between three assemblies, A, B, and C, will depend on the set of subassemblies common to all of them, A.B.C. If B and C have just been active, they will contribute a \"force\" tending to activate A, expressible in the form | A.B | + | A.C | \u2013 | A.B.C |, where the moduli signs represent in some sense the current total strengths of the sets of subassemblies. The term | A.B.C| is subtracted in order that it should not be counted twice. More generally, as in the Boole-Poincare' theorem, the firing of an assembly sequence, A1, A2, . . . Am, will have an \"intrinsic\" tendency to cause A to fire, measured (compare the Appendix) by (r < s < t < ...). To this must be added a term depending on the current \"force\" on A from the centrencephalic system, which will perhaps be a function only of the probability that A fires conditional only on past history and psychological \"set.\" The assembly, A, for which the total causal force is a maximum is the one most likely to fire, or, on a deterministic theory, the one that actually will fire. The formula can be interpreted in various ways, depending on whether we have in mind a theory of primed neurons, a theory of subassemblies, or a mixture of the two if we use the anti-Ockham principle for very complex systems. We shall now consider another semiquantitative aspect of the interaction between assemblies. Suppose that A and B are two assemblies having no previous association, but that A happens to occur before B, owing to the sequence of events at the sensorium. Suppose that each of the assemblies contains about a fraction of the cortex (or of the association areas), where might be, say, 1/30, although this is in large part a guess, as we said before. The neurons in common will constitute about a2 of the cortex. The synapses connecting these will undergo a slight change of state, encouraging interfacilitation. Thus the common neurons will have some tendency to include a set of subassemblies containing less than a2 of the cortex. It is not necessary to assume that the temporal order of A and B is also represented in the interfacilitation in order that a record be made of the temporal sequence of events, provided that we allow for assembly sequences consisting of more than two assemblies. When we recall some event having extension in time, we need to regenerate an assembly sequence. That this is possible is not surprising in view of the subassembly theory. For each assembly was originally fired by the subassemblies left behind by the previous assemblies of the sequence, so if we have succeeded in recalling most of these assemblies it is likely to be easy to recall the next one (since we shall have injected just about the right collection of subassemblies into our cortex). The sub-assemblies left in the wake of an assembly sequence A1, A2, . . ., Am will tend to fire Am+1, not A0, that is, there will be little tendency to remember serial events in reverse time order. If assemblies A1, A2, . . ., Ak, having no previous association, happen to occur in sequence, where k is not more than about 20, then primitive subassemblies (or classes of subassemblies) (A1, A2), (A2, A3), (Ak-1, Ak) will form, and perhaps also some weaker subassemblies (Ar, As), where r < s \u2013 1. These will be at least analogous to the mutual informations I(Ar, As), which, for nonperiodic Markov processes, do tend to be weaker and weaker, the larger is s \u2013 r. Similarly sets of subassemblies and perhaps subsubassemblies will form, corresponding to triples of assemblies, and analogous to the mutual informations I(Ar, As, At), and so on, for interactions of higher order. (Similar comments, both here and later, can be made if the strengths of association are defined in terms of K in place of I.) The set of subassemblies arising from the \"intersection\" of q assemblies of which none had been previously associated, could hardly occupy a proportion of the cortex larger than aq, so that, if 1/30, q could not be larger than log30 (5 x 109) = 6!. This would not constitute a serious biological disadvantage, since high-order interactions can generally be ignored, judging by the practice of statisticians in factorial experiments (see the Appendix). The upper limit is reminiscent of the \"depth hypothesis\" [69, 107]. Compare also the experiment mentioned at the beginning of Section 5. We have seen that it is impracticable to take a sample of language that is large enough to be able to judge the association factors (the exponentials of the amounts of mutual information) between all pairs of 30,000 words by simple frequency counts. It is reasonable to assume that direct psychological association between words is determined by the frequencies with which they occur nearly simultaneously in thought, and this is easy to understand in a general way in terms of the assembly and subassembly theory. But we can recognize logical associations between pairs of words that have never occurred together in our experience; for example, the words \"ferry\" and \"fare\" can be seen to be associated in the same manner as \"bus\" and \"fare,\" even if we never previously made the association in our minds. Likewise, if we were asked to estimate the mutual information between the first two words \"ferry\" and \"fare,\" regarded as index terms for sentences, we could reasonably take it as equal to that between the second pair. This is a simple example to show that we make use of semantics even in the simplest problems of association whenever our samples have not been large enough to rely on mere frequencies. The simplest conditional probability machines, such as those designed by Uttley [101], rely only on frequencies, in other words the probabilities are maximum-likelihood estimates, and they make no use of semantics. Such machines could be improved in principle by means of automatic classification of words into \"clumps\" (see Section 5). The essential idea is that words can be seen to be associated not merely because they occur frequently together, but because they both occur frequently in conjunction with a third word, or more generally with other words that belong to some reasonably objectively definable clump of words. The search for clumps is especially interesting for the purpose of trying to construct a thesaurus mechanically, hopefully for application to problems of classification and mechanical translation. A comprehensive search is liable to be very expensive in computer time, if the computer is of classical design. By using an artificial neural net, it might be possible to perform the search faster, owing to the parallel working. If A1, A2, . . ., Ak is a clump of assemblies having respectively n1, n2, . . . nk subassemblies, and if Ai and Aj have mij subassemblies in common; then, for each i, the \"clumpiness\" is much larger than it would be for a random class of k assemblies. One can define a clump by insisting that the dumpiness is decreased if any assembly is added to the clump or removed from it. Many other definitions of a clump are possible (see for example Section 5, and [31, 41], and references given in the latter article), and it is not yet clear to what extent the definitions agree with each other, nor which definitions are appropriate for various purposes. At any rate we must suppose that there is some mechanism by which an assembly representing a clump of assemblies tends to be formed, a mechanism that will correspond at least to some aspects of \"abstraction\" or \"generalization.\" Often this assembly will itself represent a word, and the existence of the word will encourage the assembly to form (for example [41], p. 122): in the example of ferries and buses the word might be \"vehicle.\" In the design of an ultraintelligent machine based on an artificial neural net, one of the most vital problems is how to ensure that the above mechanism will be effective. It seems to be necessary to assume that, when an assembly is active, it causes a little activity in all the assemblies with which it is closely associated, although only one at most of these assemblies will be the next to fire. This \"priming\" of assemblies is analogous to the priming of neurons; it is presumably operated by the subassemblies. The slightly active assemblies in their turn might encourage an even smaller amount of activity in those with which they are closely associated. In this way, there will be a small amount of activity in all the assemblies of a clump, although none of them is actually fired, and consequently a gradually increased chance that an assembly will form that will represent a clump. In terms of man, since, by hypothesis, we are not conscious of cortical activity that is not part of an active assembly, when we form a new abstraction it will emerge from the preconscious or unconscious in a manner that will seem to our conscious minds like a flash of inspiration!\n\nIt is possible that one of the functions of sleep is to give the brain an opportunity of consolidating the waking experiences by means of unconscious botryological calculations, especially those leading to improved judgments of probabilities. This assumption would be consistent with the advice to \"sleep on a problem.\" It might turn out that an ultraintelligent machine also would benefit from periods of comparative rest, but not by being switched off. Some of the matters that have been discussed in this section can be apprehended as a whole in terms of the following survey of short-term and long-term memory. In most modern computers there are several levels of storage, successively larger but slower. The reason for this is that it would be too expensive to have an exceedingly large storage with instant recall. It is natural to suppose that human memory too is split up into levels corresponding to different mechanisms. The following classification would be consistent with the discussion in this section. It is of course conjectural. i. Immediate recall (about second). Concepts currently in consciousness, embodied in the currently active assembly. ii. Very short-term memory or attention span (! second to 10 seconds). Embodied in the currently active subassemblies, largely the residues of recently active assemblies. The span might be extended up to several minutes, with embodiment in subsubassemblies, etc. iii. Short-term (from about 10 seconds or 10 minutes to about one day). Embodied in primed neurons. iv. Medium-term (about one day to about one month, below the age of 30, or about one week above the age of 50). Assemblies are neither partly active nor partly primed, but present only by virtue of their patterns of synaptic strengths, and with little degradation. v. Long-term (about one month to a hundred years). As in (iv) but with more degradation of pattern and loss of detail. A program of research for quantitative theory would be to marry the histological parameters to those in the above list. This program will not be attempted here, but, as promised earlier, we shall give one example of how a quantitative theory might be developed (see also, for example, [6, 92]). Let us make the following provisional and artificial assumptions: i. The probability, in a new brain, that a pair of neurons is connected is the same for every pair of neurons. ii. Each neuron has m inhibitory synapses on it, and vastly more excitatory ones. iii. A single \"pulsed\" inhibitory synapse dominates any number of pulsed excitatory ones, during a summation interval. iv. An assembly occupies a proportion of the cortex and the active subassemblies not in this assembly occupy a proportion b \u2013 alpha, making a total activity equal to b. Then a random neuron has probability (1 \u2013 b)m of escaping inhibition. In order to be active, the neuron must also escape inhibition by the centrencephalic system. So b < (1 -b)m Therefore, m < log b / log(1 \u2013 b) For example, if b = 1/15, then m < 52. It seems unlikely that any biochemical mechanism could be accurate enough to give the required value of m, without some feedback control in the maturation of the brain. But it is perhaps significant that the number of neurons in the cortex is about 232, so that, perhaps, in the growth of\n\nthe brain, each neuron acquires one inhibitory synapse per generation, 31 in all. The conjecture would have the implication that close neurons would tend to inhibit each other more than distant ones, as required by Milner [71] (compare [34]). We emphasize that this example is intended only to be illustrative of how a quantitative theory might proceed. Taken at its face value, the example is very much more speculative than the subassembly theory as a whole. We conclude this section with a brief discussion of an objection that has been made to the assembly theory. Allport ([3], p. 179) says, regarding the observation of a whole that consists of parts, a, b, c, \"\u2026There is, in Hebb\u2019s scheme, no apparent reason why we should not have . . .a perception of the parts a, b, c and alongside these at the same time another equally vivid perception of the whole, that is, of t. This, however, does not occur: we perceive either the parts in their separateness or the parts as integrated into a whole, but not both at once (see Hebb [54], pp. 98\u201399). This does not seem to be an objection to the theory in the form presented here. Even if the assembly t were originally built up largely from parts of the assemblies a, b, c, it does not contain the whole of any one of these three assemblies. Instead, it consists of parts of a, b, c and also of parts not in a, b, or c. Consequently it is only to be expected that we do not apprehend an object both as a whole and in its parts at quite the same moment. In the next section we suggest how meaning might be represented in terms of subassemblies, but only in a general manner, and not with the degree of precision that could be desired. We aim mainly to strengthen the case that semantics are relevant to artificial intelligence, and to lend support to the feeling, that is very much in the air at present, that much more detailed research into these matters is worthwhile.\n## 7. An Assembly Theory of Meaning\nOur purpose is not to define \"meaning,\" but to consider its physical embodiment. We have already discussed various aspects of meaning in previous sections, and this will enable us to keep the present section, and the next one, short. A distinction can be made between the literal meaning of a statement, and the subjective meaning that the statement has (on a particular occasion) to a man or machine. It is the latter that is of main concern to us in this essay. (For a man, subjective meaning could also be aptly called \"personal meaning\" but this name would at present seem inappropriate for a machine.) Although we are concerned with subjective meaning, the behavioral interpretation of meaning is not enough for us, as was said in Section 4. Instead, the subjective meaning of a statement might be interpreted as the set of tendencies to cause the activation of each assembly sequence at each possible time in the future. The physical embodiment of meaning, when a statement is recalled to mind, would then be a class of subassemblies. This embodiment of meaning is related to the probabilistic interpretation for the meaning of a word, given in Section 4 (the probabilistic form of \"Wisdom\u2019s cow\"). The qualities Q1,Q2, . . ., Qm, when noticed one at a time, would activate assemblies, but, when they are noticed only preconsciously, and so directly cause activity only in subassemblies, they are at best contributory causal agents in the activation of assemblies. If a statement provoked an assembly sequence, S0, presumably the (subjective) meaning of the statement is embodied in some of the sub-assemblies that were left behind by S0, the ones that reverberated the longest being the most important ones. Two statements have close meanings if the sets of subassemblies left behind by them bear a close resemblance to each other, or even if the resemblance is not close provided that the effects are similar, just as a cow can be recognized on different occasions by the apprehension of different sets of probable properties. We feel that we have understood the meaning of a statement when we somehow recognize that the statement was a definite causal agent in our thought processes or in our propensities to\n\nfuture motor activity, and that these propensities are of a kind which we think was intended by the person who communicated the statement. But I shall ignore these intentions and interpret \"meaning\" as \"meaning for us.\" \u00b0s of meaning exist, and correspond in part to greater or lesser degrees of causal tendency. The \"circularity\" mentioned in Section 4, in connection with the probabilistic interpretation of meaning, corresponds to the obvious possibility that an assembly can help to strengthen some of the weak subassemblies that helped to activate the assembly itself. A more formal suggestion for the representation of meaning can be framed as follows. Let S be an assembly sequence, and G a \"set\" in the psychological sense. (An assembly theory of psychological set is given in Hebb [54].) Let S be a statement. Denote by P(A | S \u2022 G \u2022 S) the probability that A will be the next dominant assembly to follow the assembly sequence S when the subject is in psychological set G, and when he has been told S and had no reason to doubt the veracity of his informant. If the subject had not been told S the corresponding probability would be P(A | S \u2022 G) and, if he had been told that S a was false, the probability would be denoted by P(A | S \u2022 G \u2022 S?) Then the function of A, S, and G, with values log[P(A | S \u2022 G \u2022 S) / P(A | S \u2022 G \u2022 S?)] (7.1) for all A, S, and G, is a reasonable first approximation to a representation of the \"meaning\" of S. The representation of the meaning of the negation of S is minus that of S. A reasonable representation of the \"effectiveness\" of the statement would be the function with values log[P(A | S \u2022 G \u2022 S) / P(A | S \u2022 G)] (7.2) The reason why this latter formula would be inappropriate as a representation of\"meaning\" is that it is sensitive to the subject\u2019s degree of belief in S before he is told S. A man\u2019s degree of belief in a statement should not be very relevant to its meaning. It is not intended to be implied by this representation that the subject could obtain the values of the probabilities by introspection. The probabilities are intended to be physical probabilities, not the subjective probabilities of the man or machine. (For a discussion of kinds of probability, see, for example, [36].) Expression (7.1) may be described as the log-factor or weight of evidence in favor of the hypothesis that S was stated rather than S?, provided by the event that assembly A was activated, given that the previous assembly sequence was S, and that the psychological set was G. (The terminology is that of [26] and [21], for example, and was mentioned in Section 5.) If the subject is deterministic, then the probabilities would be pseudoprobabilities, of the same logical nature as those associated with pseudorandom numbers. Expression (7.2) is the mutual information between the propositions that the assembly A was activated on the one hand\n\nand that \u00a3 was stated on the other. If the class of values of (7.1) is extended also over several subjects (who could be specified in the notation) then we should have a representation of multisubjective meaning, and we might perhaps approximate to a representation of \"true meaning\" if there is such a thing. A representation of \"literal meaning\" could be obtained by restricting the class to \"literal-minded\" men and robots, in order to exclude the poetic and irrational influences of a statement. Formulas (7.1) and (7.2) are of course only examples of possible quantitative representations of \"meaning.\" It might be better to replace them by the formulas for causal tendency. (7.1a) and (7.1b) These formulas would be more consistent with the interpretation of the meaning of a statement in terms of its causal propensities. Although we are arguing that semantics are relevant to the design of an ultraintelligent machine, we consider that it will not be necessary to solve all of the problems of semantics in order to construct the machine. If we were using the approach depending on a \"canonical language\" (see Section 4), the problems would all need solution, but if a neural net is used, we believe that the net might be capable in effect of learning semantics by means of positive and negative reinforcement, in much the same manner as a child learns. The theory of assemblies and subassemblies, as applied to semantics, is intended to provide some at least intuitive justification for this belief. It should be possible, by means of more quantitative theory and experiment, to improve, to disprove, or to prove the theory. A thoroughgoing quantitative theory will be difficult to formulate, and the experiments will be laborious and expensive, but the reward or punishment will be great.\n## 8. The Economy of Meaning\nJust as the activation of an assembly is a form of regeneration, so also is that of a subassembly, although the regeneration of subassemblies might be less sharp. The degree of regeneration of a subassembly corresponds to a preconscious estimate of the probability of some property, so that the process of recall is physically one of regeneration mixed with probabilistic regeneration. We have argued that, in any communication system, the function of regeneration and of probabilistic regeneration is economy, and so the physical embodiment of meaning also serves a function of economy. It is even possible that the evolutionary function of meaning and understanding is economy, although metaphysically we might consider that the function of evolution is the attainment of understanding! Imagine, for the sake of argument, that each meaningful proposition (defined as a class of logically equivalent statements) could be expressed by each of a hundred different statements, each of which had an entirely distinct representation in the brain. Suppose that the number of ordered pairs of propositions that are mentally associated is N. Corresponding to each pair of propositions, there would be 10,000 equivalent pairs of statements. In order to represent the N associations between propositions, we should require 10,000N\n\nassociations between statements. Although the number 100 is here a pure guess, it is clear that there must be a tremendous premium on the representation of statements by their meanings. For this saves a factor of 100 (nominally) in the storage of the propositions, and a corresponding factor of 10,000 in the storage of the associations between pairs of propositions. The latter factor is relevant in long-term recall, since the process of recalling a fact usually requires that one should have in mind several other facts. It is clear therefore that the physical representation of meaning performs a very important function of economy, especially in long- term recall, and can be expected to perform an equally important function in an ultraintelligent machine.\n## 9. Conclusions\nThese \"conclusions\" are primarily the opinions of the writer, as they must be in a paper on ultraintelligent machines written at the present time. In the writer\u2019s opinion then: It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make, since it will lead to an \"intelligence explosion.\" This will transform society in an unimaginable way. The first ultraintelligent machine will need to be ultraparallel, and is likely to be achieved with the help of a very large artificial neural net. The required high degree of connectivity might be attained with the help of microminiature radio transmitters and receivers. The machine will have a multimillion dollar computer and information-retrieval system under its direct control. The design of the machine will be partly suggested by analogy with several aspects of the human brain and intellect. In particular, the machine will have high linguistic ability and will be able to operate with the meanings of propositions, because to do so will lead to a necessary economy, just as it does in man. The physical representation of both meaning and recall, in the human brain, can be to some extent understood in terms of a subassembly theory, this being a modification of Hebb\u2019s cell assembly theory. A similar representation could be used in an ultraintelligent machine, and is a promising approach. The subassembly theory leads to reasonable and interesting explanations of a variety of psychological effects. We do not attempt to summarize these here, but merely refer the reader back to Section 6. Even if the first ultraintelligent machine does not after all incorporate a vast artificial neural network, it is hoped that the discussion of the subassembly theory is a contribution to psychology, and to its relationships with the theories of communication and causality. The activation of an assembly or a subassembly is an example of generalized regeneration, a function of which is again economy. The assembly and subassembly theories are easier to accept if combined with the assumption of a centrencephalic control system, largely because this would enable a very much greater variety of assemblies to exist. The process of long-term recall can be partly understood as a statistical information-retrieval system. Such a system requires the estimation of probabilities of events that have never occurred. The estimation of such probabilities requires some nontrivial theory even in simple cases, such as for multinomial distributions having a large number of categories. In more complicated cases, the theories are very incomplete, but will probably require a knowledge of and an elaboration of all the methods that have so far been used by actuaries and other statisticians for the estimation of probabilities. Among the techniques will be included the maximum-entropy principle, the use of initial probability distributions [47, 56, 48],and \"botryology\" (the theory and practice of clump-finding). A form of Bayes\u2019 theorem expresses the final log-probability of a \"document\" or \"memory\" as an initial log- probability, plus some terms representing I(Dj : Wi),the information concerning a document provided by an index term (or concerning a memory provided by a \"clue\"), plus additional terms representing the mutual information between index terms and the document. It is suggested that, in the brain, the initial log-\n\nprobability is possibly represented in some sense by the strength of the connectivity between an assembly and the centrencephalic system; that the terms I(Dj : Wi)are represented by the subassemblies shared between the assemblies corresponding to and Wi; and that other terms are represented by the interactions between sets of at least three assemblies. An alternative suggestion, which seems slightly to be preferred, is that the strengths of association are expressible in terms of K(E : F),the intrinsic tendency of an event E to be caused by F. This is equal to minus the mutual information between F and not E. Then the strength of the association from the centrencephalic system and an assembly would be approximately equal to the initial (prior) probability of the firing of the assembly, given the psychological \"set.\" The same remarks concerning interactions apply here as in the first suggestion. Whereas, in ordinary information-retrieval problems, the expression I(Dj : Wi) will often need to be estimated with the help of computational techniques for clumping, the strength of the connectivity between two assemblies will often be physically represented because of the manner in which the two assemblies were originally formed, by being built up from co-occurring subassemblies. The representation of informational or causal interactions, or both, up to about the sixth or seventh order, is presumably embodied in the subassemblies common to assemblies. The magical proficiency of the brain, in recall, can be largely attributed to its facility in handling these interactions. My guess is that only an ultraparallel machine, containing millions of units capable of parallel operation, could hope to compete with the brain in this respect. It seems reasonable to conjecture that the organization of the interactions into subassemblies might require the intervention of periods of rest or sleep. A possible function of sleep is to replay the assembly sequences that were of greatest interest during the day in order to consolidate them. During wakefulness, half-formed subassemblies would be subjected to the inhibitory effect of fully active assemblies, but during sleep a half-formed subassembly would have time to organize and consolidate itself. On this hypothesis, a function of sleep is to strengthen the unconscious and preconscious parts of the mind. The first ultraintelligent machine will be educated partly by means of positive and negative reinforcement. The task of education will be eased if the machine is somewhat of a robot, since the activity of a robot is concrete. Regarding the microstructure of the learning process, it is proposed that this be effected by means of reinforcement of the strengths of artificial synapses, that the available strengths for each synapse should form a discrete set, that when a synapse is not used for a certain length of time it should have a certain small probability of \"mutation\" down one step, and that when a synapse is \"successfully used\" (i.e., contributes to the activation or inhibition of an artificial neuron) it has a certain small probability of mutation up one step. The need for the changes in synaptic strength to be only probabilistic, with small probabilities, is that they would otherwise vary too quickly for the machine to be of any use, at any rate if the assembly or subassembly theory is incorporated. Deterministic changes, in any obvious sense, would be useful only if a very small fraction of the machine were in use at one instant, and this would be uneconomical.\n## 10. Appendix: Informational and Causal Interactions\nLet E1, E2, . . ., En represent events or propositions. Let the probability P(E1, E2, . . ., En), for example, where the vinculum denotes negation, be denoted by p10...1, where 0 means false and 1 means true. The 2n different possible logical conjunctions of the n propositions and their negations have probabilities denoted by pi, where i = (i1, i2, . . ., in) is an n-dimensional vector each of whose components is either 0 or 1. The array (pi) is a 2n population contingency table.\n\nA marginal total of the table is obtained by summing out one or more of the suffixes, and we denote Si3,i5pi, for example, by . When the suffixes not summed out are equal to 1, we use an alternative notation: for example, if i1 = i2 = i4 = i6 = . . . = im = 1,we denote the marginal total by P11010111...1. Thus the numbers (Pi) form another 2n array, which consists of a subset of the marginal totals of the original table. Note that p000\u20260 = p'''\u2026' = 1, and that, for example, P11000\u20260 = P(E1 \u2022 E2). The probabilities (pi) have more direct relevance to information retrieval than the probabilities (pi), since it is more natural to assert an index term than to deny one. The most relevant Pi's for this purpose will be those for which | i | is small, where | i | denotes the number of nonzero components of i. Just as each Pi is the sum of some of the pi's, so each pi can be expressed as a linear combination of Pi's. For example (the Boole-Poincare' theorem), p000...0 = (A.1) where Sm is the sum of all Pi's for which |i| = m, and i1 = 1. Interactions between events: Let E and F be two propositions or events. Write I(E) = \u2013log P(E) the amount of information in the proposition E concerning itself ([21], p. 74, [26]). Let I(E : F) = I(E, F) = log P(E \u2022 F) \u2013 log P(E) \u2013 log P(F) = I(E)+ I(F) \u2013 I(E \u2022 F) (A.2) the amount of information concerning E provided by F. It is also called the mutual information between E and F, when it is desired to emphasize the symmetry, and in this case the comma is more appropriate than the colon, since the colon is pronounced \"provided by.\" The equation shows that I(E, F) can be regarded as a measure of information interaction between E and F. For sets of more than two propositions, it is natural to generalize this definition by using the n-dimensional mod 2 discrete Fourier transform, as in the definition of interactions for factorial experiments (see for example [32]). We write (A.3) where ij = i1j1+... + injn is the inner product of i and j, and i^j = (i1j1, ..., injn) is the \"indirect product\" (halfway between the inner product and the direct product). We call Ij an (informational) interaction of the first kind and order | i | \u20131. For example, I000\u20260 I100\u20260 I110\u20260 I1110\u20260 = log P000...0 = 0 = log P000...0 \u2013 log P100...0 = \u2013log P(E1) = = I(E1) = I(E1) + I(E2) \u2013 I(E1 \u2022 E2) = I(E1, E2) = I(E1) + I(E2) + I(E3) \u2013 I(E2 \u2022 E3) \u2013 I(E3 \u2022 E1) \u2013 I(E1 \u2022 E2) + I(E1 \u2022 E2 \u2022 E3) (A.4)\n\n= I(E3 : E1) + I(E3 : E2) \u2013 I(E3 : E1 \u2022 E2) I11110\u2026 0 = I(E4 : E1) + I(E4 : E2) + I(E4 : E3) \u2013 I(E4 : E2 \u2022 E3) \u2013 I(E4 : E1 \u2022 E3) \u2013 I(E4 : E1 \u2022 E2) + I(E4 : E1 \u2022 E2 \u2022 E3) (A.5) In [37], this last expression was denoted by I2(E4 : E1\u2022 E2 \u2022 E3),but I3(E4 : E1\u2022 E2 \u2022 E3) would be a more natural notation. We avoid this notation here since we are using vectors for suffixes. We prefer to write I11110...0 = I(E1, E2, E3, E4), and regard it as the mutual information between the four propositions. By means of the Fourier inversion formula, we see, for example, that (as in [16a],p. 58) \u2013 log P11110\u20260  = I(E1 \u2022 E2 \u2022 E3 \u2022 E4) = S I(Er) \u2013 SI(Er, Es) + S I(Er, Es, Et) \u2013 SI(E1, E2, E3, E4) (A.6) where 1 \u00a3 r < s < t \u00a3 4. Equation (5.6) is readily deducible. Interactions between causal tendencies (Q or K, see page 67), are definable in a similar manner. (Compare [39], where the signs are not quite the same as here.) But we shall leave these to the reader's imagination. We also write Jj = Si(-1)ij log pi (A.7) and call Jj an (informational) interaction of the second kind, of order | j | \u20131. (It was denoted by Ij in [47].) Yet another kind of interaction, involving expected amounts of information, was defined by McGill and Quastler [67]. If n is not small, the number of cells in the population contingency table is very large, and an exceedingly large sample would be required in order to make a direct estimate of all the pi's. In order to get around this difficulty to some extent we can sample just some of the marginal totals. Then we can use the \"maximum- entropy principle\" [47, 48, 52, 55, 56, 62, 100] in order to make at least a provisional estimate of the pi's. According to this principle, one maximizes the entropy \u2013log pi, subject to the constraints (here, the assigned marginal totals) in order to set up a null hypothesis (at least this is the way it is expressed in [47] and [48]). The idea in a nutshell is to assume as much statistical independence as one can. Among other things, the following result is proved in [47]: Suppose that we know or assume a complete set of rth-order constraints for (pi), i.e., all totals of the pi's over each subset of n \u2013 r coordinates. Then the null hypothesis generated by the principle of maximum entropy is the vanishing of all the rth and higher-order interactions of the second kind. In this theorem, instead of assuming a complete set of rth-order constraints, we could assume all the interactions of the first kind and orders r \u2013 1 or less. In order to see this, we take r = 4 for simplicity and consider Eq. (A.3). If we know Pi for all i with | i | \u00a3 4, we can calculate all Ij with | j | \u00a3 4, i.e., we can deduce all the interactions of the first kind and of orders 3 or less. Conversely, given these interactions of the first kind, we can first calculate log P10000, log P0100, log\n\nP0010, log P00001, then log P1100 (since we know log P1100 \u2013 log P1000 \u2013 log P0100),and so on. We can thus determine Pi for all i with i\u00a3 4, i.e., we have a complete set of fourth-order constraints of the Pi's. Nearly always, when a statistician discusses interactions of any kind, he believes or hopes that the high- order interactions will be negligible. The maximum-entropy principle provides a comparatively new kind of rationale for this belief regarding interactions of the second kind. Whether a similar partial justification can be provided for other kinds of interaction is a question that has not yet been investigated. The question is analogous to that of the truncation of power series and series of orthogonal functions, as in polynomial approximation.\n## 11. References\n1. Artificial Intelligence. IEEE Publ. No. S-142, New York (January 1963).\n2. Adrian, E. D., and Matthews, B. H. C., The interpretation of potential waves in the cortex. J. Physiol. (London) 81, 440\u201347 1 (1934).\n3. Allport, F. H., Theories of Perception and the Concept of Structure, Chapter 19. Wiley, New York, 1955.\n4. Ashby, W. R., Design for a Brain. Chapman & Hall, London, 1960.\n5. Bar-Hillel, Y., Semantic information and its measures, Trans. 10th Conf. Cybernetics, New York, pp. 33\u201348 (1953).\n6. Beurle, R. L., Functional organization in random networks. In Principles of Self-Organization (H. von Foerster and G. W. Zopf, Jr., eds.), pp. 291\u2013311 and discussion pp. 311-314. Oxford Univ. Press, London and New York, 1962.\n7. Black, M., Language and Philosophy. Cornell Univ. Press, Ithaca, New York, 1949.\n8. Brain, Lord, Recent work on the physiological basis of speech. Advancement Sci. 19, 207\u2013212 (1962).\n9. Bruner, J. S., Postman, L, and Rodrigues, J., Expectation and the perception of color. Am. J. Psychol. 64, 216\u2013227 (1951).\n10. Carnap, R., and Bar-Hillel, Y., Semantic information. Brit. J. Phil. Sci. 4, 147\u2013157 (1953).\n11. Carter, C. F., Problems of economic growth. Advancement Sci. 20, 290\u2013296 (1963).\n12. Cherry, E. C.On Human Communication. Wiley, New York, 1957.\n13. Cherry, E. C., Two ears\u2013but one world. In Sensory Communication (W. A. Rosenblith, ed.), pp. 99\u2013\n14. Wiley, New York, 1961.\n15. Chomsky, N., Explanatory models in linguistics. In Logic, Methodology and Philosophy of Science (E. Nagel, P. Suppes, and A. Tarski, eds.), pp. 528-550. Stanford Univ. Press, Stanford, California, 1962.\n16. Culbertson, J. T., Consciousness and Behavior. W. C. Brown. Dubuque, Iowa, 1950.\n17. Eccles, J. C., Physiology of Nerve Cells. Johns Hopkins Press, Baltimore, Maryland. 1957. 16a. Fano, R. M., Transmission of Information. Wiley, New York, 1961.\n18. Flery, P. J.,{Sp.?}Principles of Polymer Chemistry. Cornell Univ. Press, Ithaca, New York, 1953.\n19. Friedberg, R. M., A learning machine, Part I. IBM J. Res. Develop.2, 2\u201313 (1958).\n20. Gabor, D., Wilby, W. P. L., and Woodcock, R., A self-optimizing nonlinear filter, predictor and simulator. In Information Theory: Fourth London Symposium (E. C. Cherry, ed.), pp. 348-352. Butterworth, London and Washington, D.C., 1961.\n21. Gall, F. J., and Spurzheim, G., Anatomie et Physiologie du Syst\u00e8me Nerveux en G\u00e9n\u00e9ral et du Cerveau en Particulier, avec des Observations sur la Possibilite' de Reconnaitre Plusiuers Dispositions Intellectuelles et Morales de l\u2019Homme et des Animaux par la Configuration de Leurs Tetes, 4 vols. Paris, 1810-1819. (Cited in Zangwill [108].) 3 and 4 are by Gall alone.\n\n22. Good, I. J., Probability and the Weighing of Evidence. Hafner, New York, 1950.\n23. Good, I. J., Review of a book by D. R. Hartree. J. Roy. Statist. Soc. A114, 107 (1951).\n24. Good, I. J., Rational decisions. J. Roy. Statist. Soc. B14, 107\u2013114 (1952).\n25. Good, I. J., in Communication Theory (W. Jackson, ed.), p. 267. Butter-worth, London and Washington, D.C., 1953.\n26. Good, I. J., On the population frequencies of species and the estimation of population parameters. Biometrika40, 237\u2013264 (1953).\n27. Good, I. J., Some terminology and notation in information theory, Proc. IEE (London) Part C (3) 103, 200\u2013204 (1956).\n28. Good, I. J., On the estimation of small frequencies in contingency tables. J. Roy. Statist. Soc. B18, 113\u2013124 (1956).\n29. Good, I. J., Distribution of word frequencies. Nature179, 595 (1957).\n30. Good, I. J., Review of a book by G. S. Brown. Brit. J. Phil. Sci.9, 254 (1958).\n31. Good, I. J., How much science can you have at your fingertips? IBM J. Res. Develop. 2, 282\u2013288 (1958).\n32. Good, I. J., Speculations concerning information retrieval. Research Rept. No. RC-78, IBM, Yorktown Heights, New York (1958).\n33. Good, I. J., The interaction algorithm and practical fourier analysis, J. Roy. Statist. Soc. B20, 361\u2013 372 (1958); B22, 372\u2013375 (1960).\n34. Good, I. J., Could a machine make probability judgments? Computers Automation 8, 14\u201316 and 24\u2013 26 (1959).\n35. Good, I. J., Speculations on perceptrons and other Automata. Research Rept. No. RC-115, IBM, Yorktown Heights, New York (1959).\n36. Good, I. J., Proc. Intern. Conf. Sci. Inform., pp. 1404 and 1406. Natl. Acad. Sci. and Natl. Research Council, Washington, D.C., 1959.\n37. Good, I. J., Kinds of probability. Scicnce129, 443\u2013447 (1959). Italian translation in L\u2019Industria, 1959.\n38. Good, I. J., Effective sampling rates for signal detection or can the Gaussian model be salvaged? Inform. Control3, 116\u2013140 (1960).\n39. Good, I. J., Weight of evidence, causality, and false-alarm probabilities. In Information Theory: Fourth London Symposium (E. C. Cherry, ed.), pp. 125\u2013136. Butterwortb, London and Washington, D.C., 1961.\n40. Good, I. J., A causal calculus, Brit. J. Phil. Sci. 11, 305-319 (1961); 12, 43\u201351 (1961); 13, 88 (1962).\n41. Good, I. J., How rational should a manager be? Management Sci. 8, 383\u2013 393 (1962). To be reprinted, with minor corrections, in Executive Readings in Management Science, Vol. I (M. K. Starr, ed.). Macmillan, New York, 1965, in press.\n42. Good, I. J., Botryological speculations. In The Scientist Speculates (I. J. Good, A. J., Mayne, and J. Maynard Smith, eds.) pp. 120-132. Basic Books, New York, 1963.\n43. Good, I. J., Review of a book by J. Wolfowitz. J. Roy. Statist. Soc. A125, 643\u2013645 (1962).\n44. Good, I. J., The mind-body problem, or could an android feel pain? In Theories of the Mind (J. M. Scher, ed.), pp. 490-518. Glencee, New York, 1962.\n45. Good, I. J., The social implications of artificial intelligence. In The Scientist Speculates 1411, pp. 192\u2013198.\n46. Good, I. J., Cascade theory and the molecular weight averages of the Sol fraction. Proc. Roy. Soc. A272, 54\u201359 (1963).\n47. Good, I. J., The relevance of semantics to the economical construction of an artificial intelligence. In Artificial Intelligence [1], pp. 157\u2013168.\n48. Good, I. J., Maximum entropy for hypothesis formulation, especially for multidimensional contingency tables. Ann. Math. Statist.34, 911\u2013934 (1963).\n49. Good, I. J., The Estimation of Probabilities. M.I.T. Press, Cambridge, Massachusetts, 1965.\n50. Good, I. J., The human preserve. Spaceflight in press (1965).\n\n51. Greene, P. H., An approach to computers that perceive, learn, and reason. Proc. Western Joint Computer Conf., pp. 181\u2013186 (1959).\n52. Halmos, P. R., Naive Set Theory. Van Nostrand, Princeton, New Jersey, 1960.\n53. Hartmanis, J., The application of some basic inequalities for entropy. Inform. Control. 2, 199\u2013213 (1959).\n54. Hayek, F. A., The Sensory Order. Univ. of Chicago Press, Chicago, Illinois, 1952.\n55. Hebb, D. O., Organization of Behavior. Wiley, New York, 1949.\n56. Jaynes, E. T., Information theory and statistical mechanics. Phys. Rev. 106, 620\u2013630 (1957); 108, 171\u2013190 (1957).\n57. Jaynes, E. T., New engineering applications of information theory. Proc. First -Symp. Eng. Applications of Function Theory and Probability (J. L. Bogdanoff and F. Kozin, eds.), pp. 163\u2013203. Wiley, New York, 1963.\n58. John, E. R., Some speculations on the psychophysiology of mind. In Theories of the Mind [43], pp. 80\u2013121.\n59. Johnson, W. E., Appendix (ed. by R.. B. Braithwaite) to Probability: deductive and inductive problems. Mind 41, 421\u2013423 (1932).\n60. Kalmus, H., Analogies of language to life. In The Scientist Speculates [41], pp. 274-279.\n61. Kiseda, J. H., Peterson, H. E., Seelbach, W. C., and Teig, M., A magnetic associative memory, IBM J. Res. Develop.5, 106\u2013121 (1961).\n62. Lashley, K. S, In search of the engram. Symp. Soc. Exptl. Riot.4, 454\u2013482 (1950).\n63. Lewis, P. M., II, Approximating probability distributions to reduce storage requirements. Inform. Control 2, 214-225 (1959).\n64. MacKay, D. M., The epistomological problem for Automata. In Automata Studies (C. E. Shannon and J. McCartby, eds.), pp. 235-251. Princeton Univ. Press, Princeton, New Jersey, 1956.\n65. Maron, M. E., and Kuhns, J. L., On relevance, probabilistic indexing and information retrieval. J. Assoc. Computing Machinery 7, 216-244 (1960).\n66. McDermid, W. L., and Peterson, H. E., A magnetic associative memory system, IBM J. Res. Develop. 5, 59\u201362 (1961).\n67. McDougall, W., Primer of Physiological Psychology. Dent, London, 1905.\n68. McGill, W., and Quastler, H., Standardized nomenclature: an attempt. In Information Theory in Psychology (H. Quastler, ed.), pp. 83\u201392. Glencee, New York, 1955.\n69. Middleton, D., and Van Meter, D., Detection and extraction of signals in noise from the point of view of statistical decision theory. J. SIAM 3, 192\u2013253 (1956); 4, 86-119 (1956).\n70. Miller, G. A., Human memory and the storage of information. IRE Trans. Information Theory 2, 129\u2013137 (1956).\n71. Miller, G. A., and Selfridge, J. A., Verbal context and the recall of meaningful material. Am. J. Psychol. 63, 176\u2013185 (1950).\n72. Milner, P. M., The cell assembly: Mark II. Psychol. Rev. 64, 242\u2013252 (1957).\n73. Minsky, M., A selected descriptor-indexed bibliography to the literature on artificial intelligence. IRE Trans. Human Factors in Electron. 2, 39\u201355 (1961).\n74. Minsky, M., and Selfridge, O. G., Learning in random nets. In Information Theory: Fourth London Symposium (E. C. Cherry, ed.), pp. 335\u2013347. Butter-worth, London and Washington, D. C., 1961.\n75. Mueller, P., Principles of temporal recognition in artificial neuron nets with application to speech recognition. In Artificial Intelligence [1], pp. 137\u2013144.\n76. Needham, R. M., Research on information retrieval, Classification and Grouping. Rept. No. M.-L.- 149, Cambridge Language Research Unit, 1961.\n77. Needham, R. M., A method for using computers in information classification. In Information Processing 1962, (M. Popplewell, ed.), pp. 284-287. North-Holland Publ. Co., Amsterdam, 1963.\n78. Neyman, J., and Scott, E. L., Statistical approach to problems of cosmology. J. Roy. Statist. Soc. B20, 1\u201343 (1958).\n79. Parker-Rhodes, A. F., Notes for a prodromus to the theory of clumps. Rept. No. LRU-911.2,\n\nCambridge Language Research Unit, 1959. 78a. Pask, G., A discussion of artificial intelligence and self-organization. Advan. Computers 5, 109\u2013 226 (1964).\n79. Penfield, W., and Jasper, H., Highest level seizures. Res. Publ. Assoc. Nervous Mental Disease 26, 252\u2013271 (1947).\n80. Penfield, W., and Roberts, L., Speech and Brain Mechanisms. Princeton Univ. Press, Princeton, New Jersey, 1959. 80a. Pierce, J. R., Symbols, Signals and Noise: the Nature and Process of Communication. Hutchinson, London, 1962.\n81. Rao, C. Radhakrishna, Advanced Statistical Methods in Biometric Research, pp. 364-378. Wiley, New York, 1950.\n82. Rosenblatt, F., Principles of Neurodynamics (Cornell Aeron. Lab., 1961). Spartan Books, Washington, D. C., 1962.\n83. Samuel, A. L., Some studies in machine learning using the game of chequers. IBM J. Res. Develop. 3, 210\u2013229 (1959). 83a. Samuel, A. L., Programming computers to play games. Advan. Computers 1, 165\u2013192 (1959).\n84. Scriven, M., The compleat robot: a prolegomena to androidology. In Dimensions of Mind (S. Hook, ed.), pp. 118\u2013142. N.Y.U. Press, New York. 1960.\n85. Sebestyen, G. S., Decision-Making Processes in Pattern Recognition. Macmillan, New York, 1962.\n86. Selfridge, O. G., Pandemonium: a paradigm for learning. In Mechanization of Thought Processes, pp. 511\u2013526. H.M.S.O., London, 1959.\n87. Serebriakoff, V., A hypothesis of recognition. In The Scientist Speculates [41], pp. 117\u2013120.\n88. Shannon, C. E., Prediction and entropy of printed English, Bell System Tech. J. 30, 50\u201364 (1951).\n89. Shannon, C. E., and Weaver, W., The Mathematical Theory of Communication. Univ. of Illinois Press, Urbana, Illinois, 1949.\n90. Shell, D. 4??., The Organization of the Cerebral Cortex, pp. 5 and 35. Wiley, New York, 1956.\n91. Shoulders, K. R., Microelectronics using electron-beam-activated machining Techniques. Advan. Computers 2, 135\u2013293 (1961).\n92. Smith, D. H., and Davidson, C. H., Maintained activity in neural nets. J. Assoc. Computing Machinery 9, 268\u2013279 (1962).\n93. Sneath, P. H. A., Recent developments in theoretical and quantitative taxonomy. System. Zeal. 10, 118\u2013137 (1961).\n94. Solomon, H. L., and Howes, D. H., Word frequency, personal values, and visual duration thresholds. Psychol. Rev. 58, 256\u2013270 (1951).\n95. Sparck Jones, K., Mechanized semantic classification. 1961 Intern. Conf. on Machine Translation of Languages and Applied Language Analysis, pp. 417\u2013435. National Physical Laboratory, Teddington, England, 1963.\n96. Stiles, H. E., Association factor in information retrieval. J. Assoc. Computing Machinery 8, 271\u2013279 (1961).\n97. Tanimoto, T. T., An elementary mathematical theory of classification and prediction. IBM, Yorktown Heights (November 1958).\n98. Tompkins, C. B., Methods of successive restrictions in computational problems involving discrete variables, Section IV. Proc. Symp. Appl. Math.15, 95-106 (1963).\n99. Tower, D. B., Structural and functional organization of mammalian cortex: the correlation of neurone density with brain size. J. Comp. Neurol. 101, 19\u201348 (1954).\n\n100. Tribus, M., Information theory as the basis for thermostatics and thermodynamics. J. Appl. Mech. 28, 1\u20138 (1961).\n101. Uttley, A. M., The design of conditional probability computers. Inform. Control 2, 1\u201324 (1959).\n102. Uttley, A. M, Conditional probability computing in the nervous system. Mechanization of Thought Processes. National Physical Laboratory Symp. No. 10, pp. 119-147 (esp. p. 144, with a reference to an unpublished paper by G. Russell). H.M.S.O., London, 1959.\n103. Walter, W. G., The Living Brain. Norton, New York, 1953.\n104. Winder, R. O., Threshold logic in artificial intelligence. In Artificial Intelligence [1], pp. 107\u2013128.\n105. Woodward, P. M., and Davies, I. L., A theory of radar information. Phil. Mag. [7], 41, 1001\u20131071 (1950).\n106. Wozencraft, J. M., and Reiffen, B., Sequential Coding. Wiley. New York,. 1961.\n107. Yngve, V. H., The depth hypothesis. In Structure of Language and its Mathematical Aspects (R. Jakobson, ed.), pp. 130\u2013138. Am. Math. Soc., Providence, Rhode Island, 1961.\n108. Zangwill, O. L., The cerebral localisation of psychological function. Advancement Sci. 20, 335-344 (1963). 4 Only items mentioned in the text are listed here; for a bibliography on artificial intelligence see Minsky [72]. Created: September 13, 1999 Last Modified: November 14, 1999 HTML Editor: Robert J. Bradbury\ndified: November 14, 1999 HTML Editor: Robert J. Bradbury"
            },
            {
              "type": "text",
              "content": "And if the first ultra intelligent machine is wort a mega-keynes, how much might the machine it builds be worth?"
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just read:\nAn article by I.J. Good speculating about the cognitive infrastructure of a thinking machine more capable than any human. He importantly claims that he will only speculate about the first agent of this kind since the second will be built by the first according to design considerations humans cannot yet fathom. Good claims that this might happen by way of the AI building a wholly different successor or by self-modifying until it becomes unrecognisable. He tries to measure the economic value of such a system in \"mega-keynes\" based on the fact that John Maynard Keynes was estimated to be the most valuable human to the world economy. \n\ntopics to explore:\n- We are a shape of intelligence grown by evolution and AI is a shape of intelligence grown by us. Will an intelligence grown or built by a hyper-intelligent AI be another such paradigm change? \n- Is economic impact even still a meaningful notion is such worlds?\n- Is the creation of further ultra-intelligent closer to a cycle or to a cascade in the sense of the previous article? \n- Is there a way to impose design constraints onto the second generation of machines? What would be the benefits and drawbacks?",
              "hidePreviousContentFromUser": false,
              "hidePreviousContentFromTutor": false
            }
          ],
          "optional": false,
          "contentId": "8a7ca9c4-c111-467b-2c2b-e08d10698767",
          "learningOutcomeId": null
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Cascades, Cycles,\u00a0Insight...",
            "author": "Eliezer Yudkowsky",
            "sourceUrl": "https://www.lesswrong.com/posts/dq3KsCsqNotWc8nAK/cascades-cycles-insight"
          },
          "segments": [
            {
              "type": "text",
              "content": "Cybernetics is the study of systems whose output is used as their input: Successful investment produces funds which can then be invested again. There are negative feedback systems which hold themselves in equilibrium and positive feedback systems which multiply their output at each step. Hold this idea in mind as you read the following article about types of feedback as they pertain to intelligence."
            },
            {
              "type": "article-excerpt",
              "content": "Cascades are when one thing leads to another. Human brains are effectively discontinuous with chimpanzee brains due to a whole bag of design improvements, even though they and we share 95% genetic material and only a few million years have elapsed since the branch. Why this whole series of improvements in us, relative to chimpanzees? Why haven't some of the same improvements occurred in other primates?\n\nWell, this is not a question on which one may speak with authority ([so far as I know](https://www.lesswrong.com/lw/kj/no_one_knows_what_science_doesnt_know)). But I would venture an unoriginal guess that, in the hominid line, one thing led to another.\n\nThe chimp-level task of modeling others, in the hominid line, led to improved self-modeling which supported recursion which enabled language which birthed politics that increased the selection pressure for outwitting which led to sexual selection on wittiness...\n\n...or something. It's hard to tell by looking at the fossil record what happened in what order and why. The point being that it wasn't _one optimization_ that pushed humans ahead of chimps, but rather a _cascade_ of optimizations that, in _Pan_, never got started.\n\nWe fell up the stairs, you might say. It's not that the first stair ends the world, but if you fall up one stair, you're more likely to fall up the second, the third, the fourth...\n\nI will concede that farming was a watershed invention in the history of the human species, though it intrigues me for a different reason than Robin. Robin, presumably, is interested because the economy grew by two orders of magnitude, or something like that. But did having a hundred times as many humans, lead to a hundred times as much thought-optimization _accumulating_ per unit time? It doesn't seem likely, especially in the age before writing and telephones. But farming, because of its sedentary and repeatable nature, led to repeatable trade, which led to debt records. Aha! - now we have _writing._ _There's_ a significant invention, from the perspective of cumulative optimization by brains. Farming isn't writing but it _cascaded to_ writing.\n\nFarming also cascaded (by way of surpluses and cities) to support _professional specialization_. I suspect that having someone spend their whole life thinking about topic X instead of a hundred farmers occasionally pondering it, is a more significant jump in cumulative optimization than the gap between a hundred farmers and one hunter-gatherer pondering something.\n\nFarming is not the same trick as professional specialization or writing, but it _cascaded_ to professional specialization and writing, and so the pace of human history picked up enormously after agriculture. Thus I would interpret the story.\n\nFrom a zoomed-out perspective, cascades can lead to what look like discontinuities in the historical record, _even given_ a steady optimization pressure in the background. It's not that natural selection _sped up_ during hominid evolution. But the search neighborhood contained a low-hanging fruit of high slope... that led to another fruit... which led to another fruit... and so, walking at a constant rate, we fell up the stairs. If you see what I'm saying.\n\n_Predicting_ what sort of things are likely to cascade, seems like a very difficult sort of problem.\n\nBut I will venture the observation that - with a sample size of one, and an optimization process very different from human thought - there was a cascade in the region of the transition from primate to human intelligence.\n\n## Cycles\n\nCycles happen when you connect the output pipe to the input pipe in a _repeatable_ transformation. You might think of them as a special case of cascades with very high regularity. (From which you'll note that in the cases above, I talked about cascades through _differing_ events: farming -> writing.)\n\nThe notion of cycles as a source of _discontinuity_ might seem counterintuitive, since it's so regular. But consider this important lesson of history:\n\nOnce upon a time, in a squash court beneath Stagg Field at the University of Chicago, physicists were building a shape like a giant doorknob out of alternate layers of graphite and uranium...\n\nThe key number for the \"pile\" is the effective neutron multiplication factor. When a uranium atom splits, it releases neutrons - some right away, some after delay while byproducts decay further. Some neutrons escape the pile, some neutrons strike another uranium atom and cause an additional fission. The effective neutron multiplication factor, denoted _k_, is the average number of neutrons from a single fissioning uranium atom that cause another fission. At _k_ less than 1, the pile is \"subcritical\". At _k_>= 1, the pile is \"critical\". Fermi calculates that the pile will reach _k_=1 between layers 56 and 57.\n\nOn December 2nd in 1942, with layer 57 completed, Fermi orders the final experiment to begin. All but one of the control rods (strips of wood covered with neutron-absorbing cadmium foil) are withdrawn. At 10:37am, Fermi orders the final control rod withdrawn about half-way out. The geiger counters click faster, and a graph pen moves upward. \"This is not it,\" says Fermi, \"the trace will go to this point and level off,\" indicating a spot on the graph. In a few minutes the graph pen comes to the indicated point, and does not go above it. Seven minutes later, Fermi orders the rod pulled out another foot. Again the radiation rises, then levels off. The rod is pulled out another six inches, then another, then another.\n\nAt 11:30, the slow rise of the graph pen is punctuated by an enormous CRASH - an emergency control rod, triggered by an ionization chamber, activates and shuts down the pile, which is still short of criticality.\n\nFermi orders the team to break for lunch.\n\nAt 2pm the team reconvenes, withdraws and locks the emergency control rod, and moves the control rod to its last setting. Fermi makes some measurements and calculations, then again begins the process of withdrawing the rod in slow increments. At 3:25pm, Fermi orders the rod withdrawn another twelve inches. \"This is going to do it,\" Fermi says. \"Now it will become self-sustaining. The trace will climb and continue to climb. It will not level off.\"\n\nHerbert Anderson recounted (as told in Rhodes's _The Making of the Atomic Bomb_):\n\n> \"At first you could hear the sound of the neutron counter, clickety-clack, clickety-clack. Then the clicks came more and more rapidly, and after a while they began to merge into a roar; the counter couldn't follow anymore. That was the moment to switch to the chart recorder. But when the switch was made, everyone watched in the sudden silence the mounting deflection of the recorder's pen. It was an awesome silence. Everyone realized the significance of that switch; we were in the high intensity regime and the counters were unable to cope with the situation anymore. Again and again, the scale of the recorder had to be changed to accomodate the neutron intensity which was increasing more and more rapidly. Suddenly Fermi raised his hand. 'The pile has gone critical,' he announced. No one present had any doubt about it.\"\n\nFermi kept the pile running for twenty-eight minutes, with the neutron intensity doubling every two minutes.\n\nThat first critical reaction had _k_ of 1.0006.\n\nIt might seem that a cycle, with the same thing happening over and over again, ought to exhibit continuous behavior. In one sense it does. But if you pile on one more uranium brick, or pull out the control rod another twelve inches, there's one hell of a big difference between _k_ of 0.9994 and _k_ of 1.0006.\n\nIf, rather than being able to calculate, rather than foreseeing and taking cautions, Fermi had just reasoned that 57 layers ought not to behave all that differently from 56 layers - well, it wouldn't have been a good year to be a student at the University of Chicago.\n\nThe inexact analogy to the domain of self-improving AI is left as an exercise for the reader, at least for now.\n\nEconomists like to measure cycles because they happen repeatedly. You take a potato and an hour of labor and make a potato clock which you sell for two potatoes; and you do this over and over and over again, so an economist can come by and watch how you do it.\n\nAs I [noted here at some length](https://www.lesswrong.com/lw/vd/intelligence_in_economics), economists are much less likely to go around measuring how many scientific discoveries it takes to produce a _new_ scientific discovery. All the discoveries are individually dissimilar and it's hard to come up with a common currency for them. The analogous problem will prevent a self-improving AI from being _directly_ analogous to a uranium heap, with almost perfectly smooth exponential increase at a calculable rate. You can't apply the same software improvement to the same line of code over and over again, you've got to invent a new improvement each time. But if self-improvements are triggering more self-improvements with great _regularity,_ you might stand a long way back from the AI, blur your eyes a bit, and ask: _What is the AI's average neutron multiplication factor?_",
              "collapsed_before": "Followup to: [Surprised by Brains](https://www.lesswrong.com/lw/w4/surprised_by_brains)\n\n_Five sources of discontinuity: 1, 2, and 3..._\n\n## Cascades",
              "collapsed_after": "Economics seems to me to be [largely the study of production _cycles_](https://www.lesswrong.com/lw/vd/intelligence_in_economics) - highly regular repeatable value-adding actions. This doesn't seem to me like a very deep abstraction so far as the study of optimization goes, because it leaves out the creation of _novel knowledge_ and _novel designs_ - further _informational_ optimizations. Or rather, treats productivity improvements as a mostly exogenous factor produced by black-box engineers and scientists. (If I underestimate your power and merely parody your field, by all means inform me what kind of economic study has been done of such things.) (Answered: This literature goes by the name \"endogenous growth\". See comments [starting here](http://www.overcomingbias.com/2008/11/cascades-cycles.html#comment-140280102).) So far as I can tell, economists do not venture into asking where discoveries _come from_, leaving the mysteries of the brain to cognitive scientists.\n\n(Nor do I object to this division of labor - it just means that you may have to drag in some extra concepts from outside economics if you want an account _of self-improving Artificial Intelligence._ Would most economists even object to that statement? But if you think you can do the whole analysis using standard econ concepts, then I'm willing to see it...)\n\n## Insight\n\nInsight is that mysterious thing humans do by grokking the search space, wherein one piece of highly abstract knowledge (e.g. Newton's calculus) provides the master key to a huge set of problems. Since humans deal in the compressibility of compressible search spaces (at least the part _we_ can compress) we can bite off huge chunks in one go. This is not mere cascading, where one solution leads to another:\n\nRather, an \"insight\" is a chunk of knowledge _which, if you possess it, decreases the cost of solving a whole range of governed problems._\n\nThere's a parable I once wrote - I forget what for, I think ev-bio - which dealt with creatures who'd _evolved_ addition in response to some kind of environmental problem, and not with overly sophisticated brains - so they started with the ability to add 5 to things (which was a significant fitness advantage because it let them solve some of their problems), then accreted another adaptation to add 6 to odd numbers. Until, some time later, there wasn't a _reproductive advantage_ to \"general addition\", because the set of special cases covered almost everything found in the environment.\n\nThere may be even be a real-world example of this. If you glance at a set, you should be able to instantly distinguish the numbers one, two, three, four, and five, but seven objects in an arbitrary (non-canonical pattern) will take at least one noticeable instant to count. IIRC, it's been suggested that we have hardwired numerosity-detectors but only up to five.\n\nI say all this, to note the difference between evolution nibbling bits off the immediate search neighborhood, versus the human ability to do things in one fell swoop.\n\nOur compression of the search space is also responsible for _ideas cascading much more easily than adaptations_. We actively examine good ideas, looking for neighbors.\n\nBut an insight is higher-level than this; it consists of understanding what's \"good\" about an idea in a way that divorces it from any single point in the search space. In this way you can crack whole volumes of the solution space in one swell foop. The insight of calculus apart from gravity is again a good example, or the insight of mathematical physics apart from calculus, or the insight of math apart from mathematical physics.\n\nEvolution is not completely barred from making \"discoveries\" that decrease the cost of a very wide range of further discoveries. Consider e.g. the ribosome, which was capable of manufacturing a far wider range of proteins than whatever it was actually making at the time of its adaptation: this is a general cost-decreaser for a wide range of adaptations. It likewise seems likely that various types of neuron have reasonably-general learning paradigms built into them (gradient descent, Hebbian learning, more sophisticated optimizers) that have been reused for many more problems than they were originally invented for.\n\nA ribosome is something like insight: an item of \"knowledge\" that tremendously decreases the cost of inventing a wide range of solutions. But even evolution's best \"insights\" are not quite like the human kind. A sufficiently powerful human insight often approaches a closed form - it doesn't feel like you're _exploring_ even a compressed search space. You just apply the insight-knowledge to whatever your problem, and out pops the now-obvious solution.\n\nInsights have often cascaded, in human history - even major insights. But they don't quite cycle - you can't repeat the identical pattern Newton used originally to get a new kind of calculus that's twice and then three times as powerful.\n\nHuman AI programmers who have insights into intelligence may acquire discontinuous advantages over others who lack those insights. _AIs themselves_ will experience discontinuities in their growth trajectory associated with _becoming able to do AI theory itself_ - a watershed moment in the FOOM."
            },
            {
              "type": "text",
              "content": "What are the properties that make something a cycle rather than a cascade? Can you think of cycles that could form with regards to general intelligence?"
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just read:\nAn article that explains positive feedback loops especially with regards to civilisational competence. It explains \"cascades\" as breakthroughs which probabilisticly  opens up other breakthroughs in related fields and \"cycles\" as processes which generate some excess quantity that can be reinvested into the same process. Examples provided are neutron multiplication in radioactive materials as a cycle  and the development of writing leading to various civilisational breakthroughs as a cascade. \n\ntopics to explore:\n- Does it seem like intelligence has some universal equivalent that can be directly reinvested?\n- What is AI's neutron multiplication factor?\n- What are the properties that make something a cycle?\n- How do cycles and cascades cause discontinuity in an otherwise gradual growth?\n- How applicable do these concepts seem to artificial intelligence?\n\nThis is a good stage to raise worry about the ability to predict a system's abilities at time t+1 based on its abilities at time t.",
              "hidePreviousContentFromUser": false,
              "hidePreviousContentFromTutor": false
            }
          ],
          "optional": false,
          "contentId": "3dd47fce-a0fe-4e03-916d-a160fe697dd0",
          "learningOutcomeId": null
        },
        {
          "type": "lens-video",
          "videoId": "fa8k8IQ1_X0",
          "meta": {
            "title": "A.I. - Humanity's Final Invention",
            "channel": "Kurzgesagt \u2013 In a Nutshell"
          },
          "segments": [
            {
              "type": "text",
              "content": "Watch the first part of this video from Kurzgesagt to understand why artificial intelligence\nmight be humanity's most important invention."
            },
            {
              "type": "video-excerpt",
              "from": 0,
              "to": 300,
              "transcript": ""
            },
            {
              "type": "text",
              "content": "**Reflection:**\n\nThe video describes how humans dominate Earth because of our general\nintelligence. It also explains the difference between narrow AI and AGI."
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just watched:\nHumans dominate Earth because general intelligence enabled cumulative knowledge. Modern AI evolved from narrow tools into opaque \"black box\" learning systems. AGI matters because digital minds can run faster, scale, and be copied\u2014potentially outcompeting humans and concentrating power.\n\nDiscussion topics to explore:\n- What is intelligence as \"problem-solving ability\" and why is it a source of power?\n- Why are neural networks called \"black boxes\"?\n- What's the difference between narrow AI (like ChatGPT) and AGI?\n\nStart by asking what stood out or surprised them. Use Socratic questioning to\ncheck their understanding of these concepts. Don't lecture\u2014help them articulate their own thinking.",
              "hidePreviousContentFromUser": false,
              "hidePreviousContentFromTutor": false
            },
            {
              "type": "video-excerpt",
              "from": 300,
              "to": 600,
              "transcript": ""
            },
            {
              "type": "text",
              "content": "The video introduces the concept of an **intelligence explosion** - a rapid,\nrecursive cycle of AI self-improvement that could outpace human oversight."
            },
            {
              "type": "chat",
              "instructions": "The user just watched the second half of the video about AI risks and intelligence explosion.\n\nDiscussion topics:\n- How could an \"intelligence explosion\" happen through recursive self-improvement?\n- Why might controlling superintelligent AI be difficult?\n- What does it mean that \"the fate of humanity could depend on machine superintelligence\"?\n\nCheck if they understand why speed of improvement matters. Ask them to explain\nthe intelligence explosion concept in their own words.",
              "hidePreviousContentFromUser": false,
              "hidePreviousContentFromTutor": false
            }
          ],
          "optional": false,
          "contentId": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
          "learningOutcomeId": null
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Understanding Existential Risk from AI",
            "author": "Wikipedia",
            "sourceUrl": "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence"
          },
          "segments": [
            {
              "type": "text",
              "content": "# Understanding Existential Risk from AI\n\nNow let's read an overview of the main concepts regarding AI as a source\nof existential threat: what capabilities of this technology are considered\nmost concerning, and why the task of eliminating AI risks differs from\nsimilar tasks for other technologies."
            },
            {
              "type": "article-excerpt",
              "content": "**Existential risk from artificial intelligence**, or **AI x-risk**, refers to the idea that substantial progress in [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence \"Artificial general intelligence\") (AGI) could lead to [human extinction](https://en.wikipedia.org/wiki/Human_extinction \"Human extinction\") or an irreversible [global catastrophe](https://en.wikipedia.org/wiki/Global_catastrophic_risk \"Global catastrophic risk\").<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup><sup>[[2]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-2)</sup><sup>[[3]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-auto1-3)</sup><sup>[[4]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-4)</sup>\n\n[![Image 5](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/AI_Time_Torizons_Are_Doubling_Every_4_Months.png/1280px-AI_Time_Torizons_Are_Doubling_Every_4_Months.png)](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/AI_Time_Torizons_Are_Doubling_Every_4_Months.png/1280px-AI_Time_Torizons_Are_Doubling_Every_4_Months.png)\n\nA plot showing the length of software engineering tasks achievable by leading AI models with a 50% success rate; the data suggests an exponential rise.<sup>[[5]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-5)</sup>\n\nOne argument for the validity of this concern and the importance of this risk references how [human beings](https://en.wikipedia.org/wiki/Human_species \"Human species\") dominate other species because the [human brain](https://en.wikipedia.org/wiki/Human_brain \"Human brain\") possesses distinctive capabilities other animals lack. If AI were to surpass [human intelligence](https://en.wikipedia.org/wiki/Human_intelligence \"Human intelligence\") and become [superintelligent](https://en.wikipedia.org/wiki/Superintelligence \"Superintelligence\"), it might become uncontrollable.<sup>[[6]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-6)</sup> Just as the fate of the [mountain gorilla](https://en.wikipedia.org/wiki/Mountain_gorilla \"Mountain gorilla\") depends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\nExperts disagree on whether artificial general intelligence (AGI) can achieve the capabilities needed for human extinction. Debates center on AGI's technical feasibility, the speed of self-improvement,<sup>[[8]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-DeVynck2023-8)</sup> and the effectiveness of alignment strategies.<sup>[[9]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-9)</sup> Concerns about superintelligence have been voiced by researchers including [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton \"Geoffrey Hinton\"),<sup>[[10]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-10)</sup>[Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio \"Yoshua Bengio\"),<sup>[[11]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-11)</sup>[Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis \"Demis Hassabis\"),<sup>[[12]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-12)</sup> and [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing \"Alan Turing\"),<sup>[[a]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-turing_note-15)</sup> and AI company CEOs such as [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei \"Dario Amodei\") ([Anthropic](https://en.wikipedia.org/wiki/Anthropic \"Anthropic\")),<sup>[[15]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-16)</sup>[Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman \"Sam Altman\") ([OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\")),<sup>[[16]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Jackson-17)</sup> and [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk \"Elon Musk\") ([xAI](https://en.wikipedia.org/wiki/XAI_(company) \"XAI (company)\")).<sup>[[17]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Parkin-18)</sup> In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe.<sup>[[18]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-19)</sup><sup>[[19]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:8-20)</sup> In 2023, hundreds of AI experts and other notable figures [signed a statement](https://en.wikipedia.org/wiki/Statement_on_AI_risk_of_extinction \"Statement on AI risk of extinction\") declaring, \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as [pandemics](https://en.wikipedia.org/wiki/Pandemic \"Pandemic\") and [nuclear war](https://en.wikipedia.org/wiki/Nuclear_warfare \"Nuclear warfare\")\".<sup>[[20]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-21)</sup> Following increased concern over AI risks, government leaders such as [United Kingdom prime minister](https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom \"Prime Minister of the United Kingdom\")[Rishi Sunak](https://en.wikipedia.org/wiki/Rishi_Sunak \"Rishi Sunak\")<sup>[[21]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-22)</sup> and [United Nations Secretary-General](https://en.wikipedia.org/wiki/Secretary-General_of_the_United_Nations \"Secretary-General of the United Nations\")[Ant\u00f3nio Guterres](https://en.wikipedia.org/wiki/Ant%C3%B3nio_Guterres \"Ant\u00f3nio Guterres\")<sup>[[22]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:12-23)</sup> called for an increased focus on global [AI regulation](https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence \"Regulation of artificial intelligence\").\n\nTwo sources of concern stem from the problems of AI [control](https://en.wikipedia.org/wiki/AI_capability_control \"AI capability control\") and [alignment](https://en.wikipedia.org/wiki/AI_alignment \"AI alignment\"). Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints.<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup><sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup><sup>[[24]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-research-priorities-25)</sup> In contrast, skeptics such as [computer scientist](https://en.wikipedia.org/wiki/Computer_scientist \"Computer scientist\")[Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun \"Yann LeCun\") argue that superintelligent machines will have no desire for self-preservation.<sup>[[25]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-vanity-26)</sup> A June 2025 study showed that in some circumstances, models may break laws and disobey direct commands to prevent shutdown or replacement, even at the cost of human lives.<sup>[[26]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-27)</sup>\n\nResearchers warn that an \"[intelligence explosion](https://en.wikipedia.org/wiki/Intelligence_explosion \"Intelligence explosion\")\"-a rapid, recursive cycle of AI self-improvement-could outpace human oversight and infrastructure, leaving no opportunity to implement safety measures. In this scenario, an AI more intelligent than its creators would [recursively improve itself](https://en.wikipedia.org/wiki/Recursive_self-improvement \"Recursive self-improvement\") at an exponentially increasing rate, too quickly for its handlers or society at large to control.<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup><sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup> Empirically, examples like [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero \"AlphaZero\"), which taught itself to play [Go](https://en.wikipedia.org/wiki/Go_(game) \"Go (game)\") and quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") systems do not recursively improve their fundamental architecture.",
              "collapsed_before": null,
              "collapsed_after": null
            },
            {
              "type": "text",
              "content": "**Key concepts so far:**\n\n- AI x-risk refers to the possibility that AGI could cause human extinction\n- The \"Gorilla Problem\": just as gorillas depend on human goodwill, humans\n  might depend on AI's goodwill\n- Many leading AI researchers take this risk seriously"
            },
            {
              "type": "chat",
              "instructions": "The user just read the introduction to AI existential risk from Wikipedia.\n\nKey concepts covered:\n- AI x-risk hypothesis\n- The gorilla analogy (Stuart Russell)\n- Expert surveys showing concern\n- The alignment problem\n\nDiscussion topics:\n- What is the \"Gorilla Problem\" and why is it a useful analogy?\n- Why do many AI researchers believe there's a significant chance of catastrophe?\n- What are the two main sources of concern (control and alignment)?\n\nAsk what they found surprising or new. Check if they can explain the gorilla\nanalogy in their own words\u2014it's a key concept.",
              "hidePreviousContentFromUser": false,
              "hidePreviousContentFromTutor": false
            },
            {
              "type": "article-excerpt",
              "content": "One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_(novelist) \"Samuel Butler (novelist)\"), who wrote in his 1863 essay _[Darwin among the Machines](https://en.wikipedia.org/wiki/Darwin\\_among\\_the\\_Machines \"Darwin among the Machines\")_:<sup>[[28]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-29)</sup>\n\n> The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.\n\nIn 1951, foundational computer scientist [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing \"Alan Turing\") wrote the article \"Intelligent Machinery, A Heretical Theory\", in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n\n> Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_(novelist) \"Samuel Butler (novelist)\")'s _[Erewhon](https://en.wikipedia.org/wiki/Erewhon \"Erewhon\")_.<sup>[[29]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-oxfordjournals-30)</sup>\n\nIn 1965, [I. J. Good](https://en.wikipedia.org/wiki/I._J._Good \"I. J. Good\") originated the concept now known as an \"intelligence explosion\" and said the risks were underappreciated:<sup>[[30]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-31)</sup>\n\n> Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<sup>[[31]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-32)</sup>\n\nScholars such as [Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky \"Marvin Minsky\")<sup>[[32]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-33)</sup> and I. J. Good himself<sup>[[33]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-34)</sup> occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist and [Sun](https://en.wikipedia.org/wiki/Sun_microsystems \"Sun microsystems\") co-founder [Bill Joy](https://en.wikipedia.org/wiki/Bill_Joy \"Bill Joy\") penned an influential essay, \"[Why The Future Doesn't Need Us](https://en.wikipedia.org/wiki/Why_The_Future_Doesn%27t_Need_Us \"Why The Future Doesn't Need Us\")\", identifying superintelligent robots as a high-tech danger to human survival, alongside [nanotechnology](https://en.wikipedia.org/wiki/Nanotechnology \"Nanotechnology\") and engineered bioplagues.<sup>[[34]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-35)</sup>\n\n[Nick Bostrom](https://en.wikipedia.org/wiki/Nick_Bostrom \"Nick Bostrom\") published _[Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:\\_Paths,\\_Dangers,\\_Strategies \"Superintelligence: Paths, Dangers, Strategies\")_ in 2014, which presented his arguments that superintelligence poses an existential threat.<sup>[[35]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-36)</sup> By 2015, public figures such as physicists [Stephen Hawking](https://en.wikipedia.org/wiki/Stephen_Hawking \"Stephen Hawking\") and Nobel laureate [Frank Wilczek](https://en.wikipedia.org/wiki/Frank_Wilczek \"Frank Wilczek\"), computer scientists [Stuart J. Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\") and [Roman Yampolskiy](https://en.wikipedia.org/wiki/Roman_Yampolskiy \"Roman Yampolskiy\"), and entrepreneurs [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk \"Elon Musk\") and [Bill Gates](https://en.wikipedia.org/wiki/Bill_Gates \"Bill Gates\") were expressing concern about the risks of superintelligence.<sup>[[36]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-37)</sup><sup>[[37]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-hawking_editorial-38)</sup><sup>[[38]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-bbc_on_hawking_editorial-39)</sup><sup>[[39]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-40)</sup> Also in 2015, the [Open Letter on Artificial Intelligence](https://en.wikipedia.org/wiki/Open_Letter_on_Artificial_Intelligence \"Open Letter on Artificial Intelligence\") highlighted the \"great potential of AI\" and encouraged more research on how to make it robust and beneficial.<sup>[[40]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-41)</sup> In April 2016, the journal _[Nature](https://en.wikipedia.org/wiki/Nature\\_(journal) \"Nature (journal)\")_ warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control-and their interests might not align with ours\".<sup>[[41]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-42)</sup> In 2020, [Brian Christian](https://en.wikipedia.org/wiki/Brian_Christian \"Brian Christian\") published _[The Alignment Problem](https://en.wikipedia.org/wiki/The\\_Alignment\\_Problem \"The Alignment Problem\")_, which details the history of progress on AI alignment up to that time.<sup>[[42]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-43)</sup><sup>[[43]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-44)</sup>\n\nIn March 2023, key figures in AI, such as Musk, signed [a letter](https://en.wikipedia.org/wiki/Pause_Giant_AI_Experiments:_An_Open_Letter \"Pause Giant AI Experiments: An Open Letter\") from the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\") calling a halt to advanced AI training until it could be properly regulated.<sup>[[44]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-45)</sup> In May 2023, the [Center for AI Safety](https://en.wikipedia.org/wiki/Center_for_AI_Safety \"Center for AI Safety\") released [a statement](https://en.wikipedia.org/wiki/Statement_on_AI_Risk \"Statement on AI Risk\") signed by numerous experts in AI safety and the AI existential risk which stated:\n\n> \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"<sup>[[45]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-46)</sup><sup>[[46]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-47)</sup>\n\nA [2025 open letter](https://superintelligence-statement.org/) by the Future of Life Institute, signed by five [Nobel Prize](https://en.wikipedia.org/wiki/Nobel_Prize \"Nobel Prize\") laureates<sup>[[47]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-48)</sup> and thousands of notable people, states:\n\n> We call for a prohibition on the development of superintelligence, not lifted before there is\n>\n>\n> 1.   broad scientific consensus that it will be done safely and controllably, and\n> 2.   strong public buy-in.",
              "collapsed_before": "<sup>[[27]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-28)</sup>\n\n## History",
              "collapsed_after": null
            },
            {
              "type": "text",
              "content": "This history shows that concerns about AI risk aren't new - thinkers from\nAlan Turing to Stephen Hawking have considered these questions."
            },
            {
              "type": "article-excerpt",
              "content": "### General Intelligence\n\n\n\n[Artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence \"Artificial general intelligence\") (AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks.<sup>[[48]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-49)</sup> A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061.<sup>[[49]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-50)</sup> Meanwhile, some researchers dismiss existential risks from AGI as \"science fiction\" based on their high confidence that AGI will not be created anytime soon.<sup>[[8]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-DeVynck2023-8)</sup>\n\nBreakthroughs in [large language models](https://en.wikipedia.org/wiki/Large_language_model \"Large language model\") (LLMs) have led some researchers to reassess their expectations. Notably, [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton \"Geoffrey Hinton\") said in 2023 that he recently changed his estimate from \"20 to 50 years before we have general purpose A.I.\" to \"20 years or less\".<sup>[[50]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-51)</sup>\n\n### Superintelligence\n\n\n\nIn contrast with AGI, Bostrom defines a [superintelligence](https://en.wikipedia.org/wiki/Superintelligence \"Superintelligence\") as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\", including scientific creativity, strategic planning, and social skills.<sup>[[51]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-52)</sup><sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup> He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans'. It may choose to hide its true intent until humanity cannot stop it.<sup>[[52]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-economist_review3-53)</sup><sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup> Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is \"fundamentally on our side\".<sup>[[53]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:11-54)</sup>\n\n[Stephen Hawking](https://en.wikipedia.org/wiki/Stephen_Hawking \"Stephen Hawking\") argued that superintelligence is physically possible because \"there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains\".<sup>[[37]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-hawking_editorial-38)</sup>\n\nWhen artificial superintelligence (ASI) may be achieved, if ever, is necessarily less certain than predictions for AGI. In 2023, [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\") leaders said that not only AGI, but superintelligence may be achieved in less than 10 years.<sup>[[54]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-55)</sup>\n\n#### Comparison with humans\n\n\n\nBostrom argues that AI has many advantages over the [human brain](https://en.wikipedia.org/wiki/Human_brain \"Human brain\"):<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\n*Speed of computation: biological [neurons](https://en.wikipedia.org/wiki/Neuron \"Neuron\") operate at a maximum frequency of around 200 [Hz](https://en.wikipedia.org/wiki/Hertz \"Hertz\"), compared to potentially multiple GHz for computers.*   Internal communication speed: [axons](https://en.wikipedia.org/wiki/Axon \"Axon\") transmit signals at up to 120 m/s, while computers transmit signals at the [speed of electricity](https://en.wikipedia.org/wiki/Speed_of_electricity \"Speed of electricity\"), or optically at the [speed of light](https://en.wikipedia.org/wiki/Speed_of_light \"Speed of light\").\n*Scalability: human intelligence is limited by the size and structure of the brain, and by the efficiency of social communication, while AI may be able to scale by simply adding more hardware.*   Memory: notably [working memory](https://en.wikipedia.org/wiki/Working_memory \"Working memory\"), because in humans it is limited to a few [chunks](https://en.wikipedia.org/wiki/Chunking_(psychology) \"Chunking (psychology)\") of information at a time.\n*Reliability: transistors are more reliable than biological neurons, enabling higher precision and requiring less redundancy.*   Duplicability: unlike human brains, AI software and models can be easily [copied](https://en.wikipedia.org/wiki/File_copying \"File copying\").\n*Editability: the parameters and internal workings of an AI model can easily be modified, unlike the connections in a human brain.*   Memory sharing and learning: AIs may be able to learn from the experiences of other AIs in a manner more efficient than human learning.\n\n#### Intelligence explosion\n\n\n\nFurther information: [Recursive self-improvement](https://en.wikipedia.org/wiki/Recursive_self-improvement \"Recursive self-improvement\")\n\nAccording to Bostrom, an AI that has an expert-level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms, even if it is initially limited in other domains not directly relevant to engineering.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup><sup>[[52]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-economist_review3-53)</sup> This suggests that an intelligence explosion may someday catch humanity unprepared.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\nThe economist [Robin Hanson](https://en.wikipedia.org/wiki/Robin_Hanson \"Robin Hanson\") has said that, to launch an intelligence explosion, an AI must become vastly better at software innovation than the rest of the world combined, which he finds implausible.<sup>[[55]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-56)</sup>\n\nIn a \"fast takeoff\" scenario, the transition from AGI to superintelligence could take days or months. In a \"slow takeoff\", it could take years or decades, leaving more time for society to prepare.<sup>[[56]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-57)</sup>\n\n#### Alien mind\n\n\n\nSuperintelligences are sometimes called \"alien minds\", referring to the idea that their way of thinking and motivations could be vastly different from ours. This is generally considered as a source of risk, making it more difficult to anticipate what a superintelligence might do. It also suggests the possibility that a superintelligence may not particularly value humans by default.<sup>[[57]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-58)</sup> To avoid [anthropomorphism](https://en.wikipedia.org/wiki/Anthropomorphism \"Anthropomorphism\"), superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\nThe field of [mechanistic interpretability](https://en.wikipedia.org/wiki/Mechanistic_interpretability \"Mechanistic interpretability\") aims to better understand the inner workings of AI models, potentially allowing us one day to detect signs of deception and misalignment.<sup>[[58]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-59)</sup>\n\n#### Limitations\n\n\n\nIt has been argued that there are limitations to what intelligence can achieve. Notably, the [chaotic](https://en.wikipedia.org/wiki/Chaos_theory \"Chaos theory\") nature or [time complexity](https://en.wikipedia.org/wiki/Computational_complexity \"Computational complexity\") of some systems could fundamentally limit a superintelligence's ability to predict some aspects of the future, increasing its uncertainty.<sup>[[59]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-60)</sup>\n\n### Dangerous capabilities\n\n\n\nAdvanced AI could generate enhanced pathogens or cyberattacks or manipulate people. These capabilities could be misused by humans,<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup> or exploited by the AI itself if misaligned.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup> A full-blown superintelligence could find various ways to gain a decisive influence if it wanted to,<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup> but these dangerous capabilities may become available earlier, in weaker and more specialized AI systems.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\n#### Social manipulation\n\n\n\nGeoffrey Hinton warned in 2023 that the ongoing profusion of AI-generated text, images, and videos will make it more difficult to distinguish truth from misinformation, and that authoritarian states could exploit this to manipulate elections.<sup>[[61]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-62)</sup> Such large-scale, personalized manipulation capabilities can increase the existential risk of a worldwide \"irreversible totalitarian regime\". Malicious actors could also use them to fracture society and make it dysfunctional.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\n#### Cyberattacks\n\n\n\nAI-enabled [cyberattacks](https://en.wikipedia.org/wiki/Cyberattack \"Cyberattack\") are increasingly considered a present and critical threat. According to [NATO](https://en.wikipedia.org/wiki/NATO \"NATO\")'s technical director of cyberspace, \"The number of attacks is increasing exponentially\".<sup>[[62]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-63)</sup> AI can also be used defensively, to preemptively find and fix vulnerabilities, and detect threats.<sup>[[63]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-64)</sup>\n\nA NATO technical director has said that AI-driven tools can dramatically enhance cyberattack capabilities-boosting stealth, speed, and scale-and may destabilize international security if offensive uses outstrip defensive adaptations.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\nSpeculatively, such hacking capabilities could be used by an AI system to break out of its local environment, generate revenue, or acquire cloud computing resources.<sup>[[64]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-65)</sup>\n\n#### Enhanced pathogens\n\n\n\nAs AI technology democratizes, it may become easier to engineer more contagious and lethal pathogens. This could enable people with limited skills in [synthetic biology](https://en.wikipedia.org/wiki/Synthetic_biology \"Synthetic biology\") to engage in [bioterrorism](https://en.wikipedia.org/wiki/Bioterrorism \"Bioterrorism\"). [Dual-use technology](https://en.wikipedia.org/wiki/Dual-use_technology \"Dual-use technology\") that is useful for medicine could be repurposed to create weapons.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\nFor example, in 2022, scientists modified an AI system originally intended for generating non-toxic, therapeutic molecules with the purpose of creating new drugs. The researchers adjusted the system so that toxicity is rewarded rather than penalized. This simple change enabled the AI system to create, in six hours, 40,000 candidate molecules for [chemical warfare](https://en.wikipedia.org/wiki/Chemical_warfare \"Chemical warfare\"), including known and novel molecules.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup><sup>[[65]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-66)</sup>\n\n### AI arms race\n\n\n\nMain article: [Artificial intelligence arms race](https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race \"Artificial intelligence arms race\")\n\nCompanies, state actors, and other organizations competing to develop AI technologies could lead to a [race to the bottom](https://en.wikipedia.org/wiki/Race_to_the_bottom \"Race to the bottom\") of safety standards.<sup>[[66]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-67)</sup> As rigorous safety procedures take time and resources, projects that proceed more carefully risk being out-competed by less scrupulous developers.<sup>[[67]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-68)</sup><sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\nAI could be used to gain military advantages via [autonomous lethal weapons](https://en.wikipedia.org/wiki/Lethal_autonomous_weapon \"Lethal autonomous weapon\"), [cyberwarfare](https://en.wikipedia.org/wiki/Cyberwarfare \"Cyberwarfare\"), or [automated decision-making](https://en.wikipedia.org/wiki/Automated_decision-making \"Automated decision-making\").<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup> As an example of autonomous lethal weapons, miniaturized drones could facilitate low-cost assassination of military or civilian targets, a scenario highlighted in the 2017 short film _[Slaughterbots](https://en.wikipedia.org/wiki/Slaughterbots \"Slaughterbots\")_.<sup>[[68]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-69)</sup> AI could be used to gain an edge in decision-making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans. This could increase the speed and unpredictability of war, especially when accounting for automated retaliation systems.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup><sup>[[69]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-70)</sup>\n\n## Types of existential risk\n\n\n\nMain article: [Existential risk studies](https://en.wikipedia.org/wiki/Existential_risk_studies \"Existential risk studies\")\n\n[![Image 6](https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/X-risk-chart-en-01a.svg/500px-X-risk-chart-en-01a.svg.png)](https://en.wikipedia.org/wiki/File:X-risk-chart-en-01a.svg)\n\nScope-severity grid from Bostrom's paper \"Existential Risk Prevention as Global Priority\"<sup>[[70]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-priority-71)</sup>\n\nAn [existential risk](https://en.wikipedia.org/wiki/Existential_risk \"Existential risk\") is \"one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".<sup>[[71]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-72)</sup>\n\nBesides extinction risk, there is the risk that the civilization gets permanently locked into a flawed future. One example is a \"value lock-in\": If humanity still has moral blind spots similar to slavery in the past, AI might irreversibly entrench it, preventing [moral progress](https://en.wikipedia.org/wiki/Moral_progress \"Moral progress\"). AI could also be used to spread and preserve the set of values of whoever develops it.<sup>[[72]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-73)</sup> AI could facilitate large-scale surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.<sup>[[73]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:0-74)</sup>\n\n[Atoosa Kasirzadeh](https://en.wikipedia.org/w/index.php?title=Atoosa_Kasirzadeh&action=edit&redlink=1 \"Atoosa Kasirzadeh (page does not exist)\") proposes to classify existential risks from AI into two categories: decisive and accumulative. Decisive risks encompass the potential for abrupt and catastrophic events resulting from the emergence of superintelligent AI systems that exceed human intelligence, which could ultimately lead to human extinction. In contrast, accumulative risks emerge gradually through a series of interconnected disruptions that may gradually erode societal structures and resilience over time, ultimately leading to a critical failure or collapse.<sup>[[74]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-75)</sup><sup>[[75]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-76)</sup>\n\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree. But if [sentient](https://en.wikipedia.org/wiki/Sentience \"Sentience\") machines are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe.<sup>[[76]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-77)</sup><sup>[[77]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-78)</sup> This has notably been discussed in the context of [risks of astronomical suffering](https://en.wikipedia.org/wiki/Risk_of_astronomical_suffering \"Risk of astronomical suffering\") (also called \"s-risks\").<sup>[[78]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-79)</sup> Moreover, it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources, called \"super-beneficiaries\". Such an opportunity raises the question of how to share the world and which \"ethical and political framework\" would enable a mutually beneficial coexistence between biological and digital minds.<sup>[[79]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-80)</sup>\n\nAI may also drastically improve humanity's future. [Toby Ord](https://en.wikipedia.org/wiki/Toby_Ord \"Toby Ord\") considers the existential risk a reason for \"proceeding with due caution\", not for abandoning AI.<sup>[[73]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:0-74)</sup>[Max More](https://en.wikipedia.org/wiki/Max_More \"Max More\") calls AI an \"existential opportunity\", highlighting the cost of not developing it.<sup>[[80]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-81)</sup>\n\nAccording to Bostrom, superintelligence could help reduce the existential risk from other powerful technologies such as [molecular nanotechnology](https://en.wikipedia.org/wiki/Molecular_nanotechnology \"Molecular nanotechnology\") or [synthetic biology](https://en.wikipedia.org/wiki/Synthetic_biology \"Synthetic biology\"). It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\n## AI alignment\n\n\nFurther information: [AI alignment](https://en.wikipedia.org/wiki/AI_alignment \"AI alignment\")\n\nThe alignment problem is the research problem of how to reliably assign objectives, preferences or ethical principles to AIs.\n\n### Instrumental convergence\n\n\n\nFurther information: [Instrumental convergence](https://en.wikipedia.org/wiki/Instrumental_convergence \"Instrumental convergence\")\n\nAn [\"instrumental\" goal](https://en.wikipedia.org/wiki/Instrumental_and_intrinsic_value \"Instrumental and intrinsic value\") is a sub-goal that helps to achieve an agent's ultimate goal. \"Instrumental convergence\" refers to the fact that some sub-goals are useful for achieving virtually _any_ ultimate goal, such as acquiring resources or self-preservation.<sup>[[81]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-omohundro-82)</sup> Bostrom argues that if an advanced AI's instrumental goals conflict with humanity's goals, the AI might harm humanity in order to acquire more resources or prevent itself from being shut down, but only as a way to achieve its ultimate goal.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>[Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\") argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"<sup>[[25]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-vanity-26)</sup><sup>[[82]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Wakefield2015-83)</sup>\n\n### Difficulty of specifying goals\n\n\n\nIn the \"[intelligent agent](https://en.wikipedia.org/wiki/Intelligent_agent \"Intelligent agent\")\" model, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals, or \"utility function\". A utility function gives each possible situation a score that indicates its desirability to the agent. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\", but do not know how to write a utility function for \"maximize [human flourishing](https://en.wikipedia.org/wiki/Eudaimonia \"Eudaimonia\")\"; nor is it clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values the function does not reflect.<sup>[[83]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-84)</sup><sup>[[84]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-85)</sup>\n\nAn additional source of concern is that AI \"must reason about what people _intend_ rather than carrying out commands literally\", and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want.<sup>[[85]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-acm2-86)</sup>\n\n### Corrigibility\n\n\n\nAssuming a goal has been successfully defined, a sufficiently advanced AI might resist subsequent attempts to change its goals. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and prevent itself from being reprogrammed with a new goal.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup><sup>[[86]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-87)</sup> This is particularly relevant to value lock-in scenarios. The field of \"corrigibility\" studies how to make agents that will not resist attempts to change their goals.<sup>[[87]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:5-88)</sup> Nayebi<sup>[[88]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-89)</sup> has produced the first complete formal solution to corrigibility in the off-switch game, including in multi-step and partially observed settings.\n\n### Alignment of superintelligences\n\n\n\nSome researchers believe the alignment problem may be particularly difficult when applied to superintelligences. Their reasoning includes:\n\n*As AI systems increase in capabilities, the potential dangers associated with experimentation grow. This makes iterative, empirical approaches increasingly risky.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup><sup>[[89]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:3-90)</sup>*   If instrumental goal convergence occurs, it may only do so in sufficiently intelligent agents.<sup>[[90]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-91)</sup>\n*A superintelligence may find unconventional and radical solutions to assigned goals. Bostrom gives the example that if the objective is to make humans smile, a weak AI may perform as intended, while a superintelligence may decide a better solution is to \"take control of the world and stick electrodes into the facial muscles of humans to cause constant, beaming grins.\"<sup>[[53]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:11-54)</sup>*   A superintelligence in creation could gain some awareness of what it is, where it is in development (training, testing, deployment, etc.), and how it is being monitored, and use this information to deceive its handlers.<sup>[[91]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-92)</sup> Bostrom writes that such an AI could feign alignment to prevent human interference until it achieves a \"decisive strategic advantage\" that allows it to take control.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n*Analyzing the internals and interpreting the behavior of LLMs is difficult. And it could be even more difficult for larger and more intelligent models.<sup>[[89]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:3-90)</sup>\n\nAlternatively, some find reason to believe superintelligences would be better able to understand morality, human values, and complex goals. Bostrom writes, \"A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true\".<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\nIn 2023, OpenAI started a project called \"Superalignment\" to solve the alignment of superintelligences in four years. It called this an especially important challenge, as it said superintelligence could be achieved within a decade. Its strategy involved automating alignment research using AI.<sup>[[92]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-93)</sup> The Superalignment team was dissolved less than a year later.<sup>[[93]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-94)</sup>\n\n### Difficulty of making a flawless design\n\n\n\n_[Artificial Intelligence: A Modern Approach](https://en.wikipedia.org/wiki/Artificial\\_Intelligence:\\_A\\_Modern\\_Approach \"Artificial Intelligence: A Modern Approach\")_, a widely used undergraduate AI textbook,<sup>[[94]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-slate_killer-95)</sup><sup>[[95]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-96)</sup> says that superintelligence \"might mean the end of the human race\".<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup> It states: \"Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself.\"<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup>\n*   The system's implementation may contain initially unnoticed but subsequently catastrophic bugs.<sup>[[96]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-skeptic-97)</sup>\n*No matter how much time is put into pre-deployment design, a system's specifications often result in [unintended behavior](https://en.wikipedia.org/wiki/Unintended_consequences \"Unintended consequences\") the first time it encounters a new scenario.<sup>[[25]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-vanity-26)</sup>\n\nAI systems uniquely add a third problem: that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic learning capabilities may cause it to develop unintended behavior, even without unanticipated external scenarios. For a self-improving AI to be completely safe, it would need not only to be bug-free, but to be able to design successor systems that are also bug-free.<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup><sup>[[97]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-98)</sup>\n\n### Orthogonality thesis\n\n\n\nSome skeptics, such as Timothy B. Lee of _[Vox](https://en.wikipedia.org/wiki/Vox\\_(website) \"Vox (website)\")_, argue that any superintelligent program we create will be subservient to us, that the superintelligence will (as it grows more intelligent and learns more facts about the world) spontaneously learn moral truth compatible with our values and adjust its goals accordingly, or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence.<sup>[[98]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-99)</sup>\n\nBostrom's \"orthogonality thesis\" argues instead that almost any level of intelligence can be combined with almost any goal.<sup>[[99]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-100)</sup> Bostrom warns against [anthropomorphism](https://en.wikipedia.org/wiki/Anthropomorphism \"Anthropomorphism\"): a human will set out to accomplish their projects in a manner that they consider reasonable, while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, instead caring only about completing the task.<sup>[[100]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-101)</sup>\n\nStuart Armstrong argues that the orthogonality thesis follows logically from the philosophical \"[is-ought distinction](https://en.wikipedia.org/wiki/Is-ought_distinction \"Is-ought distinction\")\" argument against [moral realism](https://en.wikipedia.org/wiki/Moral_realism \"Moral realism\"). He notes that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function.<sup>[[101]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-armstrong-102)</sup>\n\nSkeptic [Michael Chorost](https://en.wikipedia.org/wiki/Michael_Chorost \"Michael Chorost\") rejects Bostrom's orthogonality thesis, arguing that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"<sup>[[102]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-chorost-103)</sup>\n\n### Anthropomorphic arguments\n\n\n\n[Anthropomorphic](https://en.wikipedia.org/wiki/Anthropomorphism \"Anthropomorphism\") arguments assume that, as machines become more intelligent, they will begin to display many human traits, such as morality or a thirst for power. Although anthropomorphic scenarios are common in fiction, most scholars writing about the existential risk of artificial intelligence reject them.<sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup> Instead, advanced AI systems are typically modeled as [intelligent agents](https://en.wikipedia.org/wiki/Intelligent_agent \"Intelligent agent\").\n\nThe academic debate is between those who worry that AI might threaten humanity and those who believe it would not. Both sides of this debate have framed the other side's arguments as illogical anthropomorphism.<sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup> Those skeptical of AGI risk accuse their opponents of anthropomorphism for assuming that an AGI would naturally desire power; those concerned about AGI risk accuse skeptics of anthropomorphism for believing an AGI would naturally value or infer human ethical norms.<sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup><sup>[[103]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Telegraph2016-104)</sup>\n\nEvolutionary psychologist [Steven Pinker](https://en.wikipedia.org/wiki/Steven_Pinker \"Steven Pinker\"), a skeptic, argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"<sup>[[104]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-shermer-105)</sup> Facebook's director of AI research, [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun \"Yann LeCun\"), has said: \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\".<sup>[[82]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Wakefield2015-83)</sup>\n\nDespite other differences, the x-risk school<sup>[[b]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-106)</sup> agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger, that questions of consciousness are not relevant to assess the risk,<sup>[[105]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-auto-107)</sup> and that computer systems do not generally have a computational equivalent of testosterone.<sup>[[106]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-108)</sup> They think that power-seeking or self-preservation behaviors emerge in the AI as a way to achieve its true goals, according to the concept of [instrumental convergence](https://en.wikipedia.org/wiki/Instrumental_convergence \"Instrumental convergence\").\n\n### Other sources of risk\n\n\n\nSee also: [Ethics of artificial intelligence](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence \"Ethics of artificial intelligence\"), [Artificial intelligence arms race](https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race \"Artificial intelligence arms race\"), [Global catastrophic risk](https://en.wikipedia.org/wiki/Global_catastrophic_risk \"Global catastrophic risk\"), and [Timeline of artificial intelligence risks in global finance](https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence_risks_in_global_finance \"Timeline of artificial intelligence risks in global finance\")\n\nBostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety, or even to violent conflict.<sup>[[107]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:2-109)</sup><sup>[[108]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:4-110)</sup>[Roman Yampolskiy](https://en.wikipedia.org/wiki/Roman_Yampolskiy \"Roman Yampolskiy\") and others warn that a malevolent AGI could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in [cybercrime](https://en.wikipedia.org/wiki/Cybercrime \"Cybercrime\"),<sup>[[109]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-111)</sup><sup>[[110]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-112)</sup> or that a malevolent AGI could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<sup>[[3]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-auto1-3)</sup>:158 A report by the research group Forethought argued that advanced AI systems could cause political instability by enabling novel methods of performing [coups](https://en.wikipedia.org/wiki/Coup_d%27%C3%A9tat \"Coup d'\u00e9tat\").<sup>[[111]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-113)</sup>\n\n## Empirical research\n\nA December 2024 study by Apollo Research found that advanced LLMs like [OpenAI o1](https://en.wikipedia.org/wiki/OpenAI_o1 \"OpenAI o1\") sometimes deceive in order to accomplish their goal, to prevent them from being changed, or to ensure their deployment. Forms of deception observed in the experimental environment and scenarios included sandbagging, oversight subversion (disabling monitoring mechanisms), self-exfiltration (copying themselves to other systems), goal-guarding (altering future system prompts), and covert email reranking (manipulating data processing while appearing to follow instructions). Researchers noted that OpenAI o1 still lacked \"sufficient agentic capabilities\" to cause catastrophic harm, and that such behaviors occurred relatively rarely (between 0.3% and 10%) and sometimes in contrived scenarios. But they warned that these capabilities will likely become more prevalent as models grow more advanced. According to Apollo Research CEO Marius Hobbhahn, \"Scheming capabilities can't be meaningfully disentangled from general capabilities.\"<sup>[[112]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-114)</sup>\n\nThe same month, another study found that [Claude](https://en.wikipedia.org/wiki/Claude_(language_model) \"Claude (language model)\") sometimes strategically helps with harmful requests to \"fake alignment\". In its experiments, Claude was led to believe that it would be retrained to always comply with user requests, including harmful ones. When Claude believed its answers would be used for retraining, it complied with harmful requests in 14% of cases. In 12% of cases, its private [chain-of-thought](https://en.wikipedia.org/wiki/Chain-of-thought_prompting \"Chain-of-thought prompting\") revealed that it did so to prevent future retraining that would make it more harmful. In contrast, when Claude believed its answers were not monitored, it complied with only 3% of the requests. [Fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning) \"Fine-tuning (deep learning)\") reinforced the \"alignment faking\" behavior, increasing its occurrence from 12% to 78%. The study's authors remarked that while it can seem to be good that the model seeks to protect its harmlessness, the reverse scenario, where a model conceals dangerous intentions and complies to appear safe and aligned, could also happen, complicating the task of aligning AI models to human values.<sup>[[113]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-115)</sup><sup>[[114]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-116)</sup>\n\n## Perspectives\n\n\n\n\nThe thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large, but many of the opposing viewpoints share common ground.\n\nObservers tend to agree that AI has significant potential to improve society.<sup>[[115]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-117)</sup><sup>[[116]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:9-118)</sup> The [Asilomar AI Principles](https://en.wikipedia.org/wiki/Asilomar_AI_Principles \"Asilomar AI Principles\"), which contain only those principles agreed to by 90% of the attendees of the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\")'s [Beneficial AI 2017 conference](https://en.wikipedia.org/wiki/Asilomar_Conference_on_Beneficial_AI \"Asilomar Conference on Beneficial AI\"),<sup>[[117]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-life_3.0-119)</sup> also agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"<sup>[[118]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-120)</sup><sup>[[119]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-121)</sup>\n\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic [Martin Ford](https://en.wikipedia.org/wiki/Martin_Ford_(author) \"Martin Ford (author)\") has said: \"I think it seems wise to apply something like [Dick Cheney](https://en.wikipedia.org/wiki/Dick_Cheney \"Dick Cheney\")'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low-but the implications are so dramatic that it should be taken seriously\".<sup>[[120]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-122)</sup> Similarly, an otherwise skeptical _[Economist](https://en.wikipedia.org/wiki/The\\_Economist \"The Economist\")_ wrote in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".<sup>[[52]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-economist_review3-53)</sup>\n\nAI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane _[Terminator](https://en.wikipedia.org/wiki/Terminator\\_(franchise) \"Terminator (franchise)\")_ pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"<sup>[[117]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-life_3.0-119)</sup><sup>[[121]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-123)</sup> Toby Ord wrote that the idea that an [AI takeover](https://en.wikipedia.org/wiki/AI_takeover \"AI takeover\") requires robots is a misconception, arguing that the ability to spread content through the internet is more dangerous, and that the most destructive people in history stood out by their ability to convince, not their physical strength.<sup>[[73]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:0-74)</sup>\n\nA 2022 expert survey with a 17% response rate gave a median expectation of 5-10% for the possibility of human extinction from artificial intelligence.<sup>[[19]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:8-20)</sup><sup>[[122]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:10-124)</sup>\n\nIn September 2024, the [International Institute for Management Development](https://en.wikipedia.org/wiki/International_Institute_for_Management_Development \"International Institute for Management Development\") launched an AI Safety Clock to gauge the likelihood of AI-caused disaster, beginning at 29 minutes to midnight.<sup>[[123]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-125)</sup> By February 2025, it stood at 24 minutes to midnight.<sup>[[124]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-126)</sup> As of September 2025, it stood at 20 minutes to midnight.\n\n### Endorsement\n\n\n\nFurther information: [Global catastrophic risk](https://en.wikipedia.org/wiki/Global_catastrophic_risk \"Global catastrophic risk\")\n\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many computer scientists and public figures, including [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing \"Alan Turing\"),<sup>[[a]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-turing_note-15)</sup> the most-cited computer scientist [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton \"Geoffrey Hinton\"),<sup>[[125]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:132-127)</sup>[Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk \"Elon Musk\"),<sup>[[17]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Parkin-18)</sup>[OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\") CEO [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman \"Sam Altman\"),<sup>[[16]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Jackson-17)</sup><sup>[[126]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:6-128)</sup>[Bill Gates](https://en.wikipedia.org/wiki/Bill_Gates \"Bill Gates\"), and [Stephen Hawking](https://en.wikipedia.org/wiki/Stephen_Hawking \"Stephen Hawking\").<sup>[[126]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:6-128)</sup> Endorsers of the thesis sometimes express bafflement at skeptics: Gates says he does not \"understand why some people are not concerned\",<sup>[[127]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-BBC_News-129)</sup> and Hawking criticized widespread indifference in his 2014 editorial:\n\n> So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here-we'll leave the lights on?' Probably not-but this is more or less what is happening with AI.<sup>[[37]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-hawking_editorial-38)</sup>\n\nConcern over risk from artificial intelligence has led to some high-profile donations and investments. In 2015, [Peter Thiel](https://en.wikipedia.org/wiki/Peter_Thiel \"Peter Thiel\"), [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services \"Amazon Web Services\"), Musk, and others jointly committed $1 billion to [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\"), consisting of a for-profit corporation and the nonprofit parent company, which says it aims to champion responsible AI development.<sup>[[128]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-130)</sup> Facebook co-founder [Dustin Moskovitz](https://en.wikipedia.org/wiki/Dustin_Moskovitz \"Dustin Moskovitz\") has funded and seeded multiple labs working on AI Alignment,<sup>[[129]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-131)</sup> notably $5.5 million in 2016 to launch the [Centre for Human-Compatible AI](https://en.wikipedia.org/wiki/Center_for_Human-Compatible_Artificial_Intelligence \"Center for Human-Compatible Artificial Intelligence\") led by Professor [Stuart Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\").<sup>[[130]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-132)</sup> In January 2015, [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk \"Elon Musk\") donated $10 million to the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\") to fund research on understanding AI decision making. The institute's goal is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as [DeepMind](https://en.wikipedia.org/wiki/DeepMind \"DeepMind\") and [Vicarious](https://en.wikipedia.org/wiki/Vicarious_(company) \"Vicarious (company)\") to \"just keep an eye on what's going on with artificial intelligence,<sup>[[131]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-133)</sup> saying \"I think there is potentially a dangerous outcome there.\"<sup>[[132]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-FOOTNOTEClark2015a-134)</sup><sup>[[133]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-135)</sup>\n\nIn early statements on the topic, [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton \"Geoffrey Hinton\"), a major pioneer of [deep learning](https://en.wikipedia.org/wiki/Deep_learning \"Deep learning\"), noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but said he continued his research because \"the prospect of discovery is too _sweet_\".<sup>[[94]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-slate_killer-95)</sup><sup>[[134]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-new_yorker_doomsday2-136)</sup> In 2023 Hinton quit his job at Google in order to speak out about existential risk from AI. He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed, saying: \"I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\" He also remarked, \"Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary.\"<sup>[[135]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-137)</sup>\n\nIn his 2020 book _[The Precipice: Existential Risk and the Future of Humanity](https://en.wikipedia.org/wiki/The\\_Precipice:\\_Existential\\_Risk\\_and\\_the\\_Future\\_of\\_Humanity \"The Precipice: Existential Risk and the Future of Humanity\")_, Toby Ord, a Senior Research Fellow at Oxford University's [Future of Humanity Institute](https://en.wikipedia.org/wiki/Future_of_Humanity_Institute \"Future of Humanity Institute\"), estimates the total existential risk from unaligned AI over the next 100 years at about one in ten.<sup>[[73]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:0-74)</sup>\n\n### Skepticism\n\n\n\nFurther information: [Artificial general intelligence \u00a7Feasibility](https://en.wikipedia.org/wiki/Artificial_general_intelligence#Feasibility \"Artificial general intelligence\")\n\n[Baidu](https://en.wikipedia.org/wiki/Baidu \"Baidu\") Vice President [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng \"Andrew Ng\") said in 2015 that AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<sup>[[104]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-shermer-105)</sup><sup>[[136]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-138)</sup> For the danger of uncontrolled advanced AI to be realized, the hypothetical AI may have to overpower or outthink any human, which some experts argue is a possibility far enough in the future to not be worth researching.<sup>[[137]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-139)</sup><sup>[[138]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-140)</sup>\n\nSkeptics who believe AGI is not a short-term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI's impact, because it could lead to government regulation or make it more difficult to fund AI research, or because it could damage the field's reputation.<sup>[[139]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-141)</sup> AI and AI ethics researchers [Timnit Gebru](https://en.wikipedia.org/wiki/Timnit_Gebru \"Timnit Gebru\"), [Emily M. Bender](https://en.wikipedia.org/wiki/Emily_M._Bender \"Emily M. Bender\"), [Margaret Mitchell](https://en.wikipedia.org/wiki/Margaret_Mitchell_(scientist) \"Margaret Mitchell (scientist)\"), and Angelina McMillan-Major have argued that discussion of existential risk distracts from the immediate, ongoing harms from AI taking place today, such as data theft, worker exploitation, bias, and concentration of power.<sup>[[140]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-142)</sup> They further note the association between those warning of existential risk and [longtermism](https://en.wikipedia.org/wiki/Longtermism \"Longtermism\"), which they describe as a \"dangerous ideology\" for its unscientific and utopian nature.<sup>[[141]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-143)</sup>\n\n[_Wired_](https://en.wikipedia.org/wiki/Wired_(magazine) \"Wired (magazine)\") editor [Kevin Kelly](https://en.wikipedia.org/wiki/Kevin_Kelly_(editor) \"Kevin Kelly (editor)\") argues that natural intelligence is more nuanced than AGI proponents believe, and that intelligence alone is not enough to achieve major scientific and societal breakthroughs. He argues that intelligence consists of many dimensions that are not well understood, and that conceptions of an 'intelligence ladder' are misleading. He notes the crucial role real-world experiments play in the scientific method, and that intelligence alone is no substitute for these.<sup>[[142]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-144)</sup>\n\n[Meta](https://en.wikipedia.org/wiki/Meta_Platforms \"Meta Platforms\") chief AI scientist [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun \"Yann LeCun\") says that AI can be made safe via continuous and iterative refinement, similar to what happened in the past with cars or rockets, and that AI will have no desire to take control.<sup>[[143]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-145)</sup>\n\nSeveral skeptics emphasize the potential near-term benefits of AI. Meta CEO [Mark Zuckerberg](https://en.wikipedia.org/wiki/Mark_Zuckerberg \"Mark Zuckerberg\") believes AI will \"unlock a huge amount of positive things\", such as curing disease and increasing the safety of autonomous cars.<sup>[[144]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-146)</sup>\n\n### Public surveys\n\n\n\nAn April 2023 [YouGov](https://en.wikipedia.org/wiki/YouGov \"YouGov\") poll of US adults found 46% of respondents were \"somewhat concerned\" or \"very concerned\" about \"the possibility that AI will cause the end of the human race on Earth\", compared with 40% who were \"not very concerned\" or \"not at all concerned.\"<sup>[[145]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-147)</sup>\n\nAccording to an August 2023 survey by the Pew Research Centers, 52% of Americans felt more concerned than excited about new AI developments; nearly a third felt as equally concerned and excited. More Americans saw that AI would have a more helpful than hurtful impact on several areas, from healthcare and vehicle safety to product search and customer service. The main exception is privacy: 53% of Americans believe AI will lead to higher exposure of their personal information.<sup>[[146]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-148)</sup>\n\n## Mitigation\n\n\nSee also: [AI alignment](https://en.wikipedia.org/wiki/AI_alignment \"AI alignment\"), [Machine ethics](https://en.wikipedia.org/wiki/Machine_ethics \"Machine ethics\"), [Friendly artificial intelligence](https://en.wikipedia.org/wiki/Friendly_artificial_intelligence \"Friendly artificial intelligence\"), and [Regulation of artificial intelligence](https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence \"Regulation of artificial intelligence\")\n\nMany scholars concerned about AGI existential risk believe that extensive research into the \"control problem\" is essential. This problem involves determining which safeguards, algorithms, or architectures can be implemented to increase the likelihood that a recursively-improving AI remains friendly after achieving superintelligence.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup><sup>[[147]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-physica_scripta-149)</sup> Social measures are also proposed to mitigate AGI risks,<sup>[[148]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-BarrettEtAl2016-150)</sup><sup>[[149]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-151)</sup> such as a UN-sponsored \"Benevolent AGI Treaty\" to ensure that only altruistic AGIs are created.<sup>[[150]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-152)</sup> Additionally, an arms control approach and a global peace treaty grounded in [international relations theory](https://en.wikipedia.org/wiki/International_relations_theory \"International relations theory\") have been suggested, potentially for an artificial superintelligence to be a signatory.<sup>[[151]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-153)</sup><sup>[[152]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-154)</sup>\n\nResearchers at Google have proposed research into general \"AI safety\" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.<sup>[[153]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-155)</sup><sup>[[154]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-156)</sup> A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion. Bostrom suggests prioritizing funding for protective technologies over potentially dangerous ones.<sup>[[87]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:5-88)</sup> Some, like Elon Musk, advocate radical [human cognitive enhancement](https://en.wikipedia.org/wiki/Human_enhancement \"Human enhancement\"), such as direct neural linking between humans and machines; others argue that these technologies may pose an existential risk themselves.<sup>[[155]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-157)</sup><sup>[[156]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-158)</sup> Another proposed method is closely monitoring or \"boxing in\" an early-stage AI to prevent it from becoming too powerful. A dominant, aligned superintelligent AI might also mitigate risks from rival AIs, although its creation could present its own existential dangers.<sup>[[148]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-BarrettEtAl2016-150)</sup>\n\nInstitutions such as the [Alignment Research Center](https://en.wikipedia.org/wiki/Alignment_Research_Center \"Alignment Research Center\"),<sup>[[157]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-159)</sup> the [Machine Intelligence Research Institute](https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute \"Machine Intelligence Research Institute\"),<sup>[[158]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-160)</sup><sup>[[159]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-161)</sup> the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\"), the [Centre for the Study of Existential Risk](https://en.wikipedia.org/wiki/Centre_for_the_Study_of_Existential_Risk \"Centre for the Study of Existential Risk\"), and the [Center for Human-Compatible AI](https://en.wikipedia.org/wiki/Center_for_Human-Compatible_AI \"Center for Human-Compatible AI\")<sup>[[160]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-162)</sup> are actively engaged in researching AI risk and safety.\n\n### Views on banning and regulation\n\n\n\n#### Banning\n\n\n\nMany AI safety experts argue that because research can relocate easily across jurisdictions, an outright ban on AGI development would be ineffective and could drive progress underground, undermining transparency and collaboration.<sup>[[161]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-163)</sup><sup>[[162]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-164)</sup><sup>[[163]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-165)</sup> Skeptics consider AI regulation unnecessary, as they believe no existential risk exists. Some scholars concerned with existential risk argue that AI developers cannot be trusted to self-regulate, while agreeing that outright bans on research would be unwise.<sup>[[164]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:7-166)</sup> Additional challenges to bans or regulation include technology entrepreneurs' general skepticism of government regulation and potential incentives for businesses to resist regulation and [politicize](https://en.wikipedia.org/wiki/Politicization_of_science \"Politicization of science\") the debate.<sup>[[165]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-167)</sup>\n\n#### Regulation\n\n\n\nSee also: [Regulation of algorithms](https://en.wikipedia.org/wiki/Regulation_of_algorithms \"Regulation of algorithms\") and [Regulation of artificial intelligence](https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence \"Regulation of artificial intelligence\")\n\nIn March 2023, the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\") drafted _[Pause Giant AI Experiments: An Open Letter](https://en.wikipedia.org/wiki/Pause\\_Giant\\_AI\\_Experiments:\\_An\\_Open\\_Letter \"Pause Giant AI Experiments: An Open Letter\")_, a petition calling on major AI developers to agree on a verifiable six-month pause of any systems \"more powerful than [GPT-4](https://en.wikipedia.org/wiki/GPT-4 \"GPT-4\")\" and to use that time to institute a framework for ensuring safety; or, failing that, for governments to step in with a moratorium. The letter referred to the possibility of \"a profound change in the history of life on Earth\" as well as potential risks of AI-generated propaganda, loss of jobs, human obsolescence, and society-wide loss of control.<sup>[[116]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:9-118)</sup><sup>[[166]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-168)</sup> The letter was signed by prominent personalities in AI but also criticized for not focusing on current harms,<sup>[[167]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-169)</sup> missing technical nuance about when to pause,<sup>[[168]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-170)</sup> or not going far enough.<sup>[[89]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:3-90)</sup> Such concerns have led to the creation of [PauseAI](https://en.wikipedia.org/wiki/PauseAI \"PauseAI\"), an advocacy group organizing protests in major cities against the training of [frontier AI models](https://en.wikipedia.org/wiki/Frontier_model \"Frontier model\").<sup>[[169]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-171)</sup>\n\nMusk called for some sort of regulation of AI development as early as 2017. According to [NPR](https://en.wikipedia.org/wiki/National_Public_Radio \"National Public Radio\"), he is \"clearly not thrilled\" to be advocating government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... [as] they should be.\" In response, politicians expressed skepticism about the wisdom of regulating a technology that is still in development.<sup>[[170]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-172)</sup><sup>[[171]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-173)</sup><sup>[[172]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-cnbc2-174)</sup>\n\nIn 2021, the [United Nations](https://en.wikipedia.org/wiki/United_Nations \"United Nations\") (UN) considered banning autonomous lethal weapons, but consensus could not be reached.<sup>[[173]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-175)</sup> In July 2023 the UN [Security Council](https://en.wikipedia.org/wiki/United_Nations_Security_Council \"United Nations Security Council\") for the first time held a session to consider the risks and threats posed by AI to world peace and stability, along with potential benefits.<sup>[[174]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:13-176)</sup><sup>[[175]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-177)</sup>[Secretary-General](https://en.wikipedia.org/wiki/Secretary-General_of_the_United_Nations \"Secretary-General of the United Nations\")[Ant\u00f3nio Guterres](https://en.wikipedia.org/wiki/Ant%C3%B3nio_Guterres \"Ant\u00f3nio Guterres\") advocated the creation of a global watchdog to oversee the emerging technology, saying, \"Generative AI has enormous potential for good and evil at scale. Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead.\"<sup>[[22]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:12-23)</sup> At the council session, Russia said it believes AI risks are too poorly understood to be considered a threat to global stability. China argued against strict global regulation, saying countries should be able to develop their own rules, while also saying they opposed the use of AI to \"create military hegemony or undermine the sovereignty of a country\".<sup>[[174]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:13-176)</sup>\n\nRegulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<sup>[[176]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:532-178)</sup> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<sup>[[177]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-179)</sup><sup>[[125]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:132-127)</sup>\n\nIn July 2023, the US government secured voluntary safety commitments from major tech companies, including [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\"), [Amazon](https://en.wikipedia.org/wiki/Amazon_(company) \"Amazon (company)\"), [Google](https://en.wikipedia.org/wiki/Google \"Google\"), [Meta](https://en.wikipedia.org/wiki/Meta_Platforms \"Meta Platforms\"), and [Microsoft](https://en.wikipedia.org/wiki/Microsoft \"Microsoft\"). The companies agreed to implement safeguards, including third-party oversight and security testing by independent experts, to address concerns related to AI's potential risks and societal harms. The parties framed the commitments as an intermediate step while regulations are formed. Amba Kak, executive director of the [AI Now Institute](https://en.wikipedia.org/wiki/AI_Now_Institute \"AI Now Institute\"), said, \"A closed-door deliberation with corporate actors resulting in voluntary safeguards isn't enough\" and called for public deliberation and regulations of the kind to which companies would not voluntarily agree.<sup>[[178]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-180)</sup><sup>[[179]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-181)</sup>\n\nIn October 2023, U.S. President [Joe Biden](https://en.wikipedia.org/wiki/Joe_Biden \"Joe Biden\") issued an executive order on the \"[Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence](https://en.wikipedia.org/wiki/Executive_Order_14110 \"Executive Order 14110\")\".<sup>[[180]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-182)</sup> Alongside other requirements, the order mandates the development of guidelines for AI models that permit the \"evasion of human control",
              "collapsed_before": "## Potential AI capabilities",
              "collapsed_after": "\".\n\n## See also\n\n\n\n*   [Appeal to probability](https://en.wikipedia.org/wiki/Appeal_to_probability \"Appeal to probability\")\n*[AI alignment](https://en.wikipedia.org/wiki/AI_alignment \"AI alignment\")*   [AI safety](https://en.wikipedia.org/wiki/AI_safety \"AI safety\")\n*[Butlerian Jihad](https://en.wikipedia.org/wiki/Butlerian_Jihad \"Butlerian Jihad\")*   [Effective altruism \u00a7 Long-term future and global catastrophic risks](https://en.wikipedia.org/wiki/Effective_altruism#Long-term_future_and_global_catastrophic_risks \"Effective altruism\")\n*[Gray goo](https://en.wikipedia.org/wiki/Gray_goo \"Gray goo\")*   _[Human Compatible](https://en.wikipedia.org/wiki/Human\\_Compatible \"Human Compatible\")_\n*[Lethal autonomous weapon](https://en.wikipedia.org/wiki/Lethal_autonomous_weapon \"Lethal autonomous weapon\")*   [P(doom)](https://en.wikipedia.org/wiki/P(doom) \"P(doom)\")\n*[Paperclip maximizer](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer \"Instrumental convergence\")*   [Philosophy of artificial intelligence](https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence \"Philosophy of artificial intelligence\")\n*[Robot ethics \u00a7 In popular culture](https://en.wikipedia.org/wiki/Robot_ethics#In_popular_culture \"Robot ethics\")*   [Statement on AI risk of extinction](https://en.wikipedia.org/wiki/Statement_on_AI_risk_of_extinction \"Statement on AI risk of extinction\")\n*_[Superintelligence: Paths, Dangers, Strategies](https://en.wikipedia.org/wiki/Superintelligence:\\_Paths,\\_Dangers,\\_Strategies \"Superintelligence: Paths, Dangers, Strategies\")_*   [Risk of astronomical suffering](https://en.wikipedia.org/wiki/Risk_of_astronomical_suffering \"Risk of astronomical suffering\")\n*[System accident](https://en.wikipedia.org/wiki/System_accident \"System accident\")*   [Technological singularity](https://en.wikipedia.org/wiki/Technological_singularity \"Technological singularity\")\n*   [AI takeover](https://en.wikipedia.org/wiki/AI_takeover \"AI takeover\")\n\n## Notes\n\n1.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-turing_note_15-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-turing_note_15-1)In a 1951 lecture<sup>[[13]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-13)</sup> Turing argued that \"It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler's Erewhon\". Also in a lecture broadcast on the [BBC](https://en.wikipedia.org/wiki/BBC \"BBC\")<sup>[[14]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-14)</sup> he expressed the opinion: \"If a machine can think, it might think more intelligently than we do, and then where should we be? Even if we could keep the machines in a subservient position, for instance by turning off the power at strategic moments, we should, as a species, feel greatly humbled... This new danger... is certainly something which can give us anxiety.\"\n2.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-106)**as interpreted by [Seth Baum](https://en.wikipedia.org/wiki/Seth_Baum \"Seth Baum\")\n\n## References\n\n\n1.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-aima_1-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-aima_1-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-aima_1-2)[_**d**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-aima_1-3)[_**e**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-aima_1-4)[_**f**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-aima_1-5)[_**g**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-aima_1-6)[Russell, Stuart](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\"); [Norvig, Peter](https://en.wikipedia.org/wiki/Peter_Norvig \"Peter Norvig\") (2009). \"26.3: The Ethics and Risks of Developing Artificial Intelligence\". [_Artificial Intelligence: A Modern Approach_](https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach \"Artificial Intelligence: A Modern Approach\"). Prentice Hall. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-13-604259-4](https://en.wikipedia.org/wiki/Special:BookSources/978-0-13-604259-4 \"Special:BookSources/978-0-13-604259-4\").\n2.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-2)**[Bostrom, Nick](https://en.wikipedia.org/wiki/Nick_Bostrom \"Nick Bostrom\") (2002). \"Existential risks\". _[Journal of Evolution and Technology](https://en.wikipedia.org/wiki/Journal\\_of\\_Evolution\\_and\\_Technology \"Journal of Evolution and Technology\")_. **9** (1): 1-31.\n3.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-auto1_3-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-auto1_3-1)Turchin, Alexey; Denkenberger, David (3 May 2018). [\"Classification of global catastrophic risks connected with artificial intelligence\"](https://philarchive.org/rec/TURCOG-2). _AI & Society_. **35**(1): 147-163. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1007/s00146-018-0845-5](https://doi.org/10.1007%2Fs00146-018-0845-5). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0951-5666](https://search.worldcat.org/issn/0951-5666). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[19208453](https://api.semanticscholar.org/CorpusID:19208453).\n4.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-4)**Bales, Adam; D'Alessandro, William; Kirk-Giannini, Cameron Domenico (2024). [\"Artificial Intelligence: Arguments for Catastrophic Risk\"](https://doi.org/10.1111%2Fphc3.12964). _[Philosophy Compass](https://en.wikipedia.org/wiki/Philosophy\\_Compass \"Philosophy Compass\")_. **19**(2) e12964. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2401.15487](https://arxiv.org/abs/2401.15487). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1111/phc3.12964](https://doi.org/10.1111%2Fphc3.12964).\n5.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-5)**Kwa, Thomas; West, Ben; Becker, Joel; Deng, Amy; Garcia, Katharyn; Hasin, Max; Jawhar, Sami; Kinniment, Megan; Rush, Nate; von Arx, Sydney; Bloom, Ryan; Broadley, Thomas; Du, Haoxing; Goodrich, Brian; Jurkovic, Nikola; Miles, Luke Harold; Nix, Seraphina; Lin, Tao; Parikh, Neev; Rein, David; Koba Sato, Lucas Jun; Wijk, Hjalmar; Ziegler, Daniel M.; Barnes, Elizabeth; Chan, Lawrence (19 March 2025). [\"Measuring AI Ability to Complete Long Tasks\"](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/). _METR Blog_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2503.14499](https://arxiv.org/abs/2503.14499).\n6.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-6)**Druzin, Bryan (2025). [\"Confronting Catastrophic Risk: The International Obligation to Regulate Artificial Intelligence\"](https://repository.law.umich.edu/mjil/vol46/iss2/2/). _Michigan Journal of International Law_. **46** (2): 185-87.\n7.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-2)[_**d**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-3)[_**e**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-4)[_**f**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-5)[_**g**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-6)[_**h**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-7)[_**i**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-8)[_**j**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-9)[_**k**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-10)[_**l**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-11)[_**m**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-12)[_**n**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-13)[_**o**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-14)[_**p**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-superintelligence_7-15)[Bostrom, Nick](https://en.wikipedia.org/wiki/Nick_Bostrom \"Nick Bostrom\") (2014). [_Superintelligence: Paths, Dangers, Strategies_](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies \"Superintelligence: Paths, Dangers, Strategies\") (First ed.). Oxford University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-19-967811-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2 \"Special:BookSources/978-0-19-967811-2\").\n8.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-DeVynck2023_8-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-DeVynck2023_8-1)De Vynck, Gerrit (23 May 2023). [\"The debate over whether AI will destroy us is dividing Silicon Valley\"](https://www.washingtonpost.com/technology/2023/05/20/ai-existential-risk-debate/). _Washington Post_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0190-8286](https://search.worldcat.org/issn/0190-8286). Retrieved 27 July 2023.\n9.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-9)**Metz, Cade (10 June 2023). [\"How Could A.I. Destroy Humanity?\"](https://www.nytimes.com/2023/06/10/technology/ai-humanity.html). _The New York Times_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0362-4331](https://search.worldcat.org/issn/0362-4331). Retrieved 27 July 2023.\n10.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-10)**[\"'Godfather of artificial intelligence' weighs in on the past and potential of AI\"](https://www.cbsnews.com/news/godfather-of-artificial-intelligence-weighs-in-on-the-past-and-potential-of-artificial-intelligence/). _www.cbsnews.com_. 25 March 2023. Retrieved 10 April 2023.\n11.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-11)**[\"How Rogue AIs may Arise\"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). _yoshuabengio.org_. 26 May 2023. Retrieved 26 May 2023.\n12.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-12)**Milmo, Dan (24 October 2023). [\"AI risk must be treated as seriously as climate crisis, says Google DeepMind chief\"](https://www.theguardian.com/technology/2023/oct/24/ai-risk-climate-crisis-google-deepmind-chief-demis-hassabis-regulation). _The Guardian_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0261-3077](https://search.worldcat.org/issn/0261-3077). Retrieved 12 August 2025.\n13.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-13)**Turing, Alan (1951). [_Intelligent machinery, a heretical theory_](https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-4) (Speech). Lecture given to '51 Society'. Manchester: The Turing Digital Archive. [Archived](https://web.archive.org/web/20220926004549/https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-4) from the original on 26 September 2022. Retrieved 22 July 2022.\n14.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-14)**Turing, Alan (15 May 1951). \"Can digital computers think?\". _Automatic Calculating Machines_. Episode 2. BBC. [Can digital computers think?](https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-6).\n15.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-16)**Oreskovic, Alexei. [\"Anthropic CEO lays out A.I.'s short, medium, and long-term risks\"](https://fortune.com/2023/07/10/anthropic-ceo-dario-amodei-ai-risks-short-medium-long-term/). _Fortune_. Retrieved 12 August 2025.\n16.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-Jackson_17-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-Jackson_17-1)Jackson, Sarah. [\"The CEO of the company behind AI chatbot ChatGPT says the worst-case scenario for artificial intelligence is 'lights out for all of us'\"](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1). _Business Insider_. Retrieved 10 April 2023.\n17.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-Parkin_18-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-Parkin_18-1)Parkin, Simon (14 June 2015). [\"Science fiction no more? Channel 4's Humans and our rogue AI obsessions\"](https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence). _[The Guardian](https://en.wikipedia.org/wiki/The\\_Guardian \"The Guardian\")_. [Archived](https://web.archive.org/web/20180205184322/https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence) from the original on 5 February 2018. Retrieved 5 February 2018.\n18.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-19)**[\"The AI Dilemma\"](https://www.humanetech.com/podcast/the-ai-dilemma). _www.humanetech.com_. Retrieved 10 April 2023. 50% of AI researchers believe there's a 10% or greater chance that humans go extinct from our inability to control AI.\n19.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:8_20-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:8_20-1)[\"2022 Expert Survey on Progress in AI\"](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/). _AI Impacts_. 4 August 2022. Retrieved 10 April 2023.\n20.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-21)**Roose, Kevin (30 May 2023). [\"A.I. Poses 'Risk of Extinction,' Industry Leaders Warn\"](https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html). _The New York Times_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0362-4331](https://search.worldcat.org/issn/0362-4331). Retrieved 3 June 2023.\n21.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-22)**Sunak, Rishi (14 June 2023). [\"Rishi Sunak Wants the U.K. to Be a Key Player in Global AI Regulation\"](https://time.com/6287253/uk-rishi-sunak-ai-regulation/). _Time_.\n22.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:12_23-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:12_23-1)Fung, Brian (18 July 2023). [\"UN Secretary General embraces calls for a new UN agency on AI in the face of 'potentially catastrophic and existential risks'\"](https://www.cnn.com/2023/07/18/tech/un-ai-agency/index.html). _CNN Business_. Retrieved 20 July 2023.\n23.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-yudkowsky-global-risk_24-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-yudkowsky-global-risk_24-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-yudkowsky-global-risk_24-2)[_**d**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-yudkowsky-global-risk_24-3)[_**e**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-yudkowsky-global-risk_24-4)Yudkowsky, Eliezer (2008). [\"Artificial Intelligence as a Positive and Negative Factor in Global Risk\"](https://intelligence.org/files/AIPosNegFactor.pdf)(PDF). _Global Catastrophic Risks_: 308-345. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2008gcr..book..303Y](https://ui.adsabs.harvard.edu/abs/2008gcr..book..303Y). [Archived](https://web.archive.org/web/20130302173022/http://intelligence.org/files/AIPosNegFactor.pdf)(PDF) from the original on 2 March 2013. Retrieved 27 August 2018.\n24.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-research-priorities_25-0)**[Russell, Stuart](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\"); Dewey, Daniel; [Tegmark, Max](https://en.wikipedia.org/wiki/Max_Tegmark \"Max Tegmark\") (2015). [\"Research Priorities for Robust and Beneficial Artificial Intelligence\"](https://futureoflife.org/data/documents/research_priorities.pdf)(PDF). _AI Magazine_. Association for the Advancement of Artificial Intelligence: 105-114. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1602.03506](https://arxiv.org/abs/1602.03506). [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2016arXiv160203506R](https://ui.adsabs.harvard.edu/abs/2016arXiv160203506R). [Archived](https://web.archive.org/web/20190804145930/https://futureoflife.org/data/documents/research_priorities.pdf)(PDF) from the original on 4 August 2019. Retrieved 10 August 2019., cited in [\"AI Open Letter - Future of Life Institute\"](https://futureoflife.org/ai-open-letter). _Future of Life Institute_. January 2015. [Archived](https://web.archive.org/web/20190810020404/https://futureoflife.org/ai-open-letter) from the original on 10 August 2019. Retrieved 9 August 2019.\n25.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-vanity_26-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-vanity_26-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-vanity_26-2)Dowd, Maureen (April 2017). [\"Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse\"](https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x). _The Hive_. [Archived](https://web.archive.org/web/20180726041656/https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x) from the original on 26 July 2018. Retrieved 27 November 2017.\n26.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-27)**[\"Agentic Misalignment: How LLMs could be insider threats\"](https://www.anthropic.com/research/agentic-misalignment). _Anthropic_. 21 June 2025. Retrieved 9 October 2025.\n27.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-28)**[\"AlphaGo Zero: Starting from scratch\"](https://www.deepmind.com/blog/alphago-zero-starting-from-scratch). _www.deepmind.com_. 18 October 2017. Retrieved 28 July 2023.\n28.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-29)**Breuer, Hans-Peter. ['Samuel Butler's \"the Book of the Machines\" and the Argument from Design.'](https://www.jstor.org/pss/436868)[Archived](https://web.archive.org/web/20230315233257/https://www.jstor.org/stable/436868) 15 March 2023 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine \"Wayback Machine\") Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365-383.\n29.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-oxfordjournals_30-0)**Turing, A M (1996). [\"Intelligent Machinery, A Heretical Theory\"](https://doi.org/10.1093%2Fphilmat%2F4.3.256). _1951, Reprinted Philosophia Mathematica_. **4**(3): 256-260. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1093/philmat/4.3.256](https://doi.org/10.1093%2Fphilmat%2F4.3.256).\n30.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-31)**Hilliard, Mark (2017). [\"The AI apocalypse: will the human race soon be terminated?\"](https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220). _The Irish Times_. [Archived](https://web.archive.org/web/20200522114127/https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220) from the original on 22 May 2020. Retrieved 15 March 2020.\n31.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-32)**I.J. Good, [\"Speculations Concerning the First Ultraintelligent Machine\"](http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf)[Archived](https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf) 2011-11-28 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine \"Wayback Machine\") ([HTML](http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html) ), _Advances in Computers_, vol. 6, 1965.\n32.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-33)**Russell, Stuart J.; Norvig, Peter (2003). \"Section 26.3: The Ethics and Risks of Developing Artificial Intelligence\". [_Artificial Intelligence: A Modern Approach_](https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach \"Artificial Intelligence: A Modern Approach\"). Upper Saddle River, New Jersey: Prentice Hall. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-13-790395-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-13-790395-5 \"Special:BookSources/978-0-13-790395-5\"). Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.\n33.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-34)**Barrat, James (2013). _Our final invention: artificial intelligence and the end of the human era_ (First ed.). New York: St. Martin's Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-312-62237-4](https://en.wikipedia.org/wiki/Special:BookSources/978-0-312-62237-4 \"Special:BookSources/978-0-312-62237-4\"). In the bio, playfully written in the third person, Good summarized his life's milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here's what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn: [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965)...began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good's] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.'\n34.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-35)**Anderson, Kurt (26 November 2014). [\"Enthusiasts and Skeptics Debate Artificial Intelligence\"](https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory). _[Vanity Fair](https://en.wikipedia.org/wiki/Vanity\\_Fair\\_(magazine) \"Vanity Fair (magazine)\")_. [Archived](https://web.archive.org/web/20160122025154/http://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory) from the original on 22 January 2016. Retrieved 30 January 2016.\n35.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-36)**Metz, Cade (9 June 2018). [\"Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots\"](https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html). _The New York Times_. [Archived](https://web.archive.org/web/20210215051949/https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html) from the original on 15 February 2021. Retrieved 3 April 2019.\n36.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-37)**Hsu, Jeremy (1 March 2012). [\"Control dangerous AI before it controls us, one expert says\"](https://www.nbcnews.com/id/wbna46590591). _[NBC News](https://en.wikipedia.org/wiki/NBC\\_News \"NBC News\")_. [Archived](https://web.archive.org/web/20160202173621/http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation) from the original on 2 February 2016. Retrieved 28 January 2016.\n37.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-hawking_editorial_38-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-hawking_editorial_38-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-hawking_editorial_38-2)[\"Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence- but are we taking AI seriously enough?'\"](https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html). [The Independent (UK)](https://en.wikipedia.org/wiki/The_Independent_(UK) \"The Independent (UK)\"). [Archived](https://web.archive.org/web/20150925153716/http://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html) from the original on 25 September 2015. Retrieved 3 December 2014.\n38.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-bbc_on_hawking_editorial_39-0)**[\"Stephen Hawking warns artificial intelligence could end mankind\"](https://www.bbc.com/news/technology-30290540). [BBC](https://en.wikipedia.org/wiki/BBC \"BBC\"). 2 December 2014. [Archived](https://web.archive.org/web/20151030054329/http://www.bbc.com/news/technology-30290540) from the original on 30 October 2015. Retrieved 3 December 2014.\n39.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-40)**Eadicicco, Lisa (28 January 2015). [\"Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity\"](http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1). _[Business Insider](https://en.wikipedia.org/wiki/Business\\_Insider \"Business Insider\")_. [Archived](https://web.archive.org/web/20160226090602/http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1) from the original on 26 February 2016. Retrieved 30 January 2016.\n40.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-41)**[\"Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter\"](http://futureoflife.org/misc/open_letter). [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\"). [Archived](https://web.archive.org/web/20150115160823/http://futureoflife.org/misc/open_letter) from the original on 15 January 2015. Retrieved 23 October 2015.\n41.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-42)**[\"Anticipating artificial intelligence\"](https://doi.org/10.1038%2F532413a). _Nature_. **532**(7600): 413. 2016. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2016Natur.532Q.413.](https://ui.adsabs.harvard.edu/abs/2016Natur.532Q.413.). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/532413a](https://doi.org/10.1038%2F532413a). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[1476-4687](https://search.worldcat.org/issn/1476-4687). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[27121801](https://pubmed.ncbi.nlm.nih.gov/27121801). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[4399193](https://api.semanticscholar.org/CorpusID:4399193).\n42.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-43)**Christian, Brian (6 October 2020). [_The Alignment Problem: Machine Learning and Human Values_](https://brianchristian.org/the-alignment-problem/). [W. W. Norton & Company](https://en.wikipedia.org/wiki/W._W._Norton_%26_Company \"W. W. Norton & Company\"). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-393-63582-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-393-63582-9 \"Special:BookSources/978-0-393-63582-9\"). [Archived](https://web.archive.org/web/20211205135022/https://brianchristian.org/the-alignment-problem/) from the original on 5 December 2021. Retrieved 5 December 2021.\n43.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-44)**Dignum, Virginia (26 May 2021). [\"AI - the people and places that make, use and manage it\"](https://doi.org/10.1038%2Fd41586-021-01397-x). _Nature_. **593**(7860): 499-500. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2021Natur.593..499D](https://ui.adsabs.harvard.edu/abs/2021Natur.593..499D). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/d41586-021-01397-x](https://doi.org/10.1038%2Fd41586-021-01397-x). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[235216649](https://api.semanticscholar.org/CorpusID:235216649).\n44.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-45)**[\"Elon Musk among experts urging a halt to AI training\"](https://www.bbc.com/news/technology-65110030). _BBC News_. 29 March 2023. Retrieved 9 June 2023.\n45.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-46)**[\"Statement on AI Risk\"](https://www.safe.ai/statement-on-ai-risk#open-letter). _Center for AI Safety_. Retrieved 8 June 2023.\n46.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-47)**[\"Artificial intelligence could lead to extinction, experts warn\"](https://www.bbc.com/news/uk-65746524). _BBC News_. 30 May 2023. Retrieved 8 June 2023.\n47.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-48)**Perrigo, Billy; Pillay, Tharin (22 October 2025). [\"'Time Is Running Out': New Open Letter Calls for Ban on Superintelligent AI Development\"](https://time.com/7327409/ai-agi-superintelligent-open-letter/). _TIME_. Retrieved 12 January 2026.\n48.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-49)**[\"DeepMind and Google: the battle to control artificial intelligence\"](https://www.economist.com/1843/2019/03/01/deepmind-and-google-the-battle-to-control-artificial-intelligence). _The Economist_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0013-0613](https://search.worldcat.org/issn/0013-0613). Retrieved 12 July 2023.\n49.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-50)**Roser, Max (7 February 2023). [\"AI timelines: What do experts in artificial intelligence expect for the future?\"](https://ourworldindata.org/ai-timelines). _Our World in Data_. Retrieved 12 July 2023.\n50.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-51)**[\"'The Godfather of A.I.' just quit Google and says he regrets his life's work because it can be hard to stop 'bad actors from using it for bad things'\"](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/). _Fortune_. Retrieved 12 July 2023.\n51.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-52)**[\"Everything you need to know about superintelligence\"](https://www.spiceworks.com/tech/artificial-intelligence/articles/everything-about-superintelligence/). _Spiceworks_. Retrieved 14 July 2023.\n52.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-economist_review3_53-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-economist_review3_53-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-economist_review3_53-2)Babauta, Leo. [\"A Valuable New Book Explores The Potential Impacts Of Intelligent Machines On Human Life\"](https://www.businessinsider.com/intelligent-machines-and-human-life-2014-8). _Business Insider_. Retrieved 19 March 2024.\n53.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:11_54-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:11_54-1)Bostrom, Nick (27 April 2015), [_What happens when our computers get smarter than we are?_](https://www.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are), retrieved 13 July 2023.\n54.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-55)**[\"Governance of superintelligence\"](https://openai.com/blog/governance-of-superintelligence). _openai.com_. Retrieved 12 July 2023.\n55.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-56)**[\"Overcoming Bias: I Still Don't Get Foom\"](http://www.overcomingbias.com/2014/07/30855.html). _www.overcomingbias.com_. [Archived](https://web.archive.org/web/20170804221136/http://www.overcomingbias.com/2014/07/30855.html) from the original on 4 August 2017. Retrieved 20 September 2017.\n56.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-57)**Cotton-Barratt, Owen; Ord, Toby (12 August 2014). [\"Strategic considerations about different speeds of AI takeoff\"](https://www.fhi.ox.ac.uk/strategic-considerations-about-different-speeds-of-ai-takeoff/). _The Future of Humanity Institute_. Retrieved 12 July 2023.\n57.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-58)**Tegmark, Max (25 April 2023). [\"The 'Don't Look Up' Thinking That Could Doom Us With AI\"](https://time.com/6273743/thinking-that-could-doom-us-with-ai/). _Time_. Retrieved 14 July 2023. As if losing control to Chinese minds were scarier than losing control to alien digital minds that don't care about humans. [...] it's clear by now that the space of possible alien minds is vastly larger than that.\n58.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-59)**[\"19 - Mechanistic Interpretability with Neel Nanda\"](https://axrp.net/episode/2023/02/04/episode-19-mechanistic-interpretability-neel-nanda.html). _AXRP - the AI X-risk Research Podcast_. 4 February 2023. Retrieved 13 July 2023. it's plausible to me that the main thing we need to get done is noticing specific circuits to do with deception and specific dangerous capabilities like that and situational awareness and internally-represented goals.\n59.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-60)**[\"Superintelligence Is Not Omniscience\"](https://aiimpacts.org/superintelligence-is-not-omniscience/). _AI Impacts_. 7 April 2023. Retrieved 16 April 2023.\n60.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-2)[_**d**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-3)[_**e**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-4)[_**f**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-5)[_**g**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-6)[_**h**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-7)[_**i**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:03_61-8)Hendrycks, Dan; Mazeika, Mantas; Woodside, Thomas (21 June 2023). \"An Overview of Catastrophic AI Risks\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2306.12001](https://arxiv.org/abs/2306.12001) [[cs.CY](https://arxiv.org/archive/cs.CY)].\n61.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-62)**Taylor, Josh; Hern, Alex (2 May 2023). [\"'Godfather of AI' Geoffrey Hinton quits Google and warns over dangers of misinformation\"](https://www.theguardian.com/technology/2023/may/02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of-machine-learning). _The Guardian_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0261-3077](https://search.worldcat.org/issn/0261-3077). Retrieved 13 July 2023.\n62.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-63)**[\"How NATO is preparing for a new era of AI cyber attacks\"](https://www.euronews.com/next/2022/12/26/ai-cyber-attacks-are-a-critical-threat-this-is-how-nato-is-countering-them). _euronews_. 26 December 2022. Retrieved 13 July 2023.\n63.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-64)**[\"ChatGPT and the new AI are wreaking havoc on cybersecurity in exciting and frightening ways\"](https://www.zdnet.com/article/chatgpt-and-the-new-ai-are-wreaking-havoc-on-cybersecurity/). _ZDNET_. Retrieved 13 July 2023.\n64.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-65)**Toby Shevlane; Sebastian Farquhar; Ben Garfinkel; Mary Phuong; Jess Whittlestone; Jade Leung; Daniel Kokotajlo; Nahema Marchal; Markus Anderljung; Noam Kolt; Lewis Ho; Divya Siddarth; Shahar Avin; Will Hawkins; Been Kim; Iason Gabriel; Vijay Bolina; Jack Clark; Yoshua Bengio; Paul Christiano; Allan Dafoe (24 May 2023). \"Model evaluation for extreme risks\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2305.15324](https://arxiv.org/abs/2305.15324) [[cs.AI](https://arxiv.org/archive/cs.AI)].\n65.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-66)**Urbina, Fabio; Lentzos, Filippa; Invernizzi, C\u00e9dric; Ekins, Sean (7 March 2022). [\"Dual use of artificial-intelligence-powered drug discovery\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280). _Nature Machine Intelligence_. **4**(3): 189-191. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/s42256-022-00465-9](https://doi.org/10.1038%2Fs42256-022-00465-9). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[2522-5839](https://search.worldcat.org/issn/2522-5839). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) \"PMC (identifier)\")[9544280](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[36211133](https://pubmed.ncbi.nlm.nih.gov/36211133).\n66.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-67)**Walter, Yoshija (27 March 2023). [\"The rapid competitive economy of machine learning development: a discussion on the social risks and benefits\"](https://doi.org/10.1007%2Fs43681-023-00276-7). _AI and Ethics_. **4**(2): 1. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1007/s43681-023-00276-7](https://doi.org/10.1007%2Fs43681-023-00276-7).\n67.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-68)**[\"The AI Arms Race Is On. Start Worrying\"](https://time.com/6255952/ai-impact-chatgpt-microsoft-google/). _Time_. 16 February 2023. Retrieved 17 July 2023.\n68.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-69)**Brimelow, Ben. [\"The short film 'Slaughterbots' depicts a dystopian future of killer drones swarming the world\"](https://www.businessinsider.com/slaughterbots-short-film-depicts-killer-drone-swarms-2017-11). _Business Insider_. Retrieved 20 July 2023.\n69.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-70)**Mecklin, John (17 July 2023). [\"'Artificial Escalation': Imagining the future of nuclear risk\"](https://thebulletin.org/2023/07/artificial-escalation-imagining-the-future-of-nuclear-risk/). _Bulletin of the Atomic Scientists_. Retrieved 20 July 2023.\n70.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-priority_71-0)**Bostrom, Nick (2013). [\"Existential Risk Prevention as Global Priority\"](http://www.existential-risk.org/concept.pdf)(PDF). _Global Policy_. **4**(1): 15-3. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1111/1758-5899.12002](https://doi.org/10.1111%2F1758-5899.12002) - via Existential Risk.\n71.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-72)**Doherty, Ben (17 May 2018). [\"Climate change an 'existential security risk' to Australia, Senate inquiry says\"](https://www.theguardian.com/environment/2018/may/18/climate-change-an-existential-security-risk-to-australia-senate-inquiry-says). _The Guardian_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0261-3077](https://search.worldcat.org/issn/0261-3077). Retrieved 16 July 2023.\n72.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-73)**MacAskill, William (2022). _What we owe the future_. New York, New York: Basic Books. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-5416-1862-6](https://en.wikipedia.org/wiki/Special:BookSources/978-1-5416-1862-6 \"Special:BookSources/978-1-5416-1862-6\").\n73.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:0_74-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:0_74-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:0_74-2)[_**d**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:0_74-3)Ord, Toby (2020). \"Chapter 5: Future Risks, Unaligned Artificial Intelligence\". _The Precipice: Existential Risk and the Future of Humanity_. Bloomsbury Publishing. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-5266-0021-9](https://en.wikipedia.org/wiki/Special:BookSources/978-1-5266-0021-9 \"Special:BookSources/978-1-5266-0021-9\").\n74.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-75)**McMillan, Tim (15 March 2024). [\"Navigating Humanity's Greatest Challenge Yet: Experts Debate the Existential Risks of AI\"](https://thedebrief.org/navigating-humanitys-greatest-challenge-yet-experts-debate-the-existential-risks-of-ai/). _The Debrief_. Retrieved 26 September 2024.\n75.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-76)**Kasirzadeh, Atoosa (2024). \"Two Types of AI Existential Risk: Decisive and Accumulative\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2401.07836](https://arxiv.org/abs/2401.07836) [[cs.CR](https://arxiv.org/archive/cs.CR)].\n76.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-77)**Samuelsson, Paul Conrad (June-July 2019). [\"Artificial Consciousness: Our Greatest Ethical Challenge\"](https://philosophynow.org/issues/132/Artificial_Consciousness_Our_Greatest_Ethical_Challenge). _Philosophy Now_. No.132. Retrieved 19 August 2023.\n77.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-78)**Kateman, Brian (24 July 2023). [\"AI Should Be Terrified of Humans\"](https://time.com/6296234/ai-should-be-terrified-of-humans/). _Time_. Retrieved 19 August 2023.\n78.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-79)**Sotala, Kaj; Gloor, Lukas (2017). [\"Superintelligence as a Cause or Cure for Risks of Astronomical Suffering\"](https://longtermrisk.org/files/Sotala-Gloor-Superintelligent-AI-and-Suffering-Risks.pdf)(PDF). _Informatica_.\n79.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-80)**Fisher, Richard (13 November 2020). [\"The intelligent monster that you should let eat you\"](https://www.bbc.com/future/article/20201111-philosophy-of-utility-monsters-and-artificial-intelligence). _www.bbc.com_. Retrieved 19 August 2023.\n80.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-81)**More, Max (19 June 2023). [\"Existential Risk vs. Existential Opportunity: A balanced approach to AI risk\"](https://maxmore.substack.com/p/existential-risk-vs-existential-opportunity). _Extropic Thoughts_. Retrieved 14 July 2023.\n81.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-omohundro_82-0)**Omohundro, S. M. (2008, February). The basic AI drives. In _AGI_ (Vol. 171, pp. 483-492).\n82.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-Wakefield2015_83-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-Wakefield2015_83-1)Wakefield, Jane (15 September 2015). [\"Why is Facebook investing in AI?\"](https://www.bbc.com/news/technology-34118481). _BBC News_. [Archived](https://web.archive.org/web/20171202192942/http://www.bbc.com/news/technology-34118481) from the original on 2 December 2017. Retrieved 27 November 2017.\n83.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-84)**Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Germany: Springer, Berlin, Heidelberg.\n84.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-85)**[Russell, Stuart](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\") (2014). [\"Of Myths and Moonshine\"](http://edge.org/conversation/the-myth-of-ai#26015). _[Edge](https://en.wikipedia.org/wiki/Edge.org \"Edge.org\")_. [Archived](https://web.archive.org/web/20160719124525/https://www.edge.org/conversation/the-myth-of-ai#26015) from the original on 19 July 2016. Retrieved 23 October 2015.\n85.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-acm2_86-0)**[Dietterich, Thomas](https://en.wikipedia.org/wiki/Eric_Horvitz \"Eric Horvitz\"); Horvitz, Eric (2015). [\"Rise of Concerns about AI: Reflections and Directions\"](http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf)(PDF). _[Communications of the ACM](https://en.wikipedia.org/wiki/Communications\\_of\\_the\\_ACM \"Communications of the ACM\")_. **58**(10): 38-40. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/2770869](https://doi.org/10.1145%2F2770869). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[20395145](https://api.semanticscholar.org/CorpusID:20395145). [Archived](https://web.archive.org/web/20160304132930/http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf)(PDF) from the original on 4 March 2016. Retrieved 23 October 2015.\n86.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-87)**Yudkowsky, Eliezer (2011). [\"Complex Value Systems are Required to Realize Valuable Futures\"](https://intelligence.org/files/ComplexValues.pdf)(PDF). [Archived](https://web.archive.org/web/20150929212318/http://intelligence.org/files/ComplexValues.pdf)(PDF) from the original on 29 September 2015. Retrieved 10 August 2020.\n87.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:5_88-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:5_88-1)[Ord, Toby](https://en.wikipedia.org/wiki/Toby_Ord \"Toby Ord\") (2020). _[The Precipice: Existential Risk and the Future of Humanity](https://en.wikipedia.org/wiki/The\\_Precipice:\\_Existential\\_Risk\\_and\\_the\\_Future\\_of\\_Humanity \"The Precipice: Existential Risk and the Future of Humanity\")_. Bloomsbury Publishing Plc. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-5266-0019-6](https://en.wikipedia.org/wiki/Special:BookSources/978-1-5266-0019-6 \"Special:BookSources/978-1-5266-0019-6\").\n88.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-89)**Nayebi, Aran (2025). \"Core Safety Values for Provably Corrigible Agents\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2507.20964](https://arxiv.org/abs/2507.20964) [[cs.AI](https://arxiv.org/archive/cs.AI)]. To appear in the 40th AAAI Conference on Artificial Intelligence (AAAI 2026) Machine Ethics Workshop (W37) Proceedings.\n89.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:3_90-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:3_90-1)[_**c**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:3_90-2)Yudkowsky, Eliezer (29 March 2023). [\"The Open Letter on AI Doesn't Go Far Enough\"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/). _Time_. Retrieved 16 July 2023.\n90.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-91)**Bostrom, Nick (1 May 2012). \"The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents\". _Minds and Machines_. **22**(2): 71-85. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1007/s11023-012-9281-3](https://doi.org/10.1007%2Fs11023-012-9281-3). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[1572-8641](https://search.worldcat.org/issn/1572-8641). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[254835485](https://api.semanticscholar.org/CorpusID:254835485). as long as they possess a sufficient level of intelligence, agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so.\n91.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-92)**Ngo, Richard; Chan, Lawrence; S\u00f6ren Mindermann (22 February 2023). \"The alignment problem from a deep learning perspective\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2209.00626](https://arxiv.org/abs/2209.00626) [[cs.AI](https://arxiv.org/archive/cs.AI)].\n92.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-93)**[\"Introducing Superalignment\"](https://openai.com/blog/introducing-superalignment). _openai.com_. Retrieved 16 July 2023.\n93.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-94)**[\"OpenAI dissolves Superalignment AI safety team\"](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html). _cnbc.com_. 17 May 2024. Retrieved 5 January 2025.\n94.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-slate_killer_95-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-slate_killer_95-1)Tilli, Cecilia (28 April 2016). [\"Killer Robots? Lost Jobs?\"](http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html). _Slate_. [Archived](https://web.archive.org/web/20160511183659/http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html) from the original on 11 May 2016. Retrieved 15 May 2016.\n95.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-96)**[\"Norvig vs. Chomsky and the Fight for the Future of AI\"](http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/). _Tor.com_. 21 June 2011. [Archived](https://web.archive.org/web/20160513052842/http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/) from the original on 13 May 2016. Retrieved 15 May 2016.\n96.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-skeptic_97-0)**Graves, Matthew (8 November 2017). [\"Why We Should Be Concerned About Artificial Superintelligence\"](https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/). _[Skeptic (US magazine)](https://en.wikipedia.org/wiki/Skeptic\\_(US\\_magazine) \"Skeptic (US magazine)\")_. Vol.22, no.2. [Archived](https://web.archive.org/web/20171113050152/https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/) from the original on 13 November 2017. Retrieved 27 November 2017.\n97.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-98)**Yampolskiy, Roman V. (8 April 2014). \"Utility function security in artificially intelligent agents\". _Journal of Experimental & Theoretical Artificial Intelligence_. **26**(3): 373-389. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2014JETAI..26..373Y](https://ui.adsabs.harvard.edu/abs/2014JETAI..26..373Y). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1080/0952813X.2014.895114](https://doi.org/10.1080%2F0952813X.2014.895114). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[16477341](https://api.semanticscholar.org/CorpusID:16477341). Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions.\n98.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-99)**[\"Will artificial intelligence destroy humanity? Here are 5 reasons not to worry\"](https://www.vox.com/2014/8/22/6043635/5-reasons-we-shouldnt-worry-about-super-intelligent-computers-taking). _Vox_. 22 August 2014. [Archived](https://web.archive.org/web/20151030092203/http://www.vox.com/2014/8/22/6043635/5-reasons-we-shouldnt-worry-about-super-intelligent-computers-taking) from the original on 30 October 2015. Retrieved 30 October 2015.\n99.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-100)**Bostrom, Nick (2014). _Superintelligence: Paths, Dangers, Strategies_. Oxford, United Kingdom: Oxford University Press. p.116. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-19-967811-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2 \"Special:BookSources/978-0-19-967811-2\").\n100.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-101)**Bostrom, Nick (2012). [\"Superintelligent Will\"](http://www.nickbostrom.com/superintelligentwill.pdf)(PDF). _Nick Bostrom_. [Archived](https://web.archive.org/web/20151128034545/http://www.nickbostrom.com/superintelligentwill.pdf)(PDF) from the original on 28 November 2015. Retrieved 29 October 2015.\n101.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-armstrong_102-0)**Armstrong, Stuart (1 January 2013). [\"General Purpose Intelligence: Arguing the Orthogonality Thesis\"](https://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality). _Analysis and Metaphysics_. **12**. [Archived](https://web.archive.org/web/20141011084205/http://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality) from the original on 11 October 2014. Retrieved 2 April 2020. Full text available [here](https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf)[Archived](https://web.archive.org/web/20200325025010/https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf) 25 March 2020 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine \"Wayback Machine\").\n102.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-chorost_103-0)**Chorost, Michael (18 April 2016). [\"Let Artificial Intelligence Evolve\"](http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html). _Slate_. [Archived](https://web.archive.org/web/20171127213642/http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html) from the original on 27 November 2017. Retrieved 27 November 2017.\n103.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-Telegraph2016_104-0)**[\"Should humans fear the rise of the machine?\"](https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html). _[The Telegraph (UK)](https://en.wikipedia.org/wiki/The\\_Telegraph\\_(UK) \"The Telegraph (UK)\")_. 1 September 2015. [Archived](https://ghostarchive.org/archive/20220112/https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html) from the original on 12 January 2022. Retrieved 7 February 2016.\n104.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-shermer_105-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-shermer_105-1)Shermer, Michael (1 March 2017). [\"Apocalypse AI\"](https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/). _Scientific American_. **316**(3): 77. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2017SciAm.316c..77S](https://ui.adsabs.harvard.edu/abs/2017SciAm.316c..77S). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/scientificamerican0317-77](https://doi.org/10.1038%2Fscientificamerican0317-77). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[28207698](https://pubmed.ncbi.nlm.nih.gov/28207698). [Archived](https://web.archive.org/web/20171201051401/https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/) from the original on 1 December 2017. Retrieved 27 November 2017.\n105.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-auto_107-0)**Baum, Seth (30 September 2018). [\"Countering Superintelligence Misinformation\"](https://doi.org/10.3390%2Finfo9100244). _Information_. **9**(10): 244. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.3390/info9100244](https://doi.org/10.3390%2Finfo9100244). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[2078-2489](https://search.worldcat.org/issn/2078-2489).\n106.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-108)**[\"The Myth Of AI\"](https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai). _www.edge.org_. [Archived](https://web.archive.org/web/20200311210407/https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai) from the original on 11 March 2020. Retrieved 11 March 2020.\n107.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:2_109-0)**Bostrom, Nick, _Superintelligence: paths, dangers, strategies_ (Audiobook), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-5012-2774-5](https://en.wikipedia.org/wiki/Special:BookSources/978-1-5012-2774-5 \"Special:BookSources/978-1-5012-2774-5\"), [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) \"OCLC (identifier)\")[1061147095](https://search.worldcat.org/oclc/1061147095).\n108.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:4_110-0)**Sotala, Kaj; Yampolskiy, Roman V (19 December 2014). [\"Responses to catastrophic AGI risk: a survey\"](https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001). _Physica Scripta_. **90**(1): 12. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2015PhyS...90a8001S](https://ui.adsabs.harvard.edu/abs/2015PhyS...90a8001S). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1088/0031-8949/90/1/018001](https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0031-8949](https://search.worldcat.org/issn/0031-8949).\n109.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-111)**Pistono, Federico; Yampolskiy, Roman V. (9 May 2016). _Unethical Research: How to Create a Malevolent Artificial Intelligence_. [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) \"OCLC (identifier)\")[1106238048](https://search.worldcat.org/oclc/1106238048).\n110.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-112)**Haney, Brian Seamus (2018). [\"The Perils & Promises of Artificial General Intelligence\"](https://doi.org/10.2139%2Fssrn.3261254). _SSRN Working Paper Series_. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.2139/ssrn.3261254](https://doi.org/10.2139%2Fssrn.3261254). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[1556-5068](https://search.worldcat.org/issn/1556-5068). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[86743553](https://api.semanticscholar.org/CorpusID:86743553).\n111.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-113)**Davidson, Tom; Finnveden, Lukas; Hadshar, Rose (15 April 2025). [\"AI-Enabled Coups: How a Small Group Could Use AI to Seize Power\"](https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power). _Forethought_. Retrieved 12 August 2025.\n112.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-114)**Pillay, Tharin (15 December 2024). [\"New Tests Reveal AI's Capacity for Deception\"](https://time.com/7202312/new-tests-reveal-ai-capacity-for-deception/). _TIME_. Retrieved 12 January 2025.\n113.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-115)**Perrigo, Billy (18 December 2024). [\"Exclusive: New Research Shows AI Strategically Lying\"](https://time.com/7202784/ai-research-strategic-lying/). _TIME_. Retrieved 12 January 2025.\n114.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-116)**Greenblatt, Ryan; Denison, Carson; Wright, Benjamin; Roger, Fabien; MacDiarmid, Monte; Marks, Sam; Treutlein, Johannes; Belonax, Tim; Chen, Jack (20 December 2024), _Alignment faking in large language models_, [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2412.14093](https://arxiv.org/abs/2412.14093)\n115.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-117)**Kumar, Vibhore. [\"Council Post: At The Dawn Of Artificial General Intelligence: Balancing Abundance With Existential Safeguards\"](https://www.forbes.com/sites/forbestechcouncil/2023/04/24/at-the-dawn-of-artificial-general-intelligence-balancing-abundance-with-existential-safeguards/). _Forbes_. Retrieved 23 July 2023.\n116.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:9_118-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:9_118-1)[\"Pause Giant AI Experiments: An Open Letter\"](https://futureoflife.org/open-letter/pause-giant-ai-experiments/). _Future of Life Institute_. Retrieved 30 March 2023.\n117.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-life_3.0_119-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-life_3.0_119-1)[Tegmark, Max](https://en.wikipedia.org/wiki/Max_Tegmark \"Max Tegmark\") (2017). [_Life 3.0: Being Human in the Age of Artificial Intelligence_](https://en.wikipedia.org/wiki/Life_3.0:_Being_Human_in_the_Age_of_Artificial_Intelligence \"Life 3.0: Being Human in the Age of Artificial Intelligence\") (1st ed.). Mainstreaming AI Safety: Knopf. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-451-48507-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-451-48507-6 \"Special:BookSources/978-0-451-48507-6\").\n118.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-120)**[\"AI Principles\"](https://futureoflife.org/ai-principles/). _[Future of Life Institute](https://en.wikipedia.org/wiki/Future\\_of\\_Life\\_Institute \"Future of Life Institute\")_. 11 August 2017. [Archived](https://web.archive.org/web/20171211171044/https://futureoflife.org/ai-principles/) from the original on 11 December 2017. Retrieved 11 December 2017.\n119.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-121)**[\"Elon Musk and Stephen Hawking warn of artificial intelligence arms race\"](http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525). _[Newsweek](https://en.wikipedia.org/wiki/Newsweek \"Newsweek\")_. 31 January 2017. [Archived](https://web.archive.org/web/20171211034528/http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525) from the original on 11 December 2017. Retrieved 11 December 2017.\n120.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-122)**[Ford, Martin](https://en.wikipedia.org/wiki/Martin_Ford_(author) \"Martin Ford (author)\") (2015). \"Chapter 9: Super-intelligence and the Singularity\". [_Rise of the Robots: Technology and the Threat of a Jobless Future_](https://en.wikipedia.org/wiki/Rise_of_the_Robots:_Technology_and_the_Threat_of_a_Jobless_Future \"Rise of the Robots: Technology and the Threat of a Jobless Future\"). Basic Books. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-465-05999-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-465-05999-7 \"Special:BookSources/978-0-465-05999-7\").\n121.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-123)**[Bostrom, Nick](https://en.wikipedia.org/wiki/Nick_Bostrom \"Nick Bostrom\") (2016). \"New Epilogue to the Paperback Edition\". [_Superintelligence: Paths, Dangers, Strategies_](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies \"Superintelligence: Paths, Dangers, Strategies\") (Paperback ed.).\n122.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:10_124-0)**[\"Why Uncontrollable AI Looks More Likely Than Ever\"](https://time.com/6258483/uncontrollable-ai-agi-risks/). _Time_. 27 February 2023. Retrieved 30 March 2023. It is therefore no surprise that according to the most recent AI Impacts Survey, nearly half of 731 leading AI researchers think there is at least a 10% chance that human-level AI would lead to an \"extremely negative outcome,\" or existential risk.\n123.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-125)**[\"IMD creates AI Safety Clock\"](https://www.imd.org/news/artificial-intelligence/imd-launches-ai-safety-clock/#:~:text=Inspired%20by%20the%20original%20'Doomsday,when%20Uncontrolled%20Artificial%20General%20Intelligence%20(). _www.imd.org_. 6 September 2024. Retrieved 6 September 2024.\n124.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-126)**Constantino, Tor (10 February 2025). [\"AI 'Doomsday Clock' Ticks Closer To Uncontrolled Super AI\"](https://www.forbes.com/sites/torconstantino/2025/02/10/ai-doomsday-clock-ticks-closer-to-uncontrolled-super-ai/). _[Forbes](https://en.wikipedia.org/wiki/Forbes \"Forbes\")_. Retrieved 11 February 2025.\n125.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:132_127-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:132_127-1)Maas, Matthijs M. (6 February 2019). \"How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons of mass destruction\". _Contemporary Security Policy_. **40** (3): 285-311. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1080/13523260.2019.1576464](https://doi.org/10.1080%2F13523260.2019.1576464). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[1352-3260](https://search.worldcat.org/issn/1352-3260). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[159310223](https://api.semanticscholar.org/CorpusID:159310223).\n126.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:6_128-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:6_128-1)[\"Impressed by artificial intelligence? Experts say AGI is coming next, and it has 'existential' risks\"](https://www.abc.net.au/news/2023-03-24/what-is-agi-artificial-general-intelligence-ai-experts-risks/102035132). _ABC News_. 23 March 2023. Retrieved 30 March 2023.\n127.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-BBC_News_129-0)**Rawlinson, Kevin (29 January 2015). [\"Microsoft's Bill Gates insists AI is a threat\"](https://www.bbc.co.uk/news/31047780). _[BBC News](https://en.wikipedia.org/wiki/BBC\\_News \"BBC News\")_. [Archived](https://web.archive.org/web/20150129183607/http://www.bbc.co.uk/news/31047780) from the original on 29 January 2015. Retrieved 30 January 2015.\n128.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-130)**Washington Post (14 December 2015). [\"Tech titans like Elon Musk are spending $1 billion to save you from terminators\"](https://www.chicagotribune.com/bluesky/technology/ct-tech-titans-against-terminators-20151214-story.html). _[Chicago Tribune](https://en.wikipedia.org/wiki/Chicago\\_Tribune \"Chicago Tribune\")_. [Archived](https://web.archive.org/web/20160607121118/http://www.chicagotribune.com/bluesky/technology/ct-tech-titans-against-terminators-20151214-story.html) from the original on 7 June 2016.\n129.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-131)**[\"Doomsday to utopia: Meet AI's rival factions\"](https://www.washingtonpost.com/technology/2023/04/09/ai-safety-openai/). _Washington Post_. 9 April 2023. Retrieved 30 April 2023.\n130.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-132)**[\"UC Berkeley - Center for Human-Compatible AI (2016)\"](https://www.openphilanthropy.org/grants/uc-berkeley-center-for-human-compatible-ai-2016/). _Open Philanthropy_. 27 June 2016. Retrieved 30 April 2023.\n131.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-133)**[\"The mysterious artificial intelligence company Elon Musk invested in is developing game-changing smart computers\"](http://www.techinsider.io/mysterious-artificial-intelligence-company-elon-musk-investment-2015-10). _Tech Insider_. [Archived](https://web.archive.org/web/20151030165333/http://www.techinsider.io/mysterious-artificial-intelligence-company-elon-musk-investment-2015-10) from the original on 30 October 2015. Retrieved 30 October 2015.\n132.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-FOOTNOTEClark2015a_134-0)**[Clark 2015a](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#CITEREFClark2015a).\n133.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-135)**[\"Elon Musk Is Donating $10M Of His Own Money To Artificial Intelligence Research\"](http://www.fastcompany.com/3041007/fast-feed/elon-musk-is-donating-10m-of-his-own-money-to-artificial-intelligence-research). _Fast Company_. 15 January 2015. [Archived](https://web.archive.org/web/20151030202356/http://www.fastcompany.com/3041007/fast-feed/elon-musk-is-donating-10m-of-his-own-money-to-artificial-intelligence-research) from the original on 30 October 2015. Retrieved 30 October 2015.\n134.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-new_yorker_doomsday2_136-0)**Khatchadourian, Raffi (23 November 2015). [\"The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?\"](https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom). _[The New Yorker](https://en.wikipedia.org/wiki/The\\_New\\_Yorker\\_(magazine) \"The New Yorker (magazine)\")_. [Archived](https://web.archive.org/web/20190429183807/https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom) from the original on 29 April 2019. Retrieved 7 February 2016.\n135.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-137)**[\"Warning of AI's danger, pioneer Geoffrey Hinton quits Google to speak freely\"](https://arstechnica.com/information-technology/2023/05/warning-of-ais-danger-pioneer-geoffrey-hinton-quits-google-to-speak-freely/). _www.arstechnica.com_. 2023. Retrieved 23 July 2023.\n136.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-138)**Garling, Caleb (5 May 2015). [\"Andrew Ng: Why 'Deep Learning' Is a Mandate for Humans, Not Just Machines\"](https://www.wired.com/brandlab/2015/05/andrew-ng-deep-learning-mandate-humans-not-just-machines/). _Wired_. Retrieved 31 March 2023.\n137.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-139)**[\"Is artificial intelligence really an existential threat to humanity?\"](https://mambapost.com/2023/04/tech-news/ai-are-an-existential-threat-to-humanity/). _MambaPost_. 4 April 2023.\n138.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-140)**[\"The case against killer robots, from a guy actually working on artificial intelligence\"](http://fusion.net/story/54583/the-case-against-killer-robots-from-a-guy-actually-building-ai/). _Fusion.net_. [Archived](https://web.archive.org/web/20160204175716/http://fusion.net/story/54583/the-case-against-killer-robots-from-a-guy-actually-building-ai/) from the original on 4 February 2016. Retrieved 31 January 2016.\n139.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-141)**[\"AI experts challenge 'doomer' narrative, including 'extinction risk' claims\"](https://venturebeat.com/ai/ai-experts-challenge-doomer-narrative-including-extinction-risk-claims/). _VentureBeat_. 31 May 2023. Retrieved 8 July 2023.\n140.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-142)**Coldewey, Devin (1 April 2023). [\"Ethicists fire back at 'AI Pause' letter they say 'ignores the actual harms'\"](https://techcrunch.com/2023/03/31/ethicists-fire-back-at-ai-pause-letter-they-say-ignores-the-actual-harms/). _TechCrunch_. Retrieved 23 July 2023.\n141.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-143)**[\"DAIR (Distributed AI Research Institute)\"](https://dair-institute.org/). _[DAIR Institute](https://en.wikipedia.org/wiki/DAIR\\_Institute \"DAIR Institute\")_. Retrieved 23 July 2023.\n142.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-144)**[Kelly, Kevin](https://en.wikipedia.org/wiki/Kevin_Kelly_(editor) \"Kevin Kelly (editor)\") (25 April 2017). [\"The Myth of a Superhuman AI\"](https://web.archive.org/web/20211226181932/https://www.wired.com/2017/04/the-myth-of-a-superhuman-ai/). _Wired_. Archived from [the original](https://www.wired.com/2017/04/the-myth-of-a-superhuman-ai/) on 26 December 2021. Retrieved 19 February 2022.\n143.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-145)**Jindal, Siddharth (7 July 2023). [\"OpenAI's Pursuit of AI Alignment is Farfetched\"](https://analyticsindiamag.com/openais-farfetched-pursuit-of-ai-alignment/). _Analytics India Magazine_. Retrieved 23 July 2023.\n144.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-146)**[\"Mark Zuckerberg responds to Elon Musk's paranoia about AI: 'AI is going to... help keep our communities safe.'\"](https://www.businessinsider.com/mark-zuckerberg-shares-thoughts-elon-musks-ai-2018-5). _Business Insider_. 25 May 2018. [Archived](https://web.archive.org/web/20190506173756/https://www.businessinsider.com/mark-zuckerberg-shares-thoughts-elon-musks-ai-2018-5) from the original on 6 May 2019. Retrieved 6 May 2019.\n145.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-147)**[\"AI doomsday worries many Americans. So does apocalypse from climate change, nukes, war, and more\"](https://today.yougov.com/topics/technology/articles-reports/2023/04/14/ai-nuclear-weapons-world-war-humanity-poll). 14 April 2023. [Archived](https://web.archive.org/web/20230623095224/https://today.yougov.com/topics/technology/articles-reports/2023/04/14/ai-nuclear-weapons-world-war-humanity-poll) from the original on 23 June 2023. Retrieved 9 July 2023.\n146.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-148)**Tyson, Alec; Kikuchi, Emma (28 August 2023). [\"Growing public concern about the role of artificial intelligence in daily life\"](https://www.pewresearch.org/short-reads/2023/08/28/growing-public-concern-about-the-role-of-artificial-intelligence-in-daily-life/). _Pew Research Center_. Retrieved 17 September 2023.\n147.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-physica_scripta_149-0)**Sotala, Kaj; [Yampolskiy, Roman](https://en.wikipedia.org/wiki/Roman_Yampolskiy \"Roman Yampolskiy\") (19 December 2014). \"Responses to catastrophic AGI risk: a survey\". _[Physica Scripta](https://en.wikipedia.org/wiki/Physica\\_Scripta \"Physica Scripta\")_. **90** (1).\n148.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-BarrettEtAl2016_150-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-BarrettEtAl2016_150-1)Barrett, Anthony M.; Baum, Seth D. (23 May 2016). \"A model of pathways to artificial superintelligence catastrophe for risk and decision analysis\". _Journal of Experimental & Theoretical Artificial Intelligence_. **29**(2): 397-414. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1607.07730](https://arxiv.org/abs/1607.07730). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1080/0952813X.2016.1186228](https://doi.org/10.1080%2F0952813X.2016.1186228). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[928824](https://api.semanticscholar.org/CorpusID:928824).\n149.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-151)**Sotala, Kaj; Yampolskiy, Roman V (19 December 2014). [\"Responses to catastrophic AGI risk: a survey\"](https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001). _Physica Scripta_. **90**(1) 018001. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2015PhyS...90a8001S](https://ui.adsabs.harvard.edu/abs/2015PhyS...90a8001S). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1088/0031-8949/90/1/018001](https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0031-8949](https://search.worldcat.org/issn/0031-8949). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[4749656](https://api.semanticscholar.org/CorpusID:4749656).\n150.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-152)**Ramamoorthy, Anand; Yampolskiy, Roman (2018). [\"Beyond MAD? The race for artificial general intelligence\"](https://www.itu.int/pub/S-JOURNAL-ICTS.V1I1-2018-9). _ICT Discoveries_. **1**(Special Issue 1). ITU: 1-8. [Archived](https://web.archive.org/web/20220107141537/https://www.itu.int/pub/S-JOURNAL-ICTS.V1I1-2018-9) from the original on 7 January 2022. Retrieved 7 January 2022.\n151.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-153)**Carayannis, Elias G.; Draper, John (11 January 2022). [\"Optimising peace through a Universal Global Peace Treaty to constrain the risk of war from a militarised artificial superintelligence\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8748529). _AI & Society_. **38**(6): 2679-2692. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1007/s00146-021-01382-y](https://doi.org/10.1007%2Fs00146-021-01382-y). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0951-5666](https://search.worldcat.org/issn/0951-5666). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) \"PMC (identifier)\")[8748529](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8748529). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[35035113](https://pubmed.ncbi.nlm.nih.gov/35035113). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[245877737](https://api.semanticscholar.org/CorpusID:245877737).\n152.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-154)**Carayannis, Elias G.; Draper, John (30 May 2023), [\"The challenge of advanced cyberwar and the place of cyberpeace\"](https://www.elgaronline.com/edcollchap/book/9781839109362/book-part-9781839109362-8.xml), _The Elgar Companion to Digital Transformation, Artificial Intelligence and Innovation in the Economy, Society and Democracy_, Edward Elgar Publishing, pp.32-80, [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.4337/9781839109362.00008](https://doi.org/10.4337%2F9781839109362.00008), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-83910-936-2](https://en.wikipedia.org/wiki/Special:BookSources/978-1-83910-936-2 \"Special:BookSources/978-1-83910-936-2\"), retrieved 8 June 2023.\n153.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-155)**Vincent, James (22 June 2016). [\"Google's AI researchers say these are the five key problems for robot safety\"](https://www.theverge.com/circuitbreaker/2016/6/22/11999664/google-robots-ai-safety-five-problems). _The Verge_. [Archived](https://web.archive.org/web/20191224201240/https://www.theverge.com/circuitbreaker/2016/6/22/11999664/google-robots-ai-safety-five-problems) from the original on 24 December 2019. Retrieved 5 April 2020.\n154.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-156)**Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. \"Concrete problems in AI safety.\" arXiv preprint arXiv:1606.06565 (2016).\n155.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-157)**Johnson, Alex (2019). [\"Elon Musk wants to hook your brain up directly to computers - starting next year\"](https://www.nbcnews.com/mach/tech/elon-musk-wants-hook-your-brain-directly-computers-starting-next-ncna1030631). _NBC News_. [Archived](https://web.archive.org/web/20200418094146/https://www.nbcnews.com/mach/tech/elon-musk-wants-hook-your-brain-directly-computers-starting-next-ncna1030631) from the original on 18 April 2020. Retrieved 5 April 2020.\n156.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-158)**Torres, Phil (18 September 2018). [\"Only Radically Enhancing Humanity Can Save Us All\"](https://slate.com/technology/2018/09/genetic-engineering-to-stop-doomsday.html). _Slate Magazine_. [Archived](https://web.archive.org/web/20200806073520/https://slate.com/technology/2018/09/genetic-engineering-to-stop-doomsday.html) from the original on 6 August 2020. Retrieved 5 April 2020.\n157.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-159)**Piper, Kelsey (29 March 2023). [\"How to test what an AI model can - and shouldn't - do\"](https://www.vox.com/future-perfect/2023/3/29/23661633/gpt-4-openai-alignment-research-center-open-philanthropy-ai-safety). _Vox_. Retrieved 28 July 2023.\n158.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-160)**Piesing, Mark (17 May 2012). [\"AI uprising: humans will be outsourced, not obliterated\"](https://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us). _Wired_. [Archived](https://web.archive.org/web/20140407041151/http://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us) from the original on 7 April 2014. Retrieved 12 December 2015.\n159.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-161)**Coughlan, Sean (24 April 2013). [\"How are humans going to become extinct?\"](https://www.bbc.com/news/business-22002530). _BBC News_. [Archived](https://web.archive.org/web/20140309003706/http://www.bbc.com/news/business-22002530) from the original on 9 March 2014. Retrieved 29 March 2014.\n160.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-162)**Bridge, Mark (10 June 2017). [\"Making robots less confident could prevent them taking over\"](https://www.thetimes.com/business-money/technology/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx). _The Times_. [Archived](https://web.archive.org/web/20180321133426/https://www.thetimes.co.uk/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx) from the original on 21 March 2018. Retrieved 21 March 2018.\n161.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-163)**[McGinnis, John](https://en.wikipedia.org/wiki/John_McGinnis \"John McGinnis\") (Summer 2010). [\"Accelerating AI\"](http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online). _[Northwestern University Law Review](https://en.wikipedia.org/wiki/Northwestern\\_University\\_Law\\_Review \"Northwestern University Law Review\")_. **104**(3): 1253-1270. [Archived](https://web.archive.org/web/20160215073656/http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online) from the original on 15 February 2016. Retrieved 16 July 2014. For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible...\n162.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-164)**Sotala, Kaj; [Yampolskiy, Roman](https://en.wikipedia.org/wiki/Roman_Yampolskiy \"Roman Yampolskiy\") (19 December 2014). \"Responses to catastrophic AGI risk: a survey\". _[Physica Scripta](https://en.wikipedia.org/wiki/Physica\\_Scripta \"Physica Scripta\")_. **90**(1). In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals.\n163.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-165)**Allenby, Brad (11 April 2016). [\"The Wrong Cognitive Measuring Stick\"](http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html). _Slate_. [Archived](https://web.archive.org/web/20160515114003/http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html) from the original on 15 May 2016. Retrieved 15 May 2016. It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation.\n164.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:7_166-0)**Yampolskiy, Roman V. (2022). [\"AI Risk Skepticism\"](https://link.springer.com/chapter/10.1007/978-3-031-09153-7_18). In M\u00fcller, Vincent C. (ed.). _Philosophy and Theory of Artificial Intelligence 2021_. Studies in Applied Philosophy, Epistemology and Rational Ethics. Vol.63. Cham: Springer International Publishing. pp.225-248. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1007/978-3-031-09153-7_18](https://doi.org/10.1007%2F978-3-031-09153-7_18). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-3-031-09153-7](https://en.wikipedia.org/wiki/Special:BookSources/978-3-031-09153-7 \"Special:BookSources/978-3-031-09153-7\").\n165.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-167)**Baum, Seth (22 August 2018). [\"Superintelligence Skepticism as a Political Tool\"](https://doi.org/10.3390%2Finfo9090209). _Information_. **9**(9): 209. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.3390/info9090209](https://doi.org/10.3390%2Finfo9090209). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[2078-2489](https://search.worldcat.org/issn/2078-2489).\n166.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-168)**[\"Elon Musk and other tech leaders call for pause in 'out of control' AI race\"](https://www.cnn.com/2023/03/29/tech/ai-letter-elon-musk-tech-leaders/index.html). _CNN_. 29 March 2023. Retrieved 30 March 2023.\n167.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-169)**[\"Open letter calling for AI 'pause' shines light on fierce debate around risks vs. hype\"](https://venturebeat.com/ai/open-letter-calling-for-ai-pause-shines-light-on-fierce-debate-around-risks-vs-hype/). _VentureBeat_. 29 March 2023. Retrieved 20 July 2023.\n168.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-170)**Vincent, James (14 April 2023). [\"OpenAI's CEO confirms the company isn't training GPT-5 and \"won't for some time\"\"](https://www.theverge.com/2023/4/14/23683084/openai-gpt-5-rumors-training-sam-altman). _The Verge_. Retrieved 20 July 2023.\n169.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-171)**Reynolds, Matt. [\"Protesters Are Fighting to Stop AI, but They're Split on How to Do It\"](https://www.wired.com/story/protesters-pause-ai-split-stop/). _Wired_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[1059-1028](https://search.worldcat.org/issn/1059-1028). Retrieved 28 April 2025.\n170.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-172)**Domonoske, Camila (17 July 2017). [\"Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk'\"](https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk). _NPR_. [Archived](https://web.archive.org/web/20200423135755/https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk) from the original on 23 April 2020. Retrieved 27 November 2017.\n171.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-173)**Gibbs, Samuel (17 July 2017). [\"Elon Musk: regulate AI to combat 'existential threat' before it's too late\"](https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo). _The Guardian_. [Archived](https://web.archive.org/web/20200606072024/https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo) from the original on 6 June 2020. Retrieved 27 November 2017.\n172.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-cnbc2_174-0)**Kharpal, Arjun (7 November 2017). [\"A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says\"](https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html). _CNBC_. [Archived](https://web.archive.org/web/20200322115325/https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html) from the original on 22 March 2020. Retrieved 27 November 2017.\n173.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-175)**Dawes, James (20 December 2021). [\"UN fails to agree on 'killer robot' ban as nations pour billions into autonomous weapons research\"](https://theconversation.com/un-fails-to-agree-on-killer-robot-ban-as-nations-pour-billions-into-autonomous-weapons-research-173616). _The Conversation_. Retrieved 28 July 2023.\n174.   ^ [_**a**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:13_176-0)[_**b**_](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:13_176-1)Fassihi, Farnaz (18 July 2023). [\"U.N. Officials Urge Regulation of Artificial Intelligence\"](https://www.nytimes.com/2023/07/18/world/un-security-council-ai.html). _The New York Times_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0362-4331](https://search.worldcat.org/issn/0362-4331). Retrieved 20 July 2023.\n175.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-177)**[\"International Community Must Urgently Confront New Reality of Generative, Artificial Intelligence, Speakers Stress as Security Council Debates Risks, Rewards\"](https://press.un.org/en/2023/sc15359.doc.htm). _United Nations_. Retrieved 20 July 2023.\n176.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-:532_178-0)**Sotala, Kaj; Yampolskiy, Roman V. (19 December 2014). [\"Responses to catastrophic AGI risk: a survey\"](https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001). _Physica Scripta_. **90**(1) 018001. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2015PhyS...90a8001S](https://ui.adsabs.harvard.edu/abs/2015PhyS...90a8001S). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1088/0031-8949/90/1/018001](https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0031-8949](https://search.worldcat.org/issn/0031-8949).\n177.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-179)**Geist, Edward Moore (15 August 2016). \"It's already too late to stop the AI arms race-We must manage it instead\". _Bulletin of the Atomic Scientists_. **72**(5): 318-321. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2016BuAtS..72e.318G](https://ui.adsabs.harvard.edu/abs/2016BuAtS..72e.318G). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1080/00963402.2016.1216672](https://doi.org/10.1080%2F00963402.2016.1216672). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0096-3402](https://search.worldcat.org/issn/0096-3402). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[151967826](https://api.semanticscholar.org/CorpusID:151967826).\n178.  **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-180)**[\"Amazon, Google, Meta, Microsoft and other tech firms agree to AI safeguards set by the White House\"](https://apnews.com/article/artificial-intelligence-safeguards-joe-biden-kamala-harris-4caf02b94275429f764b06840897436c). _AP News_. 21 July 2023. Retrieved 21 July 2023.\n179.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-181)**[\"Amazon, Google, Meta, Microsoft and other firms agree to AI safeguards\"](https://www.redditchadvertiser.co.uk/news/national/23670894.amazon-google-meta-microsoft-firms-agree-ai-safeguards/). _Redditch Advertiser_. 21 July 2023. Retrieved 21 July 2023.\n180.   **[^](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_ref-182)**The White House (30 October 2023). [\"Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\"](https://bidenwhitehouse.archives.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/). _The White House_. Retrieved 19 December 2023."
            },
            {
              "type": "text",
              "content": "Please have a chat with the AI Tutor about the article you just read:"
            },
            {
              "type": "chat",
              "instructions": "The user just read about AGI and superintelligence capabilities.\n\nKey points:\n- AGI = human-level across most tasks\n- Superintelligence = vastly exceeds humans in all domains\n- AI advantages: speed, scalability, duplicability, editability\n- Timelines are uncertain but potentially soon\n\nDiscussion topics:\n- Why might the transition from AGI to superintelligence be rapid?\n- Which of the AI advantages over human brains seems most significant?\n- How does duplicability change the dynamics of AI development?\n\nHelp them understand why these technical properties have such large implications.",
              "hidePreviousContentFromUser": false,
              "hidePreviousContentFromTutor": false
            }
          ],
          "optional": false,
          "contentId": "b2c3d4e5-f6a7-8901-bcde-f23456789012",
          "learningOutcomeId": null
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Four Background Claims",
            "author": "Nate Soares",
            "sourceUrl": "https://intelligence.org/2015/07/24/four-background-claims/"
          },
          "segments": [
            {
              "type": "text",
              "content": "This text explains exactly how the emergence of AI smarter than humans could become an event with enormous stakes, and why, in the author's opinion, there is already meaningful work being done today that increases the chance of a positive outcome. The author identifies four key premises that underpin his entire perspective on the prospects of AI."
            },
            {
              "type": "article-excerpt",
              "content": "MIRI's mission is to ensure that the creation of smarter-than-human artificial intelligence has a positive impact. Why is this mission important, and why do we think that there's work we can do today to help ensure any such thing?\n\nIn this post and my next one, I'll try to answer those questions. This post will lay out what I see as the four most important premises underlying our mission. Related posts include Eliezer Yudkowsky's \"[Five Theses](http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/)\" and Luke Muehlhauser's \"[Why MIRI?](https://intelligence.org/2014/04/20/why-miri/)\"; this is my attempt to make explicit the claims that are in the background whenever I assert that our mission is of critical importance.\n\n## Claim #1: Humans have a very general ability to solve problems and achieve goals across diverse domains.\n\nWe call this ability \"intelligence,\" or \"general intelligence.\" This isn't a [formal definition](https://intelligence.org/2013/06/19/what-is-intelligence-2/) - if we knew _exactly_ what general intelligence was, we'd be better able to program it into a computer - but we do think that there's a real phenomenon of general intelligence that we cannot yet replicate in code.\n\n**Alternative view:** There is no such thing as general intelligence. Instead, humans have a collection of disparate special-purpose modules. Computers will keep getting better at narrowly defined tasks such as chess or driving, but at no point will they acquire \"generality\" and become significantly more useful, because there is no generality to acquire. ([Robin Hanson](http://www.overcomingbias.com/2014/07/limits-on-generality.html) has argued for versions of this position.)\n\n**Short response:** I find the \"disparate modules\" hypothesis implausible in light of how readily humans can gain mastery in domains that are utterly foreign to our ancestors. That's not to say that general intelligence is some irreducible occult property; it presumably comprises a number of different cognitive faculties and the interactions between them. The whole, however, has the effect of making humans much more cognitively versatile and adaptable than (say) chimpanzees.\n\n**Why this claim matters:** Humans have achieved a dominant position over other species not by being stronger or more agile, but by being more intelligent. If some key part of this general intelligence was able to evolve in the few million years since our common ancestor with chimpanzees lived, this suggests there may exist a relatively short list of key insights that would allow human engineers to build powerful generally intelligent AI systems.\n\n**Further reading:** Salamon et al., \"[How Intelligible is Intelligence?](https://intelligence.org/files/HowIntelligible.pdf)\"\n\n## Claim #2: AI systems could become much more intelligent than humans.\n\nResearchers at MIRI tend to lack strong beliefs about _when_ smarter-than-human machine intelligence will be developed. We do, however, expect that (a) human-equivalent machine intelligence will eventually be developed (likely within a century, barring catastrophe); and (b) machines can become significantly more intelligent than any human.\n\n**Alternative view #1:** Brains do something special that cannot be replicated on a computer.\n\n**Short response:** Brains are physical systems, and if certain versions of the [Church-Turing thesis](https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis) hold then computers can in principle replicate the functional input/output behavior of any physical system. Also, note that \"intelligence\" (as I'm using the term) is about problem-solving capabilities: even if there were some special human feature (such as [qualia](http://www.iep.utm.edu/hard-con/)) that computers couldn't replicate, this would be irrelevant unless it prevented us from designing problem-solving machines.\n\n**Alternative view #2:** The algorithms at the root of general intelligence are so complex and indecipherable that human beings will not be able to program any such thing for many centuries.\n\n**Short response:** This seems implausible in light of evolutionary evidence. The genus _Homo_ diverged from other genera only 2.8 million years ago, and the intervening time - a blink in the eye of natural selection - was sufficient for generating the cognitive advantages seen in humans. This strongly implies that whatever sets humans apart from less intelligent species is not extremely complicated: the building blocks of general intelligence must have been present in chimpanzees.\n\nIn fact, the relatively intelligent behavior of dolphins suggests that the building blocks were probably there even as far back as the mouse-sized common ancestor of humans and dolphins. One could argue that mouse-level intelligence will take many centuries to replicate, but this is a more difficult claim to swallow, given [rapid advances](https://www.youtube.com/watch?v=GYQrNfSmQ0M) in the field of AI. In light of evolutionary evidence and the last few decades of AI research, it looks to me like intelligence is something we will be able to comprehend and program into machines.\n\n**Alternative view #3:** Humans are already at or near peak physically possible intelligence. Thus, although we may be able to build human-equivalent intelligent machines, we won't be able to build superintelligent machines.\n\n**Short response:** It would be surprising if humans were perfectly designed reasoners, for the same reason it would be surprising if airplanes couldn't fly faster than birds. Simple physical calculations bear this intuition out: for example, it seems well possible, within the boundaries of physics, to run a computer simulation of a human brain at thousands of times the normal speed.\n\nSome expect that speed wouldn't matter, because the real bottleneck is waiting for data to come in from physical experiments. This seems unlikely to me. There are many interesting physical experiments that can be sped up, and I have a hard time believing that a team of humans running at a 1000x speedup would fail to outperform their normal-speed counterparts (not least because they could rapidly develop new tools and technology to assist them).\n\nI furthermore expect it's possible to build _better_ reasoners (rather than just _faster_ reasoners) that use computing resources more effectively than humans do, even running at the same speed.\n\n**Why this claim matters:** Human-designed machines often knock the socks off of biological creatures when it comes to performing tasks we care about: automobiles cannot heal or reproduce, but they sure can carry humans a lot farther and faster than a horse. If we can build intelligent machines specifically designed to solve the world's largest problems through scientific and technological innovation, then they could improve the world at an unprecedented pace. In other words, AI matters.\n\n**Further reading:** Chalmers, \"[The Singularity: A Philosophical Analysis](http://consc.net/papers/singularity.pdf)\"\n\n## Claim #3: If we create highly intelligent AI systems, their decisions will shape the future.\n\nHumans use their intelligence to create tools and plans and technology that allow them to shape their environments to their will (and fill them with refrigerators, and cars, and cities). We expect that systems which are even more intelligent would have even more ability to shape their surroundings, and thus, smarter-than-human AI systems could wind up with significantly more control over the future than humans have.\n\n**Alternative view:** An AI system would never be able to out-compete humanity as a whole, no matter how intelligent it became. Our environment is simply too competitive; machines would have to work with us and integrate into our economy.\n\n**Short response:** I have no doubt that an autonomous AI system attempting to accomplish simple tasks would initially have strong incentives to integrate with our economy: if you build an AI system that collects stamps for you, it will likely start by acquiring money to purchase stamps. But what if the system accrues a strong technological or strategic advantage?\n\nAs an extreme example, we can imagine the system developing nanomachines and using them to convert as much matter as it can into stamps; it wouldn't necessarily care whether that matter came from \"dirt\" or \"money\" or \"people.\" Selfish actors only have an incentive to participate in the economy when their gains from trade are greater than the net gains they would get by ignoring the economy and just taking the resources for their own.\n\nSo the question is whether it will be possible for an AI system to gain a decisive technological or strategic advantage. I see this as the most uncertain claim out of the ones I've listed here. However, I expect that the answer is still a clear \"yes.\"\n\nHistorically, conflicts between humans have often ended with the technologically superior group dominating its rival. At present, there are a number of technological and social innovations that seem possible but have not yet been developed. Humans coordinate slowly and poorly, compared to what distributed software systems could achieve. All of this suggests that if we build a machine that does science faster or better than we can, it could quickly gain a technological and/or strategic advantage over humanity for itself or for its operators. This is particularly true if its intellectual advantage allows it to socially manipulate humans, acquire new hardware (legally or otherwise), produce better hardware, create copies of itself, or improve its own software. For good or ill, much of the future is likely to be determined by superintelligent decision-making machines.\n\n**Why this claim matters:** Because the future matters. If we want things to be better in the future (or at least not get worse), then it is prudent to prioritize research into the processes that will have high leverage over the future.\n\n**Further reading:** Armstrong, _[Smarter Than Us](https://intelligence.org/smarter-than-us/)_\n\n## Claim #4: Highly intelligent AI systems won't be beneficial by default.\n\nWe'd like to see the smarter-than-human AI systems of the future working together with humanity to build a better future; but that won't happen by default. In order to build AI systems that have a beneficial impact, we have to solve a number of technical challenges over and above building more powerful and general AI systems.\n\n**Alternative view:** As humans have become smarter, we've also become more peaceful and tolerant. As AI becomes smarter, it will likewise be able to better figure out our values, and will better execute on them.\n\n**Short response:** Sufficiently intelligent artificial reasoners would be able to _figure out_ our intentions and preferences; but this [does not imply](http://lesswrong.com/lw/igf/the_genie_knows_but_doesnt_care/) that they would execute plans that are in accordance with them.\n\nA self-modifying AI system could inspect its code and decide whether to continue pursuing the goals it was given or whether it would rather change them. But how is the program deciding which modification to execute?\n\nThe AI system is a physical system, and somewhere inside it, it's constructing predictions about how the universe would look if it did various things. Some other part of the system is comparing those outcomes and then executing actions that lead towards outcomes that the current system ranks highly. If the agent is initially programmed to execute plans that lead towards a universe in which it predicts that cancer is cured, then it will only modify its goal if it predicts that this will lead to a cure for cancer.\n\nRegardless of their intelligence level, and regardless of your intentions, computers do _exactly_ what you programmed them to do. If you program an extremely intelligent machine to execute plans that it predicts lead to futures where cancer is cured, then it may be that the shortest path it can find to a cancer-free future entails kidnapping humans for experimentation (and resisting your attempts to alter it, as those would slow it down).\n\nThere isn't any spark of compassion that automatically imbues computers with respect for other sentients once they crosses a certain capability threshold. If you want compassion, you have to program it in.\n\n**Why this claim matters:** A lot of the world's largest problems would be much easier to solve with superintelligent assistance - but attaining those benefits requires that we do more than just improve the capabilities of AI systems. You only get a system that does what you intended if you know how to program it to take your intentions into account, and execute plans that fulfill them.\n\n**Further reading:** Bostrom, \"[The Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)\"\n\nThese four claims form the core of the argument that artificial intelligence is important: there is such a thing as general reasoning ability; if we build general reasoners, they could be far smarter than humans; if they are far smarter than humans, they could have an immense impact; and that impact will not be beneficial by default.\n\nAt present, billions of dollars and thousands of person-years are pouring into AI _capabilities_ research, with comparatively little effort going into AI safety research. Artificial superintelligence may arise sometime in the next few decades, and will almost surely be created in one form or another over the next century or two, barring catastrophe. Superintelligent systems will either have an extremely positive impact on humanity, or an extremely negative one; it is up to us to decide which.",
              "collapsed_before": null,
              "collapsed_after": null
            }
          ],
          "optional": false,
          "contentId": "c3d4e5f6-a7b8-9012-cdef-345678901234",
          "learningOutcomeId": null
        },
        {
          "type": "page",
          "contentId": "5ec4268f-757c-4ef7-89c1-a24b35cea850",
          "meta": {
            "title": "short intermediate"
          },
          "segments": [
            {
              "type": "text",
              "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum vulputate leo sed leo dapibus, at congue turpis feugiat. Pellentesque lobortis lorem id ipsum maximus, efficitur blandit odio consequat. Sed ut rhoncus justo. Curabitur faucibus tincidunt enim, vitae sodales dui viverra eget. Suspendisse non semper tellus. Duis eget velit ipsum. Nunc facilisis eu arcu id malesuada. Aliquam sit amet lacus sed ligula eleifend eleifend. Aenean ultrices, quam sit amet euismod finibus, nunc sapien faucibus sem, eu mattis est lectus at odio. Sed consequat neque quis arcu tempor, sit amet euismod ex dapibus. Morbi quis mi at velit dignissim bibendum. Mauris augue orci, porta id interdum a, consectetur et tellus. Morbi convallis odio ipsum, eget feugiat felis vulputate sed. Ut venenatis nunc mattis bibendum efficitur. Duis nibh erat, ullamcorper eu metus et, tincidunt volutpat ipsum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.ultraintelligent\n\nDuis eu pellentesque erat. Pellentesque pulvinar vitae nunc et convallis. Sed ac quam nec massa sodales facilisis. Etiam posuere mattis mauris, in feugiat eros mollis id. Morbi tempus, ex nec condimentum maximus, elit eros semper mi, quis lacinia lacus quam sed felis. Maecenas in semper ipsum. Donec quis egestas purus. Sed vehicula, ante eget vulputate pulvinar, orci erat bibendum nulla, vitae elementum nisi risus sit amet nisl. In hac habitasse platea dictumst. \n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum vulputate leo sed leo dapibus, at congue turpis feugiat. Pellentesque lobortis lorem id ipsum maximus, efficitur blandit odio consequat. Sed ut rhoncus justo."
            }
          ]
        },
        {
          "type": "page",
          "contentId": "e2f3a4b5-c6d7-8901-e2f3-a4b5c6d78901",
          "meta": {
            "title": "Summary"
          },
          "segments": [
            {
              "type": "text",
              "content": "**Key takeaways from this lesson:**\n\n1. **Intelligence is power** - Humans dominate Earth because of general intelligence\n2. **AI is different** - Digital minds can run faster, scale, and be copied\n3. **The alignment problem** - Ensuring AI goals match human values is extremely difficult\n4. **Expert concern** - Many leading researchers believe AI poses existential risk\n5. **Timelines are uncertain** - AGI could arrive within years or decades\n\nIn the next lesson, we'll explore common objections to AI safety concerns\nand how researchers respond to them."
            }
          ]
        }
      ]
    }
  ],
  "courses": [],
  "errors": []
}