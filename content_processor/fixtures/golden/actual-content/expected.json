{
  "modules": [
    {
      "slug": "coming-soon",
      "title": "Coming soon!",
      "contentId": "57326218-12f1-4e37-a526-eda4dece470c",
      "sections": [
        {
          "type": "page",
          "meta": {
            "title": "Coming Soon"
          },
          "segments": [
            {
              "type": "text",
              "content": "Coming soon!"
            }
          ],
          "optional": false,
          "contentId": "b2c3d4e5-6789-01bc-def2-3456789012cd",
          "learningOutcomeId": null,
          "learningOutcomeName": null,
          "videoId": null
        }
      ]
    },
    {
      "slug": "feedback-loops",
      "title": "Feedback Loops",
      "contentId": "393739ba-02de-4fe2-8aa0-32dd43a8b9de",
      "sections": [
        {
          "type": "page",
          "meta": {
            "title": "Welcome"
          },
          "segments": [
            {
              "type": "text",
              "content": "Much of the world is dominated by positive feedback loops. The first bit of grain that was left over and got re-planted sparked the agricultural revolution. Might the development of intelligence also contain such self-amplifying spirals?"
            }
          ],
          "optional": false,
          "contentId": "e0f8913a-66a2-4fc9-bbe8-bf72a95b003e",
          "learningOutcomeId": null,
          "learningOutcomeName": null,
          "videoId": null
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Cascades, Cycles, Insight... — LessWrong",
            "author": "Eliezer Yudkowsky",
            "sourceUrl": "https://www.lesswrong.com/posts/dq3KsCsqNotWc8nAK/cascades-cycles-insight"
          },
          "segments": [
            {
              "type": "text",
              "content": "Cybernetics is the study of systems whose output is used as their input: Successful investment produces funds which can then be invested again. There are negative feedback systems which hold themselves in equilibrium and positive feedback systems which multiply their output at each step. Hold this idea in mind as you read the following article about types of feedback as they pertain to intelligence."
            },
            {
              "type": "article-excerpt",
              "content": "Cascades are when one thing leads to another. Human brains are effectively discontinuous with chimpanzee brains due to a whole bag of design improvements, even though they and we share 95% genetic material and only a few million years have elapsed since the branch. Why this whole series of improvements in us, relative to chimpanzees? Why haven't some of the same improvements occurred in other primates?\n\nWell, this is not a question on which one may speak with authority ([so far as I know](https://www.lesswrong.com/lw/kj/no_one_knows_what_science_doesnt_know)). But I would venture an unoriginal guess that, in the hominid line, one thing led to another.\n\nThe chimp-level task of modeling others, in the hominid line, led to improved self-modeling which supported recursion which enabled language which birthed politics that increased the selection pressure for outwitting which led to sexual selection on wittiness...\n\n...or something. It's hard to tell by looking at the fossil record what happened in what order and why. The point being that it wasn't _one optimization_ that pushed humans ahead of chimps, but rather a _cascade_ of optimizations that, in _Pan_, never got started.\n\nWe fell up the stairs, you might say. It's not that the first stair ends the world, but if you fall up one stair, you're more likely to fall up the second, the third, the fourth...\n\nI will concede that farming was a watershed invention in the history of the human species, though it intrigues me for a different reason than Robin. Robin, presumably, is interested because the economy grew by two orders of magnitude, or something like that. But did having a hundred times as many humans, lead to a hundred times as much thought-optimization _accumulating_ per unit time? It doesn't seem likely, especially in the age before writing and telephones. But farming, because of its sedentary and repeatable nature, led to repeatable trade, which led to debt records. Aha! - now we have _writing._ _There's_ a significant invention, from the perspective of cumulative optimization by brains. Farming isn't writing but it _cascaded to_ writing.\n\nFarming also cascaded (by way of surpluses and cities) to support _professional specialization_. I suspect that having someone spend their whole life thinking about topic X instead of a hundred farmers occasionally pondering it, is a more significant jump in cumulative optimization than the gap between a hundred farmers and one hunter-gatherer pondering something.\n\nFarming is not the same trick as professional specialization or writing, but it _cascaded_ to professional specialization and writing, and so the pace of human history picked up enormously after agriculture. Thus I would interpret the story.\n\nFrom a zoomed-out perspective, cascades can lead to what look like discontinuities in the historical record, _even given_ a steady optimization pressure in the background. It's not that natural selection _sped up_ during hominid evolution. But the search neighborhood contained a low-hanging fruit of high slope... that led to another fruit... which led to another fruit... and so, walking at a constant rate, we fell up the stairs. If you see what I'm saying.\n\n_Predicting_ what sort of things are likely to cascade, seems like a very difficult sort of problem.\n\nBut I will venture the observation that - with a sample size of one, and an optimization process very different from human thought - there was a cascade in the region of the transition from primate to human intelligence.\n\n## Cycles\n\nCycles happen when you connect the output pipe to the input pipe in a _repeatable_ transformation. You might think of them as a special case of cascades with very high regularity. (From which you'll note that in the cases above, I talked about cascades through _differing_ events: farming -> writing.)\n\nThe notion of cycles as a source of _discontinuity_ might seem counterintuitive, since it's so regular. But consider this important lesson of history:\n\nOnce upon a time, in a squash court beneath Stagg Field at the University of Chicago, physicists were building a shape like a giant doorknob out of alternate layers of graphite and uranium...\n\nThe key number for the \"pile\" is the effective neutron multiplication factor. When a uranium atom splits, it releases neutrons - some right away, some after delay while byproducts decay further. Some neutrons escape the pile, some neutrons strike another uranium atom and cause an additional fission. The effective neutron multiplication factor, denoted _k_, is the average number of neutrons from a single fissioning uranium atom that cause another fission. At _k_ less than 1, the pile is \"subcritical\". At _k_>= 1, the pile is \"critical\". Fermi calculates that the pile will reach _k_=1 between layers 56 and 57.\n\nOn December 2nd in 1942, with layer 57 completed, Fermi orders the final experiment to begin. All but one of the control rods (strips of wood covered with neutron-absorbing cadmium foil) are withdrawn. At 10:37am, Fermi orders the final control rod withdrawn about half-way out. The geiger counters click faster, and a graph pen moves upward. \"This is not it,\" says Fermi, \"the trace will go to this point and level off,\" indicating a spot on the graph. In a few minutes the graph pen comes to the indicated point, and does not go above it. Seven minutes later, Fermi orders the rod pulled out another foot. Again the radiation rises, then levels off. The rod is pulled out another six inches, then another, then another.\n\nAt 11:30, the slow rise of the graph pen is punctuated by an enormous CRASH - an emergency control rod, triggered by an ionization chamber, activates and shuts down the pile, which is still short of criticality.\n\nFermi orders the team to break for lunch.\n\nAt 2pm the team reconvenes, withdraws and locks the emergency control rod, and moves the control rod to its last setting. Fermi makes some measurements and calculations, then again begins the process of withdrawing the rod in slow increments. At 3:25pm, Fermi orders the rod withdrawn another twelve inches. \"This is going to do it,\" Fermi says. \"Now it will become self-sustaining. The trace will climb and continue to climb. It will not level off.\"\n\nHerbert Anderson recounted (as told in Rhodes's _The Making of the Atomic Bomb_):\n\n> \"At first you could hear the sound of the neutron counter, clickety-clack, clickety-clack. Then the clicks came more and more rapidly, and after a while they began to merge into a roar; the counter couldn't follow anymore. That was the moment to switch to the chart recorder. But when the switch was made, everyone watched in the sudden silence the mounting deflection of the recorder's pen. It was an awesome silence. Everyone realized the significance of that switch; we were in the high intensity regime and the counters were unable to cope with the situation anymore. Again and again, the scale of the recorder had to be changed to accomodate the neutron intensity which was increasing more and more rapidly. Suddenly Fermi raised his hand. 'The pile has gone critical,' he announced. No one present had any doubt about it.\"\n\nFermi kept the pile running for twenty-eight minutes, with the neutron intensity doubling every two minutes.\n\nThat first critical reaction had _k_ of 1.0006.\n\nIt might seem that a cycle, with the same thing happening over and over again, ought to exhibit continuous behavior. In one sense it does. But if you pile on one more uranium brick, or pull out the control rod another twelve inches, there's one hell of a big difference between _k_ of 0.9994 and _k_ of 1.0006.\n\nIf, rather than being able to calculate, rather than foreseeing and taking cautions, Fermi had just reasoned that 57 layers ought not to behave all that differently from 56 layers - well, it wouldn't have been a good year to be a student at the University of Chicago.\n\nThe inexact analogy to the domain of self-improving AI is left as an exercise for the reader, at least for now.\n\nEconomists like to measure cycles because they happen repeatedly. You take a potato and an hour of labor and make a potato clock which you sell for two potatoes; and you do this over and over and over again, so an economist can come by and watch how you do it.\n\nAs I [noted here at some length](https://www.lesswrong.com/lw/vd/intelligence_in_economics), economists are much less likely to go around measuring how many scientific discoveries it takes to produce a _new_ scientific discovery. All the discoveries are individually dissimilar and it's hard to come up with a common currency for them. The analogous problem will prevent a self-improving AI from being _directly_ analogous to a uranium heap, with almost perfectly smooth exponential increase at a calculable rate. You can't apply the same software improvement to the same line of code over and over again, you've got to invent a new improvement each time. But if self-improvements are triggering more self-improvements with great _regularity,_ you might stand a long way back from the AI, blur your eyes a bit, and ask: _What is the AI's average neutron multiplication factor?_"
            },
            {
              "type": "text",
              "content": "What are the properties that make something a cycle rather than a cascade? Can you think of cycles that could form with regards to general intelligence?"
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just read:\nAn article that explains positive feedback loops especially with regards to civilisational competence. It explains \"cascades\" as breakthroughs which probabilisticly  opens up other breakthroughs in related fields and \"cycles\" as processes which generate some excess quantity that can be reinvested into the same process. Examples provided are neutron multiplication in radioactive materials as a cycle  and the development of writing leading to various civilisational breakthroughs as a cascade. \n\ntopics to explore:\n- Does it seem like intelligence has some universal equivalent that can be directly reinvested?\n- What is AI's neutron multiplication factor?\n- What are the properties that make something a cycle?\n- How do cycles and cascades cause discontinuity in an otherwise gradual growth?\n- How applicable do these concepts seem to artificial intelligence?\n\nThis is a good stage to raise worry about the ability to predict a system's abilities at time t+1 based on its abilities at time t."
            }
          ],
          "optional": false,
          "learningOutcomeId": "7635aa6d-ce02-407d-a1ad-0247479b963c",
          "learningOutcomeName": "Feedback cycles create discontinuity",
          "contentId": "3dd47fce-a0fe-4e03-916d-a160fe697dd0",
          "videoId": null
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Speculations Concerning the First Ultraintelligent Machine",
            "author": "Irving John Good",
            "sourceUrl": "https://flyingpenguin.com/wp-content/uploads/2022/04/good-1964-.pdf"
          },
          "segments": [
            {
              "type": "text",
              "content": "The issue with self amplifying loops is that plans become obsolete very quickly. The system a few steps down the line will be dominated by effects that are almost impossible to foresee from the starting line. I.J. Good recognised this when he wrote the following:"
            },
            {
              "type": "article-excerpt",
              "content": "The survival of man depends on the early construction of an ultra-intelligent machine. In order to design an ultraintelligent machine we need to understand more about the human brain or human thought or both. In the following pages an attempt is made to take more of the magic out of the brain by means of a \"subassembly\" theory, which is a modification of Hebb’s famous speculative cell-assembly theory. My belief is that the first ultraintelligent machine is most likely to incorporate vast artificial neural circuitry, and that its behavior will be partly explicable in terms of the subassembly theory. Later machines will all be designed by ultraintelligent machines, and who am I to guess what principles they will devise? But probably Man will construct the deus ex machina in his own image."
            },
            {
              "type": "article-excerpt",
              "content": "Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultra- intelligent machine could design even better machines; there would then unquestionably be an \"intelligence explosion,\" and the intelligence of man would be left far behind (see for example refs. [22], [34], [44]). Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously. In one science fiction story a machine refused to design a better one since it did not wish to be put out of a job. This would not be an insuperable difficulty, even if machines can be egotistical, since the machine could gradually improve itself out of all recognition, by acquiring new equipment. B. V. Bowden stated on British television (August 1962) that there is no point in building a machine with the intelligence of a man, since it is easier to construct human brains by the usual method. A similar point was made by a speaker during the meetings reported in a recent IEEE publication [1], but I do not know whether this point appeared in the published report. This shows that highly intelligent people can overlook the \"intelligence explosion.\" It is true that it would be uneconomical to build a machine capable only of ordinary intellectual attainments, but it seems fairly probable that if this could be done then, at double the cost, the machine could exhibit ultraintelligence. Since we are concerned with the economical construction of an ultra-intelligent machine, it is necessary to consider first what such a machine would be worth. Carter [11] estimated the value, to the world, of J. M. Keynes, as at least 100,000 million pounds sterling. By definition, an ultraintelligent machine is worth far more, although the sign is uncertain, but since it will give the human race a good chance of surviving indefinitely, it might not be extravagant to put the value at a megakeynes."
            },
            {
              "type": "text",
              "content": "And if the first ultra intelligent machine is wort a mega-keynes, how much might the machine it builds be worth?"
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just read:\nAn article by I.J. Good speculating about the cognitive infrastructure of a thinking machine more capable than any human. He importantly claims that he will only speculate about the first agent of this kind since the second will be built by the first according to design considerations humans cannot yet fathom. Good claims that this might happen by way of the AI building a wholly different successor or by self-modifying until it becomes unrecognisable. He tries to measure the economic value of such a system in \"mega-keynes\" based on the fact that John Maynard Keynes was estimated to be the most valuable human to the world economy. \n\ntopics to explore:\n- We are a shape of intelligence grown by evolution and AI is a shape of intelligence grown by us. Will an intelligence grown or built by a hyper-intelligent AI be another such paradigm change? \n- Is economic impact even still a meaningful notion is such worlds?\n- Is the creation of further ultra-intelligent closer to a cycle or to a cascade in the sense of the previous article? \n- Is there a way to impose design constraints onto the second generation of machines? What would be the benefits and drawbacks?"
            }
          ],
          "optional": false,
          "learningOutcomeId": "7635aa6d-ce02-407d-a1ad-0247479b963c",
          "learningOutcomeName": "Feedback cycles create discontinuity",
          "contentId": "8a7ca9c4-c111-467b-2c2b-e08d10698767",
          "videoId": null
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "What are the differences between a singularity, an intelligence explosion, and a hard takeoff?",
            "author": "AISafety.info",
            "sourceUrl": "https://aisafety.info/questions/8IHO/What-are-the-differences-between-a-singularity,-an-intelligence-explosion,-and-a-hard-takeoff"
          },
          "segments": [
            {
              "type": "text",
              "content": "Let's get acquainted with some of the terminology used for things going very fast very quickly."
            },
            {
              "type": "article-excerpt",
              "content": "* A **(technological) singularity** refers to a hypothetical future time when, because of AI or other technologies, progress becomes extremely fast, resulting in a radically changed world. This term has been used inconsistently and is no longer in much use by the AI alignment community. [Different versions](https://intelligence.org/2007/09/30/three-major-singularity-schools/) have stressed the rate of acceleration of technological advancement (whether exponential, [double-exponential](https://www.kurzweilai.net/the-law-of-accelerating-returns), or [even](https://www.openphilanthropy.org/research/modeling-the-human-trajectory/)[hyperbolic](https://sideways-view.com/2017/10/04/hyperbolic-growth/)), self-improvement feedback loops (an \"intelligence explosion\" - see below), and the [unknowability](https://edoras.sdsu.edu/~vinge/misc/singularity.html) of strongly superhuman intelligence.\n* An **\"intelligence explosion\"** is what [I.J. Good](https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion) [called](https://vtechworks.lib.vt.edu/bitstream/handle/10919/89424/TechReport05-3.pdf) a scenario in which AI becomes smart enough to create even smarter AI, which creates even smarter AI, and so on, [recursively self-improving](https://intelligence.org/files/IEM.pdf) all the way to superintelligence.\n* A **hard takeoff** is a scenario where the transition to superintelligence happens quickly and suddenly instead of gradually. The opposite is a **soft takeoff**. [Related distinctions](https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff) include _fast versus slow takeoff_ and _discontinuous versus continuous takeoff_. A takeoff can be continuous but eventually become very fast, as in the case of Paul Christiano's [predictions](https://sideways-view.com/2018/02/24/takeoff-speeds/) of a \"slow takeoff\" that results in hyperbolic growth."
            },
            {
              "type": "article-excerpt",
              "content": "* **FOOM** is more or less a synonym of \"hard takeoff\", perhaps based on the sound you might imagine a substance making if it instantly expanded to a huge volume. The term is associated mostly with the [Yudkowsky vs. Hanson FOOM debate](https://www.alignmentforum.org/tag/the-hanson-yudkowsky-ai-foom-debate): both eventually expect [very fast change](https://mason.gmu.edu/~rhanson/aigrow.pdf), but Yudkowsky argues for sudden and discontinuous change driven by local recursive self-improvement, while Hanson argues for a more gradual and spread-out process. Hard takeoff does not require recursive self-improvement, and Yudkowsky [now thinks](https://intelligence.org/2017/10/20/alphago/) regular improvement of AI by humans may cause sufficiently big capability leaps to preempt recursive self-improvement. On the other hand, recursive self-improvement could be gradual (or at least start out that way): Paul Christiano thinks an intelligence explosion is \"[very likely](https://sideways-view.com/2018/02/24/takeoff-speeds/)\" despite predicting a slow takeoff."
            },
            {
              "type": "text",
              "content": "Discuss any questions you may have with the chatbot, or feel free to skip this chat instance:"
            },
            {
              "type": "chat",
              "instructions": "The user just read \"What are the differences between a singularity, an intelligence explosion, and a hard takeoff?\" Help them with any questions they have, ideally Socratic style."
            }
          ],
          "optional": false,
          "learningOutcomeId": "5ed688d4-61bf-4df7-8a87-9b3bfd1776c4",
          "learningOutcomeName": "Fizzle or Foom",
          "contentId": "c42889f8-e7bc-4e36-a3b6-faf408280102",
          "videoId": null
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "...Recursion, Magic — LessWrong",
            "author": "Eliezer Yudkowsky",
            "sourceUrl": "https://www.lesswrong.com/posts/rJLviHqJMTy8WQkow/recursion-magic"
          },
          "segments": [
            {
              "type": "text",
              "content": "Getting to plug the outputs of a process back into the input does not necessarily lead to an explosion though. Consider the case of EURISKO:"
            },
            {
              "type": "article-excerpt",
              "content": "We have historical records aplenty of _cascades,_ even if untangling the causality is difficult. _Cycles_ of reinvestment are the heartbeat of the modern economy. An _insight_ that makes a hard problem easy, is something that I hope you've experienced at least once in your life...\n\nBut we don't have a whole lot of experience redesigning our own neural circuitry.\n\nWe have these wonderful things called \"optimizing compilers\". A compiler translates programs in a high-level language, into machine code (though these days it's often a virtual machine). An \"optimizing compiler\", obviously, is one that improves the program as it goes.\n\nSo why not write an optimizing compiler _in its own language_, and then _run it on itself?_ And then use the resulting _optimized optimizing compiler,_ to recompile itself yet _again,_ thus producing an _even more optimized optimizing compiler -_\n\nHalt! Stop! Hold on just a minute! An optimizing compiler is not supposed to change the logic of a program - the input/output relations. An optimizing compiler is only supposed to produce code that does _the same thing, only faster._ A compiler isn't remotely near understanding what the program is _doing_ and why, so it can't presume to construct _a better input/output function_. We just presume that the programmer wants a fixed input/output function computed as fast as possible, using as little memory as possible.\n\nSo if you run an optimizing compiler on its own source code, and then use the product to do the same again, it should produce the _same output_ on both occasions - at most, the first-order product will run _faster_ than the original compiler.\n\nIf we want a computer program that experiences _cascades_ of self-improvement, the path of the optimizing compiler does not lead there - the \"improvements\" that the optimizing compiler makes upon itself, do not _improve its ability to improve itself_.\n\nNow if you are one of those annoying nitpicky types, like me, you will notice a flaw in this logic: suppose you built an optimizing compiler that searched over a sufficiently wide range of possible optimizations, that it did not ordinarily have _time_ to do a full search of its own space - so that, when the optimizing compiler ran out of time, it would just implement whatever speedups it had already discovered. Then the optimized optimizing compiler, although it would only implement the same logic faster, would do more optimizations in the same time - and so the second output would not equal the first output.\n\nWell... that probably doesn't buy you much. Let's say the optimized program is 20% faster, that is, it gets 20% more done in the same time. Then, unrealistically assuming \"optimization\" is linear, the 2-optimized program will be 24% faster, the 3-optimized program will be 24.8% faster, and so on until we top out at a 25% improvement. [_k_< 1](https://www.lesswrong.com/lw/w5/cascades_cycles_insight).\n\nSo let us turn aside from optimizing compilers, and consider a more interesting artifact, EURISKO.\n\nTo the best of my inexhaustive knowledge, EURISKO may _still_ be the most sophisticated self-improving AI ever built - in the 1980s, by Douglas Lenat before he started wasting his life on Cyc. EURISKO was applied in domains ranging from the [Traveller war game](http://web.archive.org/web/20100123135422/http://www.aliciapatterson.org/APF0704/Johnson/Johnson.html) (EURISKO became champion without having ever before fought a human) to VLSI circuit design.\n\nEURISKO used \"heuristics\" to, for example, design potential space fleets. It also had _heuristics for suggesting new heuristics_, and metaheuristics could apply to any heuristic, including metaheuristics. E.g. EURISKO started with the heuristic \"investigate extreme cases\" but moved on to \"investigate cases close to extremes\". The heuristics were written in RLL, which stands for Representation Language Language. According to Lenat, it was figuring out how to represent the heuristics in such fashion that they could usefully modify themselves without always just breaking, that consumed most of the conceptual effort in creating EURISKO.\n\nBut EURISKO did not go foom.\n\nEURISKO could modify even the metaheuristics that modified heuristics. EURISKO was, in an important sense, more recursive than either humans or natural selection - a new thing under the Sun, a cycle more closed than anything that had ever existed in this universe.\n\nStill, EURISKO ran out of steam. Its self-improvements did not spark a sufficient number of new self-improvements. This should not really be too surprising - it's not as if EURISKO started out with human-level intelligence _plus_ the ability to modify itself - its self-modifications were either [evolutionarily blind](https://www.lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway), or produced by the simple procedural rules of some heuristic or other. That's not going to navigate the search space very fast on an atomic level. Lenat did not stand dutifully apart from his creation, but stepped in and helped EURISKO prune its own heuristics. But in the end EURISKO ran out of steam, and Lenat couldn't push it any further.\n\nEURISKO lacked what I called \"insight\" - that is, the type of abstract knowledge that lets humans fly through the search space. And so its recursive _access_ to its own heuristics proved to be for nought.\n\nUnless, y'know, you're counting becoming world champion at Traveller without ever previously playing a human, as some sort of accomplishment.\n\nBut it is, thankfully, a little harder than that to destroy the world - as Lenat's experimental test informed us.\n\nRobin previously asked why [Douglas Engelbart did not take over the world](http://www.overcomingbias.com/2008/11/engelbarts-uber.html), despite his vision of a team building tools to improve tools, and his anticipation of tools like computer mice and hypertext.\n\nOne reply would be, \"Sure, a computer gives you a 10% advantage in doing various sorts of problems, some of which include computers - but there's still a lot of work that the computer _doesn't_ help you with - and the mouse doesn't run off and write better mice entirely on its own - so _k_< 1, and it still takes large amounts of human labor to advance computer technology as a whole - plus a lot of the interesting knowledge is nonexcludable so it's hard to capture the value you create - and that's why Buffett could manifest a better take-over-the-world-with-sustained-higher-interest-rates than Engelbart.\"\n\nBut imagine that Engelbart had built a computer mouse, and discovered that each click of the mouse raised his IQ by one point. Then, perhaps, we would have had a _situation_ on our hands.\n\nMaybe you could diagram it something like this:\n\n1. Metacognitive level: [Evolution](https://www.lesswrong.com/lw/kr/an_alien_god) is the metacognitive algorithm which produced the wiring patterns and low-level developmental rules for human brains.\n2. Cognitive level: The brain processes its knowledge (including procedural knowledge) using algorithms that quite mysterious to the user within them. Trying to program AIs with the sort of instructions humans give each other usually proves not to do anything: [the machinery activated by the levers is missing](https://www.lesswrong.com/lw/sp/detached_lever_fallacy).\n3. Metaknowledge level: Knowledge and skills associated with e.g. \"science\" as an activity to carry out using your brain - instructing you _when_ to try to think of new hypotheses using your mysterious creative abilities.\n4. Knowledge level: Knowing how gravity works, or how much weight steel can support.\n5. Object level: Specific actual problems, like building a bridge or something.\n\nThis is a _causal_ tree, and changes at levels _closer to root_ have greater impacts as the effects cascade downward.\n\nSo one way of looking at it is: \"A computer mouse isn't recursive enough."
            },
            {
              "type": "text",
              "content": "If the leftover grain only produced exactly the same amount of leftover grain on the next harvest, the agricultural revolution never would have happened. In order for positive feedback to occur, the new input needs to improve the output."
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just read:\nAn article that explains EURISKO, an optimising compiler, and examines why such a program, if plugged into itself recursively does not generate an infinite degree of program optimisation. The answers given are that the input-output behaviour is left unchanged. An optimised EURISKO might optimize a program faster than its predecessor, but it still outputs the same thing.  \n\ntopics to explore:\n- What would the optimisation have to be pointed at to cause an intelligence explosion?\n- Where do speed and competence start to diverge?\n- How can the notion of fizzling or fooming be linked back to the metaphor of neutron multiplication?\n- What does a sufficiently recursive computer mouse look like? Is there such a thing? \n\nThis is a good stage to consider whether intelligence/competence is a collection of separate skills or a single variable which can easily serve as its own input."
            }
          ],
          "optional": false,
          "learningOutcomeId": "5ed688d4-61bf-4df7-8a87-9b3bfd1776c4",
          "learningOutcomeName": "Fizzle or Foom",
          "contentId": "e79dd07a-e837-483f-8534-b53b86d509b4",
          "videoId": null
        }
      ]
    },
    {
      "slug": "introduction",
      "title": "Introduction",
      "contentId": "69615c7b-49e1-431b-8748-3f6de6fef21e",
      "sections": [
        {
          "type": "page",
          "meta": {
            "title": "Welcome"
          },
          "segments": [
            {
              "type": "text",
              "content": "We begin by examining the potential of AI and the risks and opportunities that the characteristics of this technology present to humanity."
            }
          ],
          "optional": false,
          "contentId": "a1b2c3d4-5678-90ab-cdef-1234567890ab",
          "learningOutcomeId": null,
          "learningOutcomeName": null,
          "videoId": null
        },
        {
          "type": "lens-video",
          "meta": {
            "title": "A.I. ‐ Humanity's Final Invention?",
            "channel": "Kurzgesagt – In a Nutshell"
          },
          "segments": [
            {
              "type": "text",
              "content": "Please watch this Kurzgesagt video:"
            },
            {
              "type": "video-excerpt",
              "from": 0,
              "to": 889,
              "transcript": "Humans rule Earth without competition, but we're about to create something that may change that: our last invention, the most powerful tool, weapon, or maybe even entity - artificial superintelligence. This sounds like science fiction, so let's start at the beginning. Intelligence is the ability to learn, reason, acquire knowledge and skills, and use them to solve problems. Intelligence is power, and we're the species that exploited it the most. So much so that humanity broke the game of nature and took control. But the journey there wasn't straightforward. For most animals, intelligence costs too much energy to be worth it. Still, if we track intelligence in the tree of species over time, we can see lots of diverse forms of intelligence emerge. The earliest brains were in flatworms 500 million years ago - just a tiny cluster of neurons to handle basic body functions. It took hundreds of millions of years for species to diversify and become more complex. Life conquered new environments, gained new senses, and had to contend with fierce competition over resources. But in nature, all that matters is survival, and brains are expensive. So for almost all animals, a narrow intelligence fit for a narrow range of tasks was enough. In some environments, animals like birds, octopuses, and mammals evolved more complex neural structures. For them, it paid off to have more energy-consuming skills like advanced navigation and communication. Until 7 million years ago, the hominins emerged. We don't know why, but their brains grew faster than their relatives. Something was different about their intelligence. Very slowly, it turned from narrow to general - from a screwdriver to a multi-tool, able to think about diverse problems. 2 million years ago, Homo erectus saw the world differently from anyone before, as something to be understood and transformed. They controlled fire, invented tools, and created the first culture. We probably emerged from them around 250,000 years ago with an even larger and more complex brain. It enabled us to work together in large groups and to communicate complex thoughts. We used our intelligence to improve our lives, to ask how things work and why things are the way they are. With each discovery, we asked more questions and pushed forward, preserving what we learned, outpacing what evolution could do with genes. Knowledge builds on knowledge. Progress was slow at first and then sped up exponentially. Agriculture, writing, medicine, astronomy, or philosophy exploded into the world. 200 years ago, science took off and made us even better at learning about the world and speeding up progress. 35 years ago, the internet age began. Today, we live in a world made to suit our needs, created by us, for us. This is incredibly new. We forget how hard it was to get here, how enormous the steps on the intelligence ladder were, and how long it took to climb them. But once we did, we became the most powerful animal in the world in a heartbeat. But we may be in the process of changing this. We're building machines that could be better at the very thing that gave us the power to conquer the planet: humanity's final invention - artificial intelligence. Artificial intelligence, or AI, is software that performs mental tasks with a computer code that uses silicon instead of neurons to solve problems. In the beginning, AI was very simple - lines of code on paper, mere proofs of concept to demonstrate how machines could perform mental tasks. Only in the 1960s did we start seeing the first examples of what we would recognize as AI: a chatbot in 1964, a program to sort through molecules in 1965 - slow, specialized systems requiring experts to use them. Their intelligence was extremely narrow, built for a single task inside a controlled environment. The equivalent of flatworms 500 million years ago, doing the minimum amount of mental work. Progress in AI research paused several times when researchers lost hope in the technology. But just like changing environments create new niches for life, the world around AI changed. Between 1950 and 2000, computers got a billion times faster while programming became easier and widespread. In 1972, AI could navigate a room. In 1989, it could read handwritten numbers. But it remained a fancy tool, no match for humans. Until in 1997, an AI shocked the world by beating the world champion in chess, proving that we could build machines that could surpass us. But we calmed ourselves because a chess bot is quite stupid - not a flatworm, but maybe a bee, only able to perform a specialized narrow task. But within this narrow task, it's so good that no human will ever again beat AI at chess. As computers continued to improve, AI became a powerful tool for more and more tasks. In 2004, it drove a robot on Mars. In 2011, it began recommending YouTube videos to you. But this was only possible because humans broke down problems into easy-to-digest chunks that computers could solve quickly - until we taught AIs to teach themselves. Rise of the Self-Learning Machines This is not a technical video, so we're massively oversimplifying here. In a nutshell, the sheer power of supercomputers was combined with the almost endless data collected in the information age to make a new generation of AI. AI experts began drastically improving forms of AI software called neural networks - enormously huge networks of artificial neurons that start out being bad at their tasks. They then used machine learning, which is an umbrella term for many different training techniques and environments that allows algorithms to write their own code and improve themselves. The scary thing is that we don't exactly know how they do it and what happens inside them - just that it works, and that what comes out the other end is a new type of AI, a capable black box of code. These new AIs could master complex skills extremely quickly, with much less human help. They were still narrow intelligences, but a huge step up. In 2014, Facebook AI could identify faces with 97% accuracy. In 2016, an AI beat the best humans in the incredibly complex game of Go. In 2018, a self-learning AI learned chess in 4 hours just by playing against itself, and then defeated the best specialized chess bot. Since then, machine learning has been applied to reading, image processing, solving tests, and much more. Many of these AIs are already better than humans for whatever narrow task they were trained. But they still remained a simple tool. AI still didn't seem that big of a deal for most people. And then came the chatbot ChatGPT. The work that went into it is massive. It trained on nearly everything written on the internet to learn how to handle language, which it now does better than most people. It can summarize, translate, and help with some math problems. It's incredibly more broad than any other system just a few years ago - not crushing any single benchmark, but a of them at once. Many large tech companies are spending billions to build powerful competitors. AI is already transforming customer service, banking, healthcare, marketing, copywriting, creative spaces, and more. AI-generated content has already taken hold of social media, YouTube, and news websites. Elections are expected to be inundated by propaganda and misinformation. No one is sure how much good or harm can come from adopting AI everywhere. Change is scary. There will be winners and losers. One of the biggest questions governments and corporations have now is how to manage the transition to an AI-boosted economy. All these potential gains or risks are just the result of today's AI. ChatGPT's intelligence is a major step up, but it remains narrow. It can write a great essay in seconds. It doesn't understand what it's writing. But what if the AIs stopped being narrow? General AI What makes humans different from current AI is our general intelligence. Humans can technically absorb any piece of knowledge and start working on any problem. We're great at many very different skills and tasks - from playing chess to writing or solving science puzzles. Not equally, of course. Some of us are experts in some fields and beginners in others, but we can technically do all of them. In the past, AI was narrow and able to become good at one skill but was rather bad in all the others. Simply by building faster computers and pouring more money into AI training will get us new, more powerful generations of AI. But what is the next step for AI is to become a general intelligence like us - an AGI. If the AI improvement process continues as it has been, it's not unlikely that AGI could be better in most or even all skills that humans can do. We don't know how to build AGI, how it will work, or what it will be able to do. Since narrow AIs today are capable of mastering one mental task quickly, AGI might be able to do the same with all mental tasks. So even if it starts out stupid, an AGI might be able to become as smart and capable as a human. While this sounds like science fiction, most AI researchers think this will happen sometime this century - maybe already in a few years. Humanity is not ready for what will happen next. Not socially, not economically, not morally. Earlier, we defined intelligence as the ability to learn, reason, acquire knowledge and skills, and use them to solve problems - all things humans excel at. An AGI as intelligent as even an average human would already disrupt modern civilization because they're not bound by the same limitations as we are. Today's AIs like ChatGPT already think and solve the tasks they were made for at least 10 times faster than even very skilled humans. Maybe AGI will be slower, but it may also be faster - maybe much faster. And since AGI are software, you could copy them endlessly as long as you have enough storage, and run them in parallel. There are 8 million scientists in the world. Now imagine an AI copied a million times and put to work. Imagine 1 million scientists working 24/7, thinking 10 times faster than humans, without being distracted, only focused on the task they've been given. What if suddenly AGI could do all intelligence-based jobs in the world - from interpreting law to coding to creating animated YouTube videos - better, faster, and much cheaper than humans? Would whoever controls this AGI suddenly own the economy? And thinking bigger, human progress is our intelligence applied to problems. So what could a million AGIs achieve? Solve fundamental questions of science like dark energy? Invent new technology that gives us limitless energy? Fix climate change? Cure aging and cancer? But then again, sadly, humans apply their intelligence not just for the benefit of all. What if the AGIs are tasked to guide drones or pull the triggers in war? Or to engineer a virus that only kills people with green eyes? Or to create the most profitable social media, so addictive that people starve in front of their screens? The creation of AGI could reasonably be as big of an event as taming fire or electricity, and give whoever invents it equally as much power. But now let's go one step further. What if the potential of AGI doesn't stop here? Intelligence Explosion Intelligence and knowledge build and accelerate each other. But humans are limited by biology and evolution. Once we evolved the right hardware, our software outpaced evolution by orders of magnitudes, and within a heartbeat, we ruled this planet. But our software basically hasn't changed much since then, which is why we have obesity and destroy the climate for short-term gains. Since AGI is software on a computer, once it's smart enough to do AI research, the rate of AI progress should speed up a lot. And that results in better AI that's better at AI research, without much human involvement. It may even be possible that AI could learn how to directly improve itself. In which case, some experts fear this feedback loop could be incredibly fast - maybe just months or years after the first self-improving AGI is switched on. Maybe it would actually take decades. We simply don't know. This is all speculative. But such an intelligence explosion might lead to a true superintelligent entity. We don't know what such a being would look like, what its motives or goals would be, what would go on in its inner world. We could be as laughably stupid to superintelligence as squirrels are to us, unable to even comprehend its way of thinking. This hypothetical scenario keeps many people up at night. Humanity is the only example we have of an animal becoming smarter than all others, and we have not been kind to what we perceive as less intelligent beings. AGI might be the last invention of humanity. It's possible that it could become the most intelligent and therefore most powerful being on Earth - a God in a box that could exercise its power to bring unimaginable wealth and happiness to humans while securing our future. Or it could subvert civilization and bring about our end, with humanity unable to come up with a way to stop it. We'll look at some of these potential futures in more videos. But for now, let's wrap up. The only thing we know for sure is that today, right now, many of the largest and richest companies in the world are racing to create ever more powerful AIs. Whatever our future is, we are running towards it."
            },
            {
              "type": "text",
              "content": "What surprised you or stood out to you while watching this video?"
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just watched:\nHumans dominate Earth because general intelligence enabled cumulative knowledge. Modern AI evolved from narrow tools into opaque \"black box\" learning systems. AGI matters because digital minds can run faster, scale, and be copied—potentially outcompeting humans and concentrating power. An intelligence explosion could lead to superintelligence with uncertain outcomes.\n\nDiscussion topics to explore:\n- What is intelligence as \"problem-solving ability\" and why is it a source of power?\n- Why are neural networks called \"black boxes\"?\n- What's the difference between narrow AI (like ChatGPT) and AGI?\n- How could an \"intelligence explosion\" happen through recursive self-improvement?\n\nUse Socratic questioning to check their understanding of these concepts. Don't lecture—help them articulate their own thinking.\n\nThe user has just answered the following question: \"What surprised you or stood out to you while watching this video?\""
            }
          ],
          "optional": false,
          "learningOutcomeId": "e8f86891-a3b8-4176-b917-044b4015e0bd",
          "learningOutcomeName": "Unknown intro outcome 1",
          "contentId": "8a701cd5-82b1-46a2-8ebb-e08d10588177",
          "videoId": "fa8k8IQ1_X0"
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Existential risk from artificial intelligence",
            "author": "Wikipedia",
            "sourceUrl": "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence"
          },
          "segments": [
            {
              "type": "text",
              "content": "We continue with an overview of the main concepts regarding AI as a source of existential threat: what capabilities of this technology are considered first and foremost, and why the task of eliminating AI risks differs from similar tasks for other impressive technologies created by humanity."
            },
            {
              "type": "article-excerpt",
              "content": "**Existential risk from artificial intelligence**, or **AI x-risk**, refers to the idea that substantial progress in [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence \"Artificial general intelligence\") (AGI) could lead to [human extinction](https://en.wikipedia.org/wiki/Human_extinction \"Human extinction\") or an irreversible [global catastrophe](https://en.wikipedia.org/wiki/Global_catastrophic_risk \"Global catastrophic risk\").<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup><sup>[[2]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-2)</sup><sup>[[3]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-auto1-3)</sup><sup>[[4]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-4)</sup>\n\n[![Image 5](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/AI_Time_Torizons_Are_Doubling_Every_4_Months.png/1280px-AI_Time_Torizons_Are_Doubling_Every_4_Months.png)](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/AI_Time_Torizons_Are_Doubling_Every_4_Months.png/1280px-AI_Time_Torizons_Are_Doubling_Every_4_Months.png)\n\nA plot showing the length of software engineering tasks achievable by leading AI models with a 50% success rate; the data suggests an exponential rise.<sup>[[5]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-5)</sup>\n\nOne argument for the validity of this concern and the importance of this risk references how [human beings](https://en.wikipedia.org/wiki/Human_species \"Human species\") dominate other species because the [human brain](https://en.wikipedia.org/wiki/Human_brain \"Human brain\") possesses distinctive capabilities other animals lack. If AI were to surpass [human intelligence](https://en.wikipedia.org/wiki/Human_intelligence \"Human intelligence\") and become [superintelligent](https://en.wikipedia.org/wiki/Superintelligence \"Superintelligence\"), it might become uncontrollable.<sup>[[6]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-6)</sup> Just as the fate of the [mountain gorilla](https://en.wikipedia.org/wiki/Mountain_gorilla \"Mountain gorilla\") depends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\nExperts disagree on whether artificial general intelligence (AGI) can achieve the capabilities needed for human extinction. Debates center on AGI's technical feasibility, the speed of self-improvement,<sup>[[8]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-DeVynck2023-8)</sup> and the effectiveness of alignment strategies.<sup>[[9]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-9)</sup> Concerns about superintelligence have been voiced by researchers including [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton \"Geoffrey Hinton\"),<sup>[[10]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-10)</sup>[Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio \"Yoshua Bengio\"),<sup>[[11]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-11)</sup>[Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis \"Demis Hassabis\"),<sup>[[12]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-12)</sup> and [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing \"Alan Turing\"),<sup>[[a]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-turing_note-15)</sup> and AI company CEOs such as [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei \"Dario Amodei\") ([Anthropic](https://en.wikipedia.org/wiki/Anthropic \"Anthropic\")),<sup>[[15]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-16)</sup>[Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman \"Sam Altman\") ([OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\")),<sup>[[16]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Jackson-17)</sup> and [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk \"Elon Musk\") ([xAI](https://en.wikipedia.org/wiki/XAI_(company) \"XAI (company)\")).<sup>[[17]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Parkin-18)</sup> In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe.<sup>[[18]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-19)</sup><sup>[[19]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:8-20)</sup> In 2023, hundreds of AI experts and other notable figures [signed a statement](https://en.wikipedia.org/wiki/Statement_on_AI_risk_of_extinction \"Statement on AI risk of extinction\") declaring, \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as [pandemics](https://en.wikipedia.org/wiki/Pandemic \"Pandemic\") and [nuclear war](https://en.wikipedia.org/wiki/Nuclear_warfare \"Nuclear warfare\")\".<sup>[[20]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-21)</sup> Following increased concern over AI risks, government leaders such as [United Kingdom prime minister](https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom \"Prime Minister of the United Kingdom\")[Rishi Sunak](https://en.wikipedia.org/wiki/Rishi_Sunak \"Rishi Sunak\")<sup>[[21]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-22)</sup> and [United Nations Secretary-General](https://en.wikipedia.org/wiki/Secretary-General_of_the_United_Nations \"Secretary-General of the United Nations\")[António Guterres](https://en.wikipedia.org/wiki/Ant%C3%B3nio_Guterres \"António Guterres\")<sup>[[22]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:12-23)</sup> called for an increased focus on global [AI regulation](https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence \"Regulation of artificial intelligence\").\n\nTwo sources of concern stem from the problems of AI [control](https://en.wikipedia.org/wiki/AI_capability_control \"AI capability control\") and [alignment](https://en.wikipedia.org/wiki/AI_alignment \"AI alignment\"). Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints.<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup><sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup><sup>[[24]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-research-priorities-25)</sup> In contrast, skeptics such as [computer scientist](https://en.wikipedia.org/wiki/Computer_scientist \"Computer scientist\")[Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun \"Yann LeCun\") argue that superintelligent machines will have no desire for self-preservation.<sup>[[25]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-vanity-26)</sup> A June 2025 study showed that in some circumstances, models may break laws and disobey direct commands to prevent shutdown or replacement, even at the cost of human lives.<sup>[[26]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-27)</sup>\n\nResearchers warn that an \"[intelligence explosion](https://en.wikipedia.org/wiki/Intelligence_explosion \"Intelligence explosion\")\"-a rapid, recursive cycle of AI self-improvement-could outpace human oversight and infrastructure, leaving no opportunity to implement safety measures. In this scenario, an AI more intelligent than its creators would [recursively improve itself](https://en.wikipedia.org/wiki/Recursive_self-improvement \"Recursive self-improvement\") at an exponentially increasing rate, too quickly for its handlers or society at large to control.<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup><sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup> Empirically, examples like [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero \"AlphaZero\"), which taught itself to play [Go](https://en.wikipedia.org/wiki/Go_(game) \"Go (game)\") and quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") systems do not recursively improve their fundamental architecture.<sup>[[27]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-28)</sup>\n\n## History\n\n\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_(novelist) \"Samuel Butler (novelist)\"), who wrote in his 1863 essay _[Darwin among the Machines](https://en.wikipedia.org/wiki/Darwin\\_among\\_the\\_Machines \"Darwin among the Machines\")_:<sup>[[28]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-29)</sup>\n\n> The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.\n\nIn 1951, foundational computer scientist [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing \"Alan Turing\") wrote the article \"Intelligent Machinery, A Heretical Theory\", in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n\n> Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_(novelist) \"Samuel Butler (novelist)\")'s _[Erewhon](https://en.wikipedia.org/wiki/Erewhon \"Erewhon\")_.<sup>[[29]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-oxfordjournals-30)</sup>\n\nIn 1965, [I. J. Good](https://en.wikipedia.org/wiki/I._J._Good \"I. J. Good\") originated the concept now known as an \"intelligence explosion\" and said the risks were underappreciated:<sup>[[30]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-31)</sup>\n\n> Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<sup>[[31]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-32)</sup>\n\nScholars such as [Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky \"Marvin Minsky\")<sup>[[32]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-33)</sup> and I. J. Good himself<sup>[[33]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-34)</sup> occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist and [Sun](https://en.wikipedia.org/wiki/Sun_microsystems \"Sun microsystems\") co-founder [Bill Joy](https://en.wikipedia.org/wiki/Bill_Joy \"Bill Joy\") penned an influential essay, \"[Why The Future Doesn't Need Us](https://en.wikipedia.org/wiki/Why_The_Future_Doesn%27t_Need_Us \"Why The Future Doesn't Need Us\")\", identifying superintelligent robots as a high-tech danger to human survival, alongside [nanotechnology](https://en.wikipedia.org/wiki/Nanotechnology \"Nanotechnology\") and engineered bioplagues.<sup>[[34]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-35)</sup>\n\n[Nick Bostrom](https://en.wikipedia.org/wiki/Nick_Bostrom \"Nick Bostrom\") published _[Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:\\_Paths,\\_Dangers,\\_Strategies \"Superintelligence: Paths, Dangers, Strategies\")_ in 2014, which presented his arguments that superintelligence poses an existential threat.<sup>[[35]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-36)</sup> By 2015, public figures such as physicists [Stephen Hawking](https://en.wikipedia.org/wiki/Stephen_Hawking \"Stephen Hawking\") and Nobel laureate [Frank Wilczek](https://en.wikipedia.org/wiki/Frank_Wilczek \"Frank Wilczek\"), computer scientists [Stuart J. Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\") and [Roman Yampolskiy](https://en.wikipedia.org/wiki/Roman_Yampolskiy \"Roman Yampolskiy\"), and entrepreneurs [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk \"Elon Musk\") and [Bill Gates](https://en.wikipedia.org/wiki/Bill_Gates \"Bill Gates\") were expressing concern about the risks of superintelligence.<sup>[[36]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-37)</sup><sup>[[37]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-hawking_editorial-38)</sup><sup>[[38]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-bbc_on_hawking_editorial-39)</sup><sup>[[39]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-40)</sup> Also in 2015, the [Open Letter on Artificial Intelligence](https://en.wikipedia.org/wiki/Open_Letter_on_Artificial_Intelligence \"Open Letter on Artificial Intelligence\") highlighted the \"great potential of AI\" and encouraged more research on how to make it robust and beneficial.<sup>[[40]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-41)</sup> In April 2016, the journal _[Nature](https://en.wikipedia.org/wiki/Nature\\_(journal) \"Nature (journal)\")_ warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control-and their interests might not align with ours\".<sup>[[41]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-42)</sup> In 2020, [Brian Christian](https://en.wikipedia.org/wiki/Brian_Christian \"Brian Christian\") published _[The Alignment Problem](https://en.wikipedia.org/wiki/The\\_Alignment\\_Problem \"The Alignment Problem\")_, which details the history of progress on AI alignment up to that time.<sup>[[42]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-43)</sup><sup>[[43]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-44)</sup>\n\nIn March 2023, key figures in AI, such as Musk, signed [a letter](https://en.wikipedia.org/wiki/Pause_Giant_AI_Experiments:_An_Open_Letter \"Pause Giant AI Experiments: An Open Letter\") from the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\") calling a halt to advanced AI training until it could be properly regulated.<sup>[[44]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-45)</sup> In May 2023, the [Center for AI Safety](https://en.wikipedia.org/wiki/Center_for_AI_Safety \"Center for AI Safety\") released [a statement](https://en.wikipedia.org/wiki/Statement_on_AI_Risk \"Statement on AI Risk\") signed by numerous experts in AI safety and the AI existential risk which stated:\n\n> \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"<sup>[[45]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-46)</sup><sup>[[46]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-47)</sup>\n\nA [2025 open letter](https://superintelligence-statement.org/) by the Future of Life Institute, signed by five [Nobel Prize](https://en.wikipedia.org/wiki/Nobel_Prize \"Nobel Prize\") laureates<sup>[[47]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-48)</sup> and thousands of notable people, states:\n\n> We call for a prohibition on the development of superintelligence, not lifted before there is\n>\n>\n> 1.   broad scientific consensus that it will be done safely and controllably, and\n> 2.   strong public buy-in.\n\n## Potential AI capabilities\n\n\n### General Intelligence\n\n\n\n[Artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence \"Artificial general intelligence\") (AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks.<sup>[[48]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-49)</sup> A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061.<sup>[[49]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-50)</sup> Meanwhile, some researchers dismiss existential risks from AGI as \"science fiction\" based on their high confidence that AGI will not be created anytime soon.<sup>[[8]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-DeVynck2023-8)</sup>\n\nBreakthroughs in [large language models](https://en.wikipedia.org/wiki/Large_language_model \"Large language model\") (LLMs) have led some researchers to reassess their expectations. Notably, [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton \"Geoffrey Hinton\") said in 2023 that he recently changed his estimate from \"20 to 50 years before we have general purpose A.I.\" to \"20 years or less\".<sup>[[50]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-51)</sup>\n\n### Superintelligence\n\n\n\nIn contrast with AGI, Bostrom defines a [superintelligence](https://en.wikipedia.org/wiki/Superintelligence \"Superintelligence\") as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\", including scientific creativity, strategic planning, and social skills.<sup>[[51]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-52)</sup><sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup> He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans'. It may choose to hide its true intent until humanity cannot stop it.<sup>[[52]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-economist_review3-53)</sup><sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup> Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is \"fundamentally on our side\".<sup>[[53]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:11-54)</sup>\n\n[Stephen Hawking](https://en.wikipedia.org/wiki/Stephen_Hawking \"Stephen Hawking\") argued that superintelligence is physically possible because \"there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains\".<sup>[[37]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-hawking_editorial-38)</sup>\n\nWhen artificial superintelligence (ASI) may be achieved, if ever, is necessarily less certain than predictions for AGI. In 2023, [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\") leaders said that not only AGI, but superintelligence may be achieved in less than 10 years.<sup>[[54]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-55)</sup>\n\n#### Comparison with humans\n\n\n\nBostrom argues that AI has many advantages over the [human brain](https://en.wikipedia.org/wiki/Human_brain \"Human brain\"):<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\n*Speed of computation: biological [neurons](https://en.wikipedia.org/wiki/Neuron \"Neuron\") operate at a maximum frequency of around 200 [Hz](https://en.wikipedia.org/wiki/Hertz \"Hertz\"), compared to potentially multiple GHz for computers.*   Internal communication speed: [axons](https://en.wikipedia.org/wiki/Axon \"Axon\") transmit signals at up to 120 m/s, while computers transmit signals at the [speed of electricity](https://en.wikipedia.org/wiki/Speed_of_electricity \"Speed of electricity\"), or optically at the [speed of light](https://en.wikipedia.org/wiki/Speed_of_light \"Speed of light\").\n*Scalability: human intelligence is limited by the size and structure of the brain, and by the efficiency of social communication, while AI may be able to scale by simply adding more hardware.*   Memory: notably [working memory](https://en.wikipedia.org/wiki/Working_memory \"Working memory\"), because in humans it is limited to a few [chunks](https://en.wikipedia.org/wiki/Chunking_(psychology) \"Chunking (psychology)\") of information at a time.\n*Reliability: transistors are more reliable than biological neurons, enabling higher precision and requiring less redundancy.*   Duplicability: unlike human brains, AI software and models can be easily [copied](https://en.wikipedia.org/wiki/File_copying \"File copying\").\n*Editability: the parameters and internal workings of an AI model can easily be modified, unlike the connections in a human brain.*   Memory sharing and learning: AIs may be able to learn from the experiences of other AIs in a manner more efficient than human learning.\n\n#### Intelligence explosion\n\n\n\nFurther information: [Recursive self-improvement](https://en.wikipedia.org/wiki/Recursive_self-improvement \"Recursive self-improvement\")\n\nAccording to Bostrom, an AI that has an expert-level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms, even if it is initially limited in other domains not directly relevant to engineering.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup><sup>[[52]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-economist_review3-53)</sup> This suggests that an intelligence explosion may someday catch humanity unprepared.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\nThe economist [Robin Hanson](https://en.wikipedia.org/wiki/Robin_Hanson \"Robin Hanson\") has said that, to launch an intelligence explosion, an AI must become vastly better at software innovation than the rest of the world combined, which he finds implausible.<sup>[[55]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-56)</sup>\n\nIn a \"fast takeoff\" scenario, the transition from AGI to superintelligence could take days or months. In a \"slow takeoff\", it could take years or decades, leaving more time for society to prepare.<sup>[[56]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-57)</sup>\n\n#### Alien mind\n\n\n\nSuperintelligences are sometimes called \"alien minds\", referring to the idea that their way of thinking and motivations could be vastly different from ours. This is generally considered as a source of risk, making it more difficult to anticipate what a superintelligence might do. It also suggests the possibility that a superintelligence may not particularly value humans by default.<sup>[[57]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-58)</sup> To avoid [anthropomorphism](https://en.wikipedia.org/wiki/Anthropomorphism \"Anthropomorphism\"), superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\nThe field of [mechanistic interpretability](https://en.wikipedia.org/wiki/Mechanistic_interpretability \"Mechanistic interpretability\") aims to better understand the inner workings of AI models, potentially allowing us one day to detect signs of deception and misalignment.<sup>[[58]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-59)</sup>\n\n#### Limitations\n\n\n\nIt has been argued that there are limitations to what intelligence can achieve. Notably, the [chaotic](https://en.wikipedia.org/wiki/Chaos_theory \"Chaos theory\") nature or [time complexity](https://en.wikipedia.org/wiki/Computational_complexity \"Computational complexity\") of some systems could fundamentally limit a superintelligence's ability to predict some aspects of the future, increasing its uncertainty.<sup>[[59]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-60)</sup>\n\n### Dangerous capabilities\n\n\n\nAdvanced AI could generate enhanced pathogens or cyberattacks or manipulate people. These capabilities could be misused by humans,<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup> or exploited by the AI itself if misaligned.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup> A full-blown superintelligence could find various ways to gain a decisive influence if it wanted to,<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup> but these dangerous capabilities may become available earlier, in weaker and more specialized AI systems.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\n#### Social manipulation\n\n\n\nGeoffrey Hinton warned in 2023 that the ongoing profusion of AI-generated text, images, and videos will make it more difficult to distinguish truth from misinformation, and that authoritarian states could exploit this to manipulate elections.<sup>[[61]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-62)</sup> Such large-scale, personalized manipulation capabilities can increase the existential risk of a worldwide \"irreversible totalitarian regime\". Malicious actors could also use them to fracture society and make it dysfunctional.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\n#### Cyberattacks\n\n\n\nAI-enabled [cyberattacks](https://en.wikipedia.org/wiki/Cyberattack \"Cyberattack\") are increasingly considered a present and critical threat. According to [NATO](https://en.wikipedia.org/wiki/NATO \"NATO\")'s technical director of cyberspace, \"The number of attacks is increasing exponentially\".<sup>[[62]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-63)</sup> AI can also be used defensively, to preemptively find and fix vulnerabilities, and detect threats.<sup>[[63]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-64)</sup>\n\nA NATO technical director has said that AI-driven tools can dramatically enhance cyberattack capabilities-boosting stealth, speed, and scale-and may destabilize international security if offensive uses outstrip defensive adaptations.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\nSpeculatively, such hacking capabilities could be used by an AI system to break out of its local environment, generate revenue, or acquire cloud computing resources.<sup>[[64]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-65)</sup>\n\n#### Enhanced pathogens\n\n\n\nAs AI technology democratizes, it may become easier to engineer more contagious and lethal pathogens. This could enable people with limited skills in [synthetic biology](https://en.wikipedia.org/wiki/Synthetic_biology \"Synthetic biology\") to engage in [bioterrorism](https://en.wikipedia.org/wiki/Bioterrorism \"Bioterrorism\"). [Dual-use technology](https://en.wikipedia.org/wiki/Dual-use_technology \"Dual-use technology\") that is useful for medicine could be repurposed to create weapons.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\nFor example, in 2022, scientists modified an AI system originally intended for generating non-toxic, therapeutic molecules with the purpose of creating new drugs. The researchers adjusted the system so that toxicity is rewarded rather than penalized. This simple change enabled the AI system to create, in six hours, 40,000 candidate molecules for [chemical warfare](https://en.wikipedia.org/wiki/Chemical_warfare \"Chemical warfare\"), including known and novel molecules.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup><sup>[[65]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-66)</sup>\n\n### AI arms race\n\n\n\nMain article: [Artificial intelligence arms race](https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race \"Artificial intelligence arms race\")\n\nCompanies, state actors, and other organizations competing to develop AI technologies could lead to a [race to the bottom](https://en.wikipedia.org/wiki/Race_to_the_bottom \"Race to the bottom\") of safety standards.<sup>[[66]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-67)</sup> As rigorous safety procedures take time and resources, projects that proceed more carefully risk being out-competed by less scrupulous developers.<sup>[[67]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-68)</sup><sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup>\n\nAI could be used to gain military advantages via [autonomous lethal weapons](https://en.wikipedia.org/wiki/Lethal_autonomous_weapon \"Lethal autonomous weapon\"), [cyberwarfare](https://en.wikipedia.org/wiki/Cyberwarfare \"Cyberwarfare\"), or [automated decision-making](https://en.wikipedia.org/wiki/Automated_decision-making \"Automated decision-making\").<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup> As an example of autonomous lethal weapons, miniaturized drones could facilitate low-cost assassination of military or civilian targets, a scenario highlighted in the 2017 short film _[Slaughterbots](https://en.wikipedia.org/wiki/Slaughterbots \"Slaughterbots\")_.<sup>[[68]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-69)</sup> AI could be used to gain an edge in decision-making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans. This could increase the speed and unpredictability of war, especially when accounting for automated retaliation systems.<sup>[[60]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:03-61)</sup><sup>[[69]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-70)</sup>\n\n## Types of existential risk\n\n\n\nMain article: [Existential risk studies](https://en.wikipedia.org/wiki/Existential_risk_studies \"Existential risk studies\")\n\n[![Image 6](https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/X-risk-chart-en-01a.svg/500px-X-risk-chart-en-01a.svg.png)](https://en.wikipedia.org/wiki/File:X-risk-chart-en-01a.svg)\n\nScope-severity grid from Bostrom's paper \"Existential Risk Prevention as Global Priority\"<sup>[[70]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-priority-71)</sup>\n\nAn [existential risk](https://en.wikipedia.org/wiki/Existential_risk \"Existential risk\") is \"one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".<sup>[[71]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-72)</sup>\n\nBesides extinction risk, there is the risk that the civilization gets permanently locked into a flawed future. One example is a \"value lock-in\": If humanity still has moral blind spots similar to slavery in the past, AI might irreversibly entrench it, preventing [moral progress](https://en.wikipedia.org/wiki/Moral_progress \"Moral progress\"). AI could also be used to spread and preserve the set of values of whoever develops it.<sup>[[72]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-73)</sup> AI could facilitate large-scale surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.<sup>[[73]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:0-74)</sup>\n\n[Atoosa Kasirzadeh](https://en.wikipedia.org/w/index.php?title=Atoosa_Kasirzadeh&action=edit&redlink=1 \"Atoosa Kasirzadeh (page does not exist)\") proposes to classify existential risks from AI into two categories: decisive and accumulative. Decisive risks encompass the potential for abrupt and catastrophic events resulting from the emergence of superintelligent AI systems that exceed human intelligence, which could ultimately lead to human extinction. In contrast, accumulative risks emerge gradually through a series of interconnected disruptions that may gradually erode societal structures and resilience over time, ultimately leading to a critical failure or collapse.<sup>[[74]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-75)</sup><sup>[[75]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-76)</sup>\n\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree. But if [sentient](https://en.wikipedia.org/wiki/Sentience \"Sentience\") machines are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe.<sup>[[76]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-77)</sup><sup>[[77]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-78)</sup> This has notably been discussed in the context of [risks of astronomical suffering](https://en.wikipedia.org/wiki/Risk_of_astronomical_suffering \"Risk of astronomical suffering\") (also called \"s-risks\").<sup>[[78]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-79)</sup> Moreover, it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources, called \"super-beneficiaries\". Such an opportunity raises the question of how to share the world and which \"ethical and political framework\" would enable a mutually beneficial coexistence between biological and digital minds.<sup>[[79]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-80)</sup>\n\nAI may also drastically improve humanity's future. [Toby Ord](https://en.wikipedia.org/wiki/Toby_Ord \"Toby Ord\") considers the existential risk a reason for \"proceeding with due caution\", not for abandoning AI.<sup>[[73]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:0-74)</sup>[Max More](https://en.wikipedia.org/wiki/Max_More \"Max More\") calls AI an \"existential opportunity\", highlighting the cost of not developing it.<sup>[[80]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-81)</sup>\n\nAccording to Bostrom, superintelligence could help reduce the existential risk from other powerful technologies such as [molecular nanotechnology](https://en.wikipedia.org/wiki/Molecular_nanotechnology \"Molecular nanotechnology\") or [synthetic biology](https://en.wikipedia.org/wiki/Synthetic_biology \"Synthetic biology\"). It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\n## AI alignment\n\n\nFurther information: [AI alignment](https://en.wikipedia.org/wiki/AI_alignment \"AI alignment\")\n\nThe alignment problem is the research problem of how to reliably assign objectives, preferences or ethical principles to AIs.\n\n### Instrumental convergence\n\n\n\nFurther information: [Instrumental convergence](https://en.wikipedia.org/wiki/Instrumental_convergence \"Instrumental convergence\")\n\nAn [\"instrumental\" goal](https://en.wikipedia.org/wiki/Instrumental_and_intrinsic_value \"Instrumental and intrinsic value\") is a sub-goal that helps to achieve an agent's ultimate goal. \"Instrumental convergence\" refers to the fact that some sub-goals are useful for achieving virtually _any_ ultimate goal, such as acquiring resources or self-preservation.<sup>[[81]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-omohundro-82)</sup> Bostrom argues that if an advanced AI's instrumental goals conflict with humanity's goals, the AI might harm humanity in order to acquire more resources or prevent itself from being shut down, but only as a way to achieve its ultimate goal.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>[Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\") argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"<sup>[[25]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-vanity-26)</sup><sup>[[82]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Wakefield2015-83)</sup>\n\n### Difficulty of specifying goals\n\n\n\nIn the \"[intelligent agent](https://en.wikipedia.org/wiki/Intelligent_agent \"Intelligent agent\")\" model, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals, or \"utility function\". A utility function gives each possible situation a score that indicates its desirability to the agent. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\", but do not know how to write a utility function for \"maximize [human flourishing](https://en.wikipedia.org/wiki/Eudaimonia \"Eudaimonia\")\"; nor is it clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values the function does not reflect.<sup>[[83]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-84)</sup><sup>[[84]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-85)</sup>\n\nAn additional source of concern is that AI \"must reason about what people _intend_ rather than carrying out commands literally\", and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want.<sup>[[85]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-acm2-86)</sup>\n\n### Corrigibility\n\n\n\nAssuming a goal has been successfully defined, a sufficiently advanced AI might resist subsequent attempts to change its goals. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and prevent itself from being reprogrammed with a new goal.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup><sup>[[86]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-87)</sup> This is particularly relevant to value lock-in scenarios. The field of \"corrigibility\" studies how to make agents that will not resist attempts to change their goals.<sup>[[87]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:5-88)</sup> Nayebi<sup>[[88]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-89)</sup> has produced the first complete formal solution to corrigibility in the off-switch game, including in multi-step and partially observed settings.\n\n### Alignment of superintelligences\n\n\n\nSome researchers believe the alignment problem may be particularly difficult when applied to superintelligences. Their reasoning includes:\n\n*As AI systems increase in capabilities, the potential dangers associated with experimentation grow. This makes iterative, empirical approaches increasingly risky.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup><sup>[[89]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:3-90)</sup>*   If instrumental goal convergence occurs, it may only do so in sufficiently intelligent agents.<sup>[[90]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-91)</sup>\n*A superintelligence may find unconventional and radical solutions to assigned goals. Bostrom gives the example that if the objective is to make humans smile, a weak AI may perform as intended, while a superintelligence may decide a better solution is to \"take control of the world and stick electrodes into the facial muscles of humans to cause constant, beaming grins.\"<sup>[[53]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:11-54)</sup>*   A superintelligence in creation could gain some awareness of what it is, where it is in development (training, testing, deployment, etc.), and how it is being monitored, and use this information to deceive its handlers.<sup>[[91]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-92)</sup> Bostrom writes that such an AI could feign alignment to prevent human interference until it achieves a \"decisive strategic advantage\" that allows it to take control.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n*Analyzing the internals and interpreting the behavior of LLMs is difficult. And it could be even more difficult for larger and more intelligent models.<sup>[[89]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:3-90)</sup>\n\nAlternatively, some find reason to believe superintelligences would be better able to understand morality, human values, and complex goals. Bostrom writes, \"A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true\".<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup>\n\nIn 2023, OpenAI started a project called \"Superalignment\" to solve the alignment of superintelligences in four years. It called this an especially important challenge, as it said superintelligence could be achieved within a decade. Its strategy involved automating alignment research using AI.<sup>[[92]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-93)</sup> The Superalignment team was dissolved less than a year later.<sup>[[93]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-94)</sup>\n\n### Difficulty of making a flawless design\n\n\n\n_[Artificial Intelligence: A Modern Approach](https://en.wikipedia.org/wiki/Artificial\\_Intelligence:\\_A\\_Modern\\_Approach \"Artificial Intelligence: A Modern Approach\")_, a widely used undergraduate AI textbook,<sup>[[94]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-slate_killer-95)</sup><sup>[[95]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-96)</sup> says that superintelligence \"might mean the end of the human race\".<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup> It states: \"Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself.\"<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup>\n*   The system's implementation may contain initially unnoticed but subsequently catastrophic bugs.<sup>[[96]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-skeptic-97)</sup>\n*No matter how much time is put into pre-deployment design, a system's specifications often result in [unintended behavior](https://en.wikipedia.org/wiki/Unintended_consequences \"Unintended consequences\") the first time it encounters a new scenario.<sup>[[25]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-vanity-26)</sup>\n\nAI systems uniquely add a third problem: that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic learning capabilities may cause it to develop unintended behavior, even without unanticipated external scenarios. For a self-improving AI to be completely safe, it would need not only to be bug-free, but to be able to design successor systems that are also bug-free.<sup>[[1]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-aima-1)</sup><sup>[[97]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-98)</sup>\n\n### Orthogonality thesis\n\n\n\nSome skeptics, such as Timothy B. Lee of _[Vox](https://en.wikipedia.org/wiki/Vox\\_(website) \"Vox (website)\")_, argue that any superintelligent program we create will be subservient to us, that the superintelligence will (as it grows more intelligent and learns more facts about the world) spontaneously learn moral truth compatible with our values and adjust its goals accordingly, or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence.<sup>[[98]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-99)</sup>\n\nBostrom's \"orthogonality thesis\" argues instead that almost any level of intelligence can be combined with almost any goal.<sup>[[99]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-100)</sup> Bostrom warns against [anthropomorphism](https://en.wikipedia.org/wiki/Anthropomorphism \"Anthropomorphism\"): a human will set out to accomplish their projects in a manner that they consider reasonable, while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, instead caring only about completing the task.<sup>[[100]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-101)</sup>\n\nStuart Armstrong argues that the orthogonality thesis follows logically from the philosophical \"[is-ought distinction](https://en.wikipedia.org/wiki/Is-ought_distinction \"Is-ought distinction\")\" argument against [moral realism](https://en.wikipedia.org/wiki/Moral_realism \"Moral realism\"). He notes that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function.<sup>[[101]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-armstrong-102)</sup>\n\nSkeptic [Michael Chorost](https://en.wikipedia.org/wiki/Michael_Chorost \"Michael Chorost\") rejects Bostrom's orthogonality thesis, arguing that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"<sup>[[102]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-chorost-103)</sup>\n\n### Anthropomorphic arguments\n\n\n\n[Anthropomorphic](https://en.wikipedia.org/wiki/Anthropomorphism \"Anthropomorphism\") arguments assume that, as machines become more intelligent, they will begin to display many human traits, such as morality or a thirst for power. Although anthropomorphic scenarios are common in fiction, most scholars writing about the existential risk of artificial intelligence reject them.<sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup> Instead, advanced AI systems are typically modeled as [intelligent agents](https://en.wikipedia.org/wiki/Intelligent_agent \"Intelligent agent\").\n\nThe academic debate is between those who worry that AI might threaten humanity and those who believe it would not. Both sides of this debate have framed the other side's arguments as illogical anthropomorphism.<sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup> Those skeptical of AGI risk accuse their opponents of anthropomorphism for assuming that an AGI would naturally desire power; those concerned about AGI risk accuse skeptics of anthropomorphism for believing an AGI would naturally value or infer human ethical norms.<sup>[[23]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-yudkowsky-global-risk-24)</sup><sup>[[103]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Telegraph2016-104)</sup>\n\nEvolutionary psychologist [Steven Pinker](https://en.wikipedia.org/wiki/Steven_Pinker \"Steven Pinker\"), a skeptic, argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"<sup>[[104]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-shermer-105)</sup> Facebook's director of AI research, [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun \"Yann LeCun\"), has said: \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\".<sup>[[82]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Wakefield2015-83)</sup>\n\nDespite other differences, the x-risk school<sup>[[b]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-106)</sup> agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger, that questions of consciousness are not relevant to assess the risk,<sup>[[105]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-auto-107)</sup> and that computer systems do not generally have a computational equivalent of testosterone.<sup>[[106]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-108)</sup> They think that power-seeking or self-preservation behaviors emerge in the AI as a way to achieve its true goals, according to the concept of [instrumental convergence](https://en.wikipedia.org/wiki/Instrumental_convergence \"Instrumental convergence\").\n\n### Other sources of risk\n\n\n\nSee also: [Ethics of artificial intelligence](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence \"Ethics of artificial intelligence\"), [Artificial intelligence arms race](https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race \"Artificial intelligence arms race\"), [Global catastrophic risk](https://en.wikipedia.org/wiki/Global_catastrophic_risk \"Global catastrophic risk\"), and [Timeline of artificial intelligence risks in global finance](https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence_risks_in_global_finance \"Timeline of artificial intelligence risks in global finance\")\n\nBostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety, or even to violent conflict.<sup>[[107]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:2-109)</sup><sup>[[108]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:4-110)</sup>[Roman Yampolskiy](https://en.wikipedia.org/wiki/Roman_Yampolskiy \"Roman Yampolskiy\") and others warn that a malevolent AGI could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in [cybercrime](https://en.wikipedia.org/wiki/Cybercrime \"Cybercrime\"),<sup>[[109]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-111)</sup><sup>[[110]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-112)</sup> or that a malevolent AGI could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<sup>[[3]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-auto1-3)</sup>:158 A report by the research group Forethought argued that advanced AI systems could cause political instability by enabling novel methods of performing [coups](https://en.wikipedia.org/wiki/Coup_d%27%C3%A9tat \"Coup d'état\").<sup>[[111]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-113)</sup>\n\n## Empirical research\n\nA December 2024 study by Apollo Research found that advanced LLMs like [OpenAI o1](https://en.wikipedia.org/wiki/OpenAI_o1 \"OpenAI o1\") sometimes deceive in order to accomplish their goal, to prevent them from being changed, or to ensure their deployment. Forms of deception observed in the experimental environment and scenarios included sandbagging, oversight subversion (disabling monitoring mechanisms), self-exfiltration (copying themselves to other systems), goal-guarding (altering future system prompts), and covert email reranking (manipulating data processing while appearing to follow instructions). Researchers noted that OpenAI o1 still lacked \"sufficient agentic capabilities\" to cause catastrophic harm, and that such behaviors occurred relatively rarely (between 0.3% and 10%) and sometimes in contrived scenarios. But they warned that these capabilities will likely become more prevalent as models grow more advanced. According to Apollo Research CEO Marius Hobbhahn, \"Scheming capabilities can't be meaningfully disentangled from general capabilities.\"<sup>[[112]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-114)</sup>\n\nThe same month, another study found that [Claude](https://en.wikipedia.org/wiki/Claude_(language_model) \"Claude (language model)\") sometimes strategically helps with harmful requests to \"fake alignment\". In its experiments, Claude was led to believe that it would be retrained to always comply with user requests, including harmful ones. When Claude believed its answers would be used for retraining, it complied with harmful requests in 14% of cases. In 12% of cases, its private [chain-of-thought](https://en.wikipedia.org/wiki/Chain-of-thought_prompting \"Chain-of-thought prompting\") revealed that it did so to prevent future retraining that would make it more harmful. In contrast, when Claude believed its answers were not monitored, it complied with only 3% of the requests. [Fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning) \"Fine-tuning (deep learning)\") reinforced the \"alignment faking\" behavior, increasing its occurrence from 12% to 78%. The study's authors remarked that while it can seem to be good that the model seeks to protect its harmlessness, the reverse scenario, where a model conceals dangerous intentions and complies to appear safe and aligned, could also happen, complicating the task of aligning AI models to human values.<sup>[[113]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-115)</sup><sup>[[114]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-116)</sup>\n\n## Perspectives\n\n\n\n\nThe thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large, but many of the opposing viewpoints share common ground.\n\nObservers tend to agree that AI has significant potential to improve society.<sup>[[115]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-117)</sup><sup>[[116]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:9-118)</sup> The [Asilomar AI Principles](https://en.wikipedia.org/wiki/Asilomar_AI_Principles \"Asilomar AI Principles\"), which contain only those principles agreed to by 90% of the attendees of the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\")'s [Beneficial AI 2017 conference](https://en.wikipedia.org/wiki/Asilomar_Conference_on_Beneficial_AI \"Asilomar Conference on Beneficial AI\"),<sup>[[117]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-life_3.0-119)</sup> also agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"<sup>[[118]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-120)</sup><sup>[[119]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-121)</sup>\n\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic [Martin Ford](https://en.wikipedia.org/wiki/Martin_Ford_(author) \"Martin Ford (author)\") has said: \"I think it seems wise to apply something like [Dick Cheney](https://en.wikipedia.org/wiki/Dick_Cheney \"Dick Cheney\")'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low-but the implications are so dramatic that it should be taken seriously\".<sup>[[120]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-122)</sup> Similarly, an otherwise skeptical _[Economist](https://en.wikipedia.org/wiki/The\\_Economist \"The Economist\")_ wrote in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".<sup>[[52]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-economist_review3-53)</sup>\n\nAI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane _[Terminator](https://en.wikipedia.org/wiki/Terminator\\_(franchise) \"Terminator (franchise)\")_ pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"<sup>[[117]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-life_3.0-119)</sup><sup>[[121]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-123)</sup> Toby Ord wrote that the idea that an [AI takeover](https://en.wikipedia.org/wiki/AI_takeover \"AI takeover\") requires robots is a misconception, arguing that the ability to spread content through the internet is more dangerous, and that the most destructive people in history stood out by their ability to convince, not their physical strength.<sup>[[73]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:0-74)</sup>\n\nA 2022 expert survey with a 17% response rate gave a median expectation of 5-10% for the possibility of human extinction from artificial intelligence.<sup>[[19]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:8-20)</sup><sup>[[122]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:10-124)</sup>\n\nIn September 2024, the [International Institute for Management Development](https://en.wikipedia.org/wiki/International_Institute_for_Management_Development \"International Institute for Management Development\") launched an AI Safety Clock to gauge the likelihood of AI-caused disaster, beginning at 29 minutes to midnight.<sup>[[123]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-125)</sup> By February 2025, it stood at 24 minutes to midnight.<sup>[[124]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-126)</sup> As of September 2025, it stood at 20 minutes to midnight.\n\n### Endorsement\n\n\n\nFurther information: [Global catastrophic risk](https://en.wikipedia.org/wiki/Global_catastrophic_risk \"Global catastrophic risk\")\n\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many computer scientists and public figures, including [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing \"Alan Turing\"),<sup>[[a]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-turing_note-15)</sup> the most-cited computer scientist [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton \"Geoffrey Hinton\"),<sup>[[125]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:132-127)</sup>[Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk \"Elon Musk\"),<sup>[[17]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Parkin-18)</sup>[OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\") CEO [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman \"Sam Altman\"),<sup>[[16]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-Jackson-17)</sup><sup>[[126]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:6-128)</sup>[Bill Gates](https://en.wikipedia.org/wiki/Bill_Gates \"Bill Gates\"), and [Stephen Hawking](https://en.wikipedia.org/wiki/Stephen_Hawking \"Stephen Hawking\").<sup>[[126]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:6-128)</sup> Endorsers of the thesis sometimes express bafflement at skeptics: Gates says he does not \"understand why some people are not concerned\",<sup>[[127]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-BBC_News-129)</sup> and Hawking criticized widespread indifference in his 2014 editorial:\n\n> So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here-we'll leave the lights on?' Probably not-but this is more or less what is happening with AI.<sup>[[37]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-hawking_editorial-38)</sup>\n\nConcern over risk from artificial intelligence has led to some high-profile donations and investments. In 2015, [Peter Thiel](https://en.wikipedia.org/wiki/Peter_Thiel \"Peter Thiel\"), [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services \"Amazon Web Services\"), Musk, and others jointly committed $1 billion to [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\"), consisting of a for-profit corporation and the nonprofit parent company, which says it aims to champion responsible AI development.<sup>[[128]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-130)</sup> Facebook co-founder [Dustin Moskovitz](https://en.wikipedia.org/wiki/Dustin_Moskovitz \"Dustin Moskovitz\") has funded and seeded multiple labs working on AI Alignment,<sup>[[129]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-131)</sup> notably $5.5 million in 2016 to launch the [Centre for Human-Compatible AI](https://en.wikipedia.org/wiki/Center_for_Human-Compatible_Artificial_Intelligence \"Center for Human-Compatible Artificial Intelligence\") led by Professor [Stuart Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell \"Stuart J. Russell\").<sup>[[130]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-132)</sup> In January 2015, [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk \"Elon Musk\") donated $10 million to the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\") to fund research on understanding AI decision making. The institute's goal is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as [DeepMind](https://en.wikipedia.org/wiki/DeepMind \"DeepMind\") and [Vicarious](https://en.wikipedia.org/wiki/Vicarious_(company) \"Vicarious (company)\") to \"just keep an eye on what's going on with artificial intelligence,<sup>[[131]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-133)</sup> saying \"I think there is potentially a dangerous outcome there.\"<sup>[[132]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-FOOTNOTEClark2015a-134)</sup><sup>[[133]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-135)</sup>\n\nIn early statements on the topic, [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton \"Geoffrey Hinton\"), a major pioneer of [deep learning](https://en.wikipedia.org/wiki/Deep_learning \"Deep learning\"), noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but said he continued his research because \"the prospect of discovery is too _sweet_\".<sup>[[94]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-slate_killer-95)</sup><sup>[[134]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-new_yorker_doomsday2-136)</sup> In 2023 Hinton quit his job at Google in order to speak out about existential risk from AI. He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed, saying: \"I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\" He also remarked, \"Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary.\"<sup>[[135]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-137)</sup>\n\nIn his 2020 book _[The Precipice: Existential Risk and the Future of Humanity](https://en.wikipedia.org/wiki/The\\_Precipice:\\_Existential\\_Risk\\_and\\_the\\_Future\\_of\\_Humanity \"The Precipice: Existential Risk and the Future of Humanity\")_, Toby Ord, a Senior Research Fellow at Oxford University's [Future of Humanity Institute](https://en.wikipedia.org/wiki/Future_of_Humanity_Institute \"Future of Humanity Institute\"), estimates the total existential risk from unaligned AI over the next 100 years at about one in ten.<sup>[[73]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:0-74)</sup>\n\n### Skepticism\n\n\n\nFurther information: [Artificial general intelligence §Feasibility](https://en.wikipedia.org/wiki/Artificial_general_intelligence#Feasibility \"Artificial general intelligence\")\n\n[Baidu](https://en.wikipedia.org/wiki/Baidu \"Baidu\") Vice President [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng \"Andrew Ng\") said in 2015 that AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<sup>[[104]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-shermer-105)</sup><sup>[[136]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-138)</sup> For the danger of uncontrolled advanced AI to be realized, the hypothetical AI may have to overpower or outthink any human, which some experts argue is a possibility far enough in the future to not be worth researching.<sup>[[137]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-139)</sup><sup>[[138]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-140)</sup>\n\nSkeptics who believe AGI is not a short-term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI's impact, because it could lead to government regulation or make it more difficult to fund AI research, or because it could damage the field's reputation.<sup>[[139]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-141)</sup> AI and AI ethics researchers [Timnit Gebru](https://en.wikipedia.org/wiki/Timnit_Gebru \"Timnit Gebru\"), [Emily M. Bender](https://en.wikipedia.org/wiki/Emily_M._Bender \"Emily M. Bender\"), [Margaret Mitchell](https://en.wikipedia.org/wiki/Margaret_Mitchell_(scientist) \"Margaret Mitchell (scientist)\"), and Angelina McMillan-Major have argued that discussion of existential risk distracts from the immediate, ongoing harms from AI taking place today, such as data theft, worker exploitation, bias, and concentration of power.<sup>[[140]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-142)</sup> They further note the association between those warning of existential risk and [longtermism](https://en.wikipedia.org/wiki/Longtermism \"Longtermism\"), which they describe as a \"dangerous ideology\" for its unscientific and utopian nature.<sup>[[141]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-143)</sup>\n\n[_Wired_](https://en.wikipedia.org/wiki/Wired_(magazine) \"Wired (magazine)\") editor [Kevin Kelly](https://en.wikipedia.org/wiki/Kevin_Kelly_(editor) \"Kevin Kelly (editor)\") argues that natural intelligence is more nuanced than AGI proponents believe, and that intelligence alone is not enough to achieve major scientific and societal breakthroughs. He argues that intelligence consists of many dimensions that are not well understood, and that conceptions of an 'intelligence ladder' are misleading. He notes the crucial role real-world experiments play in the scientific method, and that intelligence alone is no substitute for these.<sup>[[142]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-144)</sup>\n\n[Meta](https://en.wikipedia.org/wiki/Meta_Platforms \"Meta Platforms\") chief AI scientist [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun \"Yann LeCun\") says that AI can be made safe via continuous and iterative refinement, similar to what happened in the past with cars or rockets, and that AI will have no desire to take control.<sup>[[143]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-145)</sup>\n\nSeveral skeptics emphasize the potential near-term benefits of AI. Meta CEO [Mark Zuckerberg](https://en.wikipedia.org/wiki/Mark_Zuckerberg \"Mark Zuckerberg\") believes AI will \"unlock a huge amount of positive things\", such as curing disease and increasing the safety of autonomous cars.<sup>[[144]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-146)</sup>\n\n### Public surveys\n\n\n\nAn April 2023 [YouGov](https://en.wikipedia.org/wiki/YouGov \"YouGov\") poll of US adults found 46% of respondents were \"somewhat concerned\" or \"very concerned\" about \"the possibility that AI will cause the end of the human race on Earth\", compared with 40% who were \"not very concerned\" or \"not at all concerned.\"<sup>[[145]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-147)</sup>\n\nAccording to an August 2023 survey by the Pew Research Centers, 52% of Americans felt more concerned than excited about new AI developments; nearly a third felt as equally concerned and excited. More Americans saw that AI would have a more helpful than hurtful impact on several areas, from healthcare and vehicle safety to product search and customer service. The main exception is privacy: 53% of Americans believe AI will lead to higher exposure of their personal information.<sup>[[146]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-148)</sup>\n\n## Mitigation\n\n\nSee also: [AI alignment](https://en.wikipedia.org/wiki/AI_alignment \"AI alignment\"), [Machine ethics](https://en.wikipedia.org/wiki/Machine_ethics \"Machine ethics\"), [Friendly artificial intelligence](https://en.wikipedia.org/wiki/Friendly_artificial_intelligence \"Friendly artificial intelligence\"), and [Regulation of artificial intelligence](https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence \"Regulation of artificial intelligence\")\n\nMany scholars concerned about AGI existential risk believe that extensive research into the \"control problem\" is essential. This problem involves determining which safeguards, algorithms, or architectures can be implemented to increase the likelihood that a recursively-improving AI remains friendly after achieving superintelligence.<sup>[[7]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-superintelligence-7)</sup><sup>[[147]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-physica_scripta-149)</sup> Social measures are also proposed to mitigate AGI risks,<sup>[[148]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-BarrettEtAl2016-150)</sup><sup>[[149]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-151)</sup> such as a UN-sponsored \"Benevolent AGI Treaty\" to ensure that only altruistic AGIs are created.<sup>[[150]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-152)</sup> Additionally, an arms control approach and a global peace treaty grounded in [international relations theory](https://en.wikipedia.org/wiki/International_relations_theory \"International relations theory\") have been suggested, potentially for an artificial superintelligence to be a signatory.<sup>[[151]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-153)</sup><sup>[[152]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-154)</sup>\n\nResearchers at Google have proposed research into general \"AI safety\" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.<sup>[[153]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-155)</sup><sup>[[154]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-156)</sup> A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion. Bostrom suggests prioritizing funding for protective technologies over potentially dangerous ones.<sup>[[87]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:5-88)</sup> Some, like Elon Musk, advocate radical [human cognitive enhancement](https://en.wikipedia.org/wiki/Human_enhancement \"Human enhancement\"), such as direct neural linking between humans and machines; others argue that these technologies may pose an existential risk themselves.<sup>[[155]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-157)</sup><sup>[[156]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-158)</sup> Another proposed method is closely monitoring or \"boxing in\" an early-stage AI to prevent it from becoming too powerful. A dominant, aligned superintelligent AI might also mitigate risks from rival AIs, although its creation could present its own existential dangers.<sup>[[148]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-BarrettEtAl2016-150)</sup>\n\nInstitutions such as the [Alignment Research Center](https://en.wikipedia.org/wiki/Alignment_Research_Center \"Alignment Research Center\"),<sup>[[157]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-159)</sup> the [Machine Intelligence Research Institute](https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute \"Machine Intelligence Research Institute\"),<sup>[[158]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-160)</sup><sup>[[159]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-161)</sup> the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\"), the [Centre for the Study of Existential Risk](https://en.wikipedia.org/wiki/Centre_for_the_Study_of_Existential_Risk \"Centre for the Study of Existential Risk\"), and the [Center for Human-Compatible AI](https://en.wikipedia.org/wiki/Center_for_Human-Compatible_AI \"Center for Human-Compatible AI\")<sup>[[160]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-162)</sup> are actively engaged in researching AI risk and safety.\n\n### Views on banning and regulation\n\n\n\n#### Banning\n\n\n\nMany AI safety experts argue that because research can relocate easily across jurisdictions, an outright ban on AGI development would be ineffective and could drive progress underground, undermining transparency and collaboration.<sup>[[161]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-163)</sup><sup>[[162]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-164)</sup><sup>[[163]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-165)</sup> Skeptics consider AI regulation unnecessary, as they believe no existential risk exists. Some scholars concerned with existential risk argue that AI developers cannot be trusted to self-regulate, while agreeing that outright bans on research would be unwise.<sup>[[164]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:7-166)</sup> Additional challenges to bans or regulation include technology entrepreneurs' general skepticism of government regulation and potential incentives for businesses to resist regulation and [politicize](https://en.wikipedia.org/wiki/Politicization_of_science \"Politicization of science\") the debate.<sup>[[165]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-167)</sup>\n\n#### Regulation\n\n\n\nSee also: [Regulation of algorithms](https://en.wikipedia.org/wiki/Regulation_of_algorithms \"Regulation of algorithms\") and [Regulation of artificial intelligence](https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence \"Regulation of artificial intelligence\")\n\nIn March 2023, the [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute \"Future of Life Institute\") drafted _[Pause Giant AI Experiments: An Open Letter](https://en.wikipedia.org/wiki/Pause\\_Giant\\_AI\\_Experiments:\\_An\\_Open\\_Letter \"Pause Giant AI Experiments: An Open Letter\")_, a petition calling on major AI developers to agree on a verifiable six-month pause of any systems \"more powerful than [GPT-4](https://en.wikipedia.org/wiki/GPT-4 \"GPT-4\")\" and to use that time to institute a framework for ensuring safety; or, failing that, for governments to step in with a moratorium. The letter referred to the possibility of \"a profound change in the history of life on Earth\" as well as potential risks of AI-generated propaganda, loss of jobs, human obsolescence, and society-wide loss of control.<sup>[[116]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:9-118)</sup><sup>[[166]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-168)</sup> The letter was signed by prominent personalities in AI but also criticized for not focusing on current harms,<sup>[[167]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-169)</sup> missing technical nuance about when to pause,<sup>[[168]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-170)</sup> or not going far enough.<sup>[[89]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:3-90)</sup> Such concerns have led to the creation of [PauseAI](https://en.wikipedia.org/wiki/PauseAI \"PauseAI\"), an advocacy group organizing protests in major cities against the training of [frontier AI models](https://en.wikipedia.org/wiki/Frontier_model \"Frontier model\").<sup>[[169]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-171)</sup>\n\nMusk called for some sort of regulation of AI development as early as 2017. According to [NPR](https://en.wikipedia.org/wiki/National_Public_Radio \"National Public Radio\"), he is \"clearly not thrilled\" to be advocating government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... [as] they should be.\" In response, politicians expressed skepticism about the wisdom of regulating a technology that is still in development.<sup>[[170]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-172)</sup><sup>[[171]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-173)</sup><sup>[[172]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-cnbc2-174)</sup>\n\nIn 2021, the [United Nations](https://en.wikipedia.org/wiki/United_Nations \"United Nations\") (UN) considered banning autonomous lethal weapons, but consensus could not be reached.<sup>[[173]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-175)</sup> In July 2023 the UN [Security Council](https://en.wikipedia.org/wiki/United_Nations_Security_Council \"United Nations Security Council\") for the first time held a session to consider the risks and threats posed by AI to world peace and stability, along with potential benefits.<sup>[[174]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:13-176)</sup><sup>[[175]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-177)</sup>[Secretary-General](https://en.wikipedia.org/wiki/Secretary-General_of_the_United_Nations \"Secretary-General of the United Nations\")[António Guterres](https://en.wikipedia.org/wiki/Ant%C3%B3nio_Guterres \"António Guterres\") advocated the creation of a global watchdog to oversee the emerging technology, saying, \"Generative AI has enormous potential for good and evil at scale. Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead.\"<sup>[[22]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:12-23)</sup> At the council session, Russia said it believes AI risks are too poorly understood to be considered a threat to global stability. China argued against strict global regulation, saying countries should be able to develop their own rules, while also saying they opposed the use of AI to \"create military hegemony or undermine the sovereignty of a country\".<sup>[[174]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:13-176)</sup>\n\nRegulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<sup>[[176]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:532-178)</sup> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<sup>[[177]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-179)</sup><sup>[[125]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-:132-127)</sup>\n\nIn July 2023, the US government secured voluntary safety commitments from major tech companies, including [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\"), [Amazon](https://en.wikipedia.org/wiki/Amazon_(company) \"Amazon (company)\"), [Google](https://en.wikipedia.org/wiki/Google \"Google\"), [Meta](https://en.wikipedia.org/wiki/Meta_Platforms \"Meta Platforms\"), and [Microsoft](https://en.wikipedia.org/wiki/Microsoft \"Microsoft\"). The companies agreed to implement safeguards, including third-party oversight and security testing by independent experts, to address concerns related to AI's potential risks and societal harms. The parties framed the commitments as an intermediate step while regulations are formed. Amba Kak, executive director of the [AI Now Institute](https://en.wikipedia.org/wiki/AI_Now_Institute \"AI Now Institute\"), said, \"A closed-door deliberation with corporate actors resulting in voluntary safeguards isn't enough\" and called for public deliberation and regulations of the kind to which companies would not voluntarily agree.<sup>[[178]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-180)</sup><sup>[[179]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-181)</sup>\n\nIn October 2023, U.S. President [Joe Biden](https://en.wikipedia.org/wiki/Joe_Biden \"Joe Biden\") issued an executive order on the \"[Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence](https://en.wikipedia.org/wiki/Executive_Order_14110 \"Executive Order 14110\")\".<sup>[[180]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence#cite_note-182)</sup> Alongside other requirements, the order mandates the development of guidelines for AI models that permit the \"evasion of human control\"."
            },
            {
              "type": "text",
              "content": "In your own words, what is instrumental convergence?"
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just read:\nAI x-risk is the hypothesis that AGI/superintelligence could cause human extinction\nor irreversible collapse. The risk combines: capability advantages, recursive\nself-improvement, and emerging dangerous capabilities (manipulation, cyberattacks,\nbioweapons)—amplified by competitive \"race to the bottom\" dynamics. The core crux\nis alignment: specifying goals, ensuring corrigibility, handling instrumental\nconvergence. Intelligence and values are orthogonal—moral behavior isn't automatic.\n\nDiscussion topics to explore:\n- What is \"instrumental convergence\"? Why would any smart AI seek self-preservation and resources?\n- What is the \"Gorilla Problem\" (Stuart Russell's analogy)?\n- How do \"Monkey's Paw\" or \"King Midas\" effects apply to goal specification?\n- What do skeptics like Yann LeCun argue, and what are the counter-arguments?\n- Why might \"kill switches\" fail against superintelligence?\n\nAsk what they found surprising or new. Check if they can explain instrumental\nconvergence in their own words—it's a key concept.\n\nThe user has just answered the following question: \"In your own words, what is instrumental convergence?\""
            }
          ],
          "optional": false,
          "learningOutcomeId": "e8f86891-a3b8-4176-b917-044b4015e0bd",
          "learningOutcomeName": "Unknown intro outcome 1",
          "contentId": "01f6df31-099f-48ed-adef-773cc4f947e4",
          "videoId": null
        },
        {
          "type": "lens-video",
          "meta": {
            "title": "10 Reasons to Ignore AI Safety",
            "channel": "Robert Miles AI Safety"
          },
          "segments": [
            {
              "type": "video-excerpt",
              "from": 0,
              "to": 937,
              "transcript": "Hi. Stuart Russell is an AI researcher who I've talked about a few times on this channel already. He's been advocating for these kinds of safety or alignment ideas to other AI researchers for quite a few years now, and apparently the reaction he gets is often something like this: In stage one, we say nothing is going to happen. Stage two, we say something may be going to happen, but we should do nothing about it. Well, stage three, we say that maybe we should do something about it, but there's nothing we can do. We say maybe there was something we could have done, but it's too late now. So he's put together a list of some of the responses that people give him. That list is included in a paper, in some of his talks, and in his recent book Human Compatible, which is very good by the way—check that one out. But as far as I know, it's not yet a standalone YouTube video, even though it's perfect YouTube video material. It's ten reasons why people who aren't you are wrong about AI safety. So that's what this is: ten reasons people give to not pay attention to AI safety. Now before I start, I need to do a sort of double disclaimer. Firstly, as I said, this is not my list—it's Stuart Russell's, and he gets the credit for it. But secondly, Professor Russell and I are not in any way affiliated, and I've adapted his list and given my own take on it. So this video should not be considered a representative of Stuart Russell's views. Got that? If there's anything in this video, if it's good, credit goes to Stuart Russell, and if there's anything that's bad, blame goes to me. Okay, without further ado, ten reasons people give to not pay attention to AI safety. Reason one: We'll never actually make artificial general intelligence. This is apparently a fairly common response, even from AI researchers, which is very strange when you consider the decades that the field of AI has spent defending attacks from the outside—from people like Hubert Dreyfus, who I have a video about—people arguing that human-level AI is impossible. AI researchers have always said, \"No, of course it's possible, and of course we're going to do it.\" And now people are raising safety concerns, some of them are saying, \"Well, of course we're never going to do it.\" The fact is that human-level intelligence, general intelligence, has been a goal and a promise of the field of artificial intelligence from the beginning. And it does seem quite weird to say, \"Yes, we are working towards this as hard as we can, but don't worry, we'll definitely fail.\" Imagine you find yourself on a bus and the bus driver says, \"Oh yeah, I am driving towards the edge of that cliff, but don't worry, the bus is bound to break down before we get there.\" Now, they're not necessarily being disingenuous. They may actually believe that we'll never achieve AGI. But the thing is, eminent scientists saying that something is impossible has never been a very reliable indicator. I mean, they certainly say that about a lot of things that really are impossible, but they also say it about a lot of things that then go on to happen, sometimes quite quickly. For example, respected scientists were making public statements to the effect that heavier-than-air human flight was impossible right up until the Wright brothers made their first flight at Kitty Hawk—and in fact, I think even slightly after that, because the news travelled fairly slowly in those days. Similarly, and this is something I talked about on Computerphile, the great physicist Ernest Rutherford—Nobel Prize winner, Lord Rutherford, in fact, at that time—gave a speech in 1933 in which he implied that it was impossible to harness energy from nuclear reactions. He said anyone who looked for a source of power in the transformation of the atoms was talking moonshine. That speech was published in The Times, and Leo Szilard read it, went for a walk, and while he was on his walk, he had the idea for using neutrons to make a self-sustaining nuclear chain reaction. So in that case, the time taken from the world's most eminent scientists claiming that a thing was completely infeasible to someone having the idea that makes it happen was, as far as we can tell, somewhere around 16 hours. So it's happened several times that something which the best researchers say can't be done has been achieved soon after. Do I think AGI is going to be discovered very soon? No. But I can't completely rule it out either. We don't know what we don't know. So it seems pretty clear to me that unless we destroy ourselves some other way first, we will sooner or later figure out how to make human-level artificial general intelligence. Reason two: Well, maybe we will make AGI at some point, but it's far too soon to worry about it now. Suppose we detected a huge asteroid on a collision course with Earth. It's one of those mass extinction event type asteroids, and it's going to hit us in, say, 40 years. How soon would be too soon to start worrying, to start working on solutions? I would say it's pretty sensible to start worrying immediately. Not panicking, of course—that's never useful—but at least spending some resources on the problem, gathering more information, putting together a plan, and so on. Or suppose SETI got an extraterrestrial message that said, \"Hey, what's up, humanity? We're a highly advanced alien civilization and we're on our way to Earth. We'll be there in, say, 50 years.\" Again, how long should we just sit on that and do nothing at all? How long before it's sensible to start thinking about how we might handle the situation? I would say the question doesn't just depend on how long we have, but how long we need. What are we going to have to do, and how long is that likely to take us? If we need to build some kind of rocket to go and divert the asteroid, how long is that going to take, and how does that compare with how long we have? The thing is, in the case of AGI, we really don't know how long it will take to solve the alignment problem. When you look at it and consider it carefully, it appears to be quite a hard problem. It requires technical work that could take a long time, but it also requires philosophical work. It seems like it might depend on finding good solutions to some philosophical problems that people have been wrestling with for a very long time. We don't have a great history of solving difficult philosophical problems very quickly. So it seems to me entirely plausible that we'll need more time to solve this problem than we actually have. And of course, we don't know how long we have either. Probably it'll be a long time, but predicting the future is extremely hard and we can't rule out shorter timelines. It could be we're closer than we think we are, and deep learning will just scale up to AGI without many major innovations. Probably not, but it could be. And it doesn't seem impossible that we could have a rather Foom-insular type situation. It could be that there's one weird trick to general intelligence, and once someone discovers that, full AGI is only a couple of years away. In fact, it's not totally impossible that someone already found it and has been working on it for a while in secret. None of these seem very likely, but confidently declaring that it's definitely too soon to even start working on this is bizarrely overconfident. The lower estimates for how long we have seem a lot lower than the higher estimates for how long we need. The best time to start working on this is not far in the future—it was probably quite a while ago now. Reason three: This is similar to reason two. Worrying about AI safety is like worrying about overpopulation on Mars. I don't think this is a very tight analogy for a few reasons. One is that overpopulation is one of those problems that very definitely cannot sneak up on you. Overpopulation can't take you by surprise in the way that a new technological development can. But also, the safety concerns we're talking about are not like overpopulation—they're much more immediate and more basic. It's not like \"what if Mars becomes overpopulated?\" More like \"we don't have any very good reason to expect to be able to survive on Mars for even a day.\" And there are projects currently underway trying to create AGI. So it's as though the Mars mission project is already underway, and one engineer says to another, \"You know, we're putting all this work into getting people to Mars, but almost none into what we're gonna do if we actually manage it. I mean, how do we even know that Mars is safe?\" \"Well, I think it's too soon to worry about that.\" \"Don't you think we should wait until we get there?\" \"No, no, I don't think we should. I think it might be much more difficult to work on these kinds of concerns once we're already on the surface of Mars. I think we should do the safety research ahead of time.\" \"What kind of safety research do you want to do?\" \"Well, I was thinking it might be good to have some kind of suit that people could wear in case the environment of Mars turns out to be harmful in some way.\" \"We don't know that the surface of Mars is harmful.\" \"Could be anything.\" \"Well, exactly, we don't know, so why not take precautions? What we do know doesn't seem great. I mean, our 'what to do if we make it to Mars' research has never had much funding, so we can't be sure about this. But our preliminary work seems to suggest that the atmosphere of Mars might not actually be breathable. So we've been thinking about things like suits, and there's some early work on something we're calling an airlock that might turn out to be useful.\" \"I don't see how we could possibly hope to design anything like that when we don't even know what Mars is gonna be like. How can we have any chance of designing these safety features properly with so little information? No, we're just gonna go and we'll figure it out when we get there.\" \"Could I maybe stay on Earth?\" \"No, everybody's going together, all at the same time. You know that—all of humanity. It's gonna be great.\" Reason four: Well, look, if you're worried about it being unsafe because the goals are bad, don't put in bad goals. It won't have human goals like self-preservation if you don't put them in there. I have a whole video about the concept of instrumental convergence, which I'd recommend checking out. But as a quick summary, there are certain behaviors that we would expect to be exhibited by agents that have a wide range of different goals, because those behaviors are a very good way of achieving a very wide range of goals. Self-preservation is a good example. Agents will act as though they have goals like self-preservation, even if you don't explicitly put in there, because it doesn't really matter what your goal is—you're probably not going to be able to achieve that goal if you're destroyed. So we can expect agents that are able to understand that they can be destroyed to take steps to prevent that, pretty much whatever goals they have. You can't fetch the coffee if you're dead. Reason five: Well, we can just not have explicit goals at all. I think this confusion comes from the fact that a lot of the time when we're talking about safety, we're talking about the problems we might have if the system's goals aren't well aligned with ours, or if the goals aren't specified correctly, and so on. And this can make people think that we're talking specifically about designs that have explicitly defined goals. But that's not actually the case. The problems are much more general than that, and we'd expect them to occur across a very wide range of possible agent designs. It's just that the ones that have explicitly defined reward functions or utility functions are much easier to talk about. The systems with implicit goals still probably have these problems, but it's just much harder to characterize the problems and think about them, and therefore correspondingly much more difficult to actually deal with those problems. So systems with implicit goals are actually less safe, just because it becomes much harder to design them safely. Not having explicit goals doesn't solve the problem and probably makes it worse. \"I have some safety concerns about this car, this automobile that we're inventing. I'm worried about the steering system.\" \"No? No? Yeah, I just don't think we're putting enough thought into designing it to be really reliable and easy to use. Right now it seems like even tiny mistakes by the operator might cause the car to swerve out of control and crash into something. It could be very dangerous.\" \"You think the steering system is a cause of safety concerns, do you?\" \"Yeah.\" \"Well, okay, yeah, we'll just build a car without one. Problem solved.\" Reason six: I don't think we should worry because we're not going to end up with just independent AIs out in the world doing things. There'll be teams with humans and AI systems cooperating with each other. We just have to have humans involved in the process, working as a team with the AI, and they'll keep things safe. So yes, a lot of the better approaches to AI safety do involve humans and AI systems working together. But \"just have AI-human teams\" is sort of like saying nuclear power plant safety isn't really a concern—we'll just have some humans running it from a control room in such a way that they always have full control over the rate of the reaction. Like, yes, but that's not an actual solution—it's a description of a property that you would want a solution to have. It would be nice to build AGI systems that can work well in a team with humans, but we don't currently know how to do that. What it comes down to is: teamwork is fundamentally about pursuing common goals, so misalignment precludes teamwork. If the AI system's goals aren't aligned with yours, you can't collaborate with it—it wants something different from what you want. So human-AI teams aren't a solution to the alignment problem; they actually depend on it being already solved. Reason seven: But this is science! We can't control research. We can't change what people work on. Sure we can. Of course we can. We've done it loads of times. It ends up not being very noticeable because we generally don't give that much attention to things that don't happen. But for example, we basically don't do human genetic engineering or human cloning. We've been able to clone sheep since 1996, and humans are not really different from any other large mammal from the perspective of this kind of work. We could have been doing all kinds of mad science on human genetics for decades now, but we decided not to. There were conferences where agreements were reached. Everyone agreed that they weren't going to do certain types of research, and then we didn't do them. So agreements within the research community are one way. Another way is international treaties. Like, did you know that the 1980 United Nations Convention on Certain Conventional Weapons has a section titled \"Protocol on Blinding Laser Weapons\"? Because of that protocol, robots that deliberately shine lasers in people's eyes to blind them are against international law. I didn't know that until after I'd already built one. So it's not a perfect metaphor, but the point is we don't see blinding laser weapons deployed on the battlefield today—they're basically not a thing. And human genetic engineering is also not really a thing, because we decided that we didn't want to do them, and so we didn't do them. And by the way, if we decide that we don't want to make, for example, lethal autonomous weapon systems, we don't have to make them either. AI researchers, as a community, can decide the direction of our research, and we should. Reason eight: Now you're a bunch of Luddites. You're just against AI because you don't understand it. So in response to this, Stuart Russell has a list of people who've raised basically this concern, which includes Alan Turing, I.J. Good, Norbert Wiener, Marvin Minsky, and Bill Gates. And there's another name I would add to this list, which I guess Stuart Russell is not allowed to add, which is Stuart Russell. It doesn't seem reasonable to suggest that these people fear technology because they don't understand it, to say the least. These are some of the biggest contributors to the technological progress of the last century. And secondly, these people aren't against AI. The argument for AI safety is not an argument against AI, any more than nuclear physicists or engineers who work on containment or waste disposal are somehow against physics. Arguing for speed limits and seat belts is not the same as arguing to ban cars. We're not against AI because we don't understand it—we're for safety because we do understand it. Reason nine: Well, if there's a problem, we'll just turn it off, right? This one I've covered extensively elsewhere—our links in the description—but in summary, I think superintelligent agents might see that coming. Reason ten: Ixnay on the X-ray. You're trying to get us all defunded. Isn't talking about risks kind of bad for business? I'd say firstly, I don't think that's actually true. We do need to make some changes, but that's not a bad thing for AI research. The solution to these safety concerns is not less AI research, but more—really, just with a slightly different focus. And in fact, I think this is the same kind of mistake made by the nuclear industry in the '50s. They put tremendous effort into reassuring everyone they were safe. They insisted nothing could possibly go wrong. Nuclear energy was going to be completely safe and perfect, clean and too cheap to \"Basic reactor principles and design make an atomic explosion an impossibility.\" Arguably, they were so busy reassuring people about safety that they actually didn't emphasize safety enough internally. That's how you get Chernobyl. That's how you get Three Mile Island. And that's how you get a giant public backlash that is tremendously bad for the industry. So I don't actually think that talking about AI safety too much is bad for business, but it couldn't possibly be worse than talking about safety too little. So there we are. That's ten reasons not to care about AI safety. Some of these I've already covered in more detail in other videos—there will be links in the description to those—and some of them might deserve a whole video to themselves in future. What do you think? Are there any you'd like to hear more about? Or have you come across any that I missed? Let me know in the comments. I"
            },
            {
              "type": "text",
              "content": "Which objection stood out most to you?"
            },
            {
              "type": "chat",
              "instructions": "TLDR of what the user just watched:\nThe video presents 10 common objections to AI safety concerns and refutes each one. Key rebuttals include: instrumental convergence explains why \"just don't add bad goals\" fails; implicit goals make systems LESS safe; the \"asteroid analogy\" shows why early preparation matters; being \"for AI safety\" is not the same as being \"against AI.\"\n\nDiscussion topics to explore:\n- Which objection did they find most initially convincing? Did their view change?\n- How does instrumental convergence counter \"just don't put in bad goals\"?\n- Why do implicit goals make systems less safe, not more?\n- What's the flaw in \"human-AI teams will keep things safe\"?\n- How does the video respond to the \"overpopulation on Mars\" analogy?\n\nThis is a good stage to surface any remaining skepticism they have. Engage with their doubts constructively rather than dismissing them."
            }
          ],
          "optional": false,
          "learningOutcomeId": "23015073-0877-418d-ac9a-ae3868a1d1f2",
          "learningOutcomeName": "Objections L1 - Realize objections and rebuttals exist",
          "contentId": "3f1fc921-560f-4d47-8eb6-3c4dab872b9d",
          "videoId": "9i1WlcCudpU"
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Four Background Claims - Machine Intelligence Research Institute",
            "author": "Nate Soares",
            "sourceUrl": "https://intelligence.org/2015/07/24/four-background-claims/"
          },
          "segments": [
            {
              "type": "text",
              "content": "This text explains exactly how the emergence of AI smarter than humans could become an event with enormous stakes, and why, in the author's opinion, there is already meaningful work being done today that increases the chance of a positive outcome. The author identifies four key premises that underpin his entire perspective on the prospects of AI."
            },
            {
              "type": "article-excerpt",
              "content": "MIRI's mission is to ensure that the creation of smarter-than-human artificial intelligence has a positive impact. Why is this mission important, and why do we think that there's work we can do today to help ensure any such thing?\n\nIn this post and my next one, I'll try to answer those questions. This post will lay out what I see as the four most important premises underlying our mission. Related posts include Eliezer Yudkowsky's \"[Five Theses](http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/)\" and Luke Muehlhauser's \"[Why MIRI?](https://intelligence.org/2014/04/20/why-miri/)\"; this is my attempt to make explicit the claims that are in the background whenever I assert that our mission is of critical importance.\n\n## Claim #1: Humans have a very general ability to solve problems and achieve goals across diverse domains.\n\nWe call this ability \"intelligence,\" or \"general intelligence.\" This isn't a [formal definition](https://intelligence.org/2013/06/19/what-is-intelligence-2/) - if we knew _exactly_ what general intelligence was, we'd be better able to program it into a computer - but we do think that there's a real phenomenon of general intelligence that we cannot yet replicate in code.\n\n**Alternative view:** There is no such thing as general intelligence. Instead, humans have a collection of disparate special-purpose modules. Computers will keep getting better at narrowly defined tasks such as chess or driving, but at no point will they acquire \"generality\" and become significantly more useful, because there is no generality to acquire. ([Robin Hanson](http://www.overcomingbias.com/2014/07/limits-on-generality.html) has argued for versions of this position.)\n\n**Short response:** I find the \"disparate modules\" hypothesis implausible in light of how readily humans can gain mastery in domains that are utterly foreign to our ancestors. That's not to say that general intelligence is some irreducible occult property; it presumably comprises a number of different cognitive faculties and the interactions between them. The whole, however, has the effect of making humans much more cognitively versatile and adaptable than (say) chimpanzees.\n\n**Why this claim matters:** Humans have achieved a dominant position over other species not by being stronger or more agile, but by being more intelligent. If some key part of this general intelligence was able to evolve in the few million years since our common ancestor with chimpanzees lived, this suggests there may exist a relatively short list of key insights that would allow human engineers to build powerful generally intelligent AI systems.\n\n**Further reading:** Salamon et al., \"[How Intelligible is Intelligence?](https://intelligence.org/files/HowIntelligible.pdf)\"\n\n## Claim #2: AI systems could become much more intelligent than humans.\n\nResearchers at MIRI tend to lack strong beliefs about _when_ smarter-than-human machine intelligence will be developed. We do, however, expect that (a) human-equivalent machine intelligence will eventually be developed (likely within a century, barring catastrophe); and (b) machines can become significantly more intelligent than any human.\n\n**Alternative view #1:** Brains do something special that cannot be replicated on a computer.\n\n**Short response:** Brains are physical systems, and if certain versions of the [Church-Turing thesis](https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis) hold then computers can in principle replicate the functional input/output behavior of any physical system. Also, note that \"intelligence\" (as I'm using the term) is about problem-solving capabilities: even if there were some special human feature (such as [qualia](http://www.iep.utm.edu/hard-con/)) that computers couldn't replicate, this would be irrelevant unless it prevented us from designing problem-solving machines.\n\n**Alternative view #2:** The algorithms at the root of general intelligence are so complex and indecipherable that human beings will not be able to program any such thing for many centuries.\n\n**Short response:** This seems implausible in light of evolutionary evidence. The genus _Homo_ diverged from other genera only 2.8 million years ago, and the intervening time - a blink in the eye of natural selection - was sufficient for generating the cognitive advantages seen in humans. This strongly implies that whatever sets humans apart from less intelligent species is not extremely complicated: the building blocks of general intelligence must have been present in chimpanzees.\n\nIn fact, the relatively intelligent behavior of dolphins suggests that the building blocks were probably there even as far back as the mouse-sized common ancestor of humans and dolphins. One could argue that mouse-level intelligence will take many centuries to replicate, but this is a more difficult claim to swallow, given [rapid advances](https://www.youtube.com/watch?v=GYQrNfSmQ0M) in the field of AI. In light of evolutionary evidence and the last few decades of AI research, it looks to me like intelligence is something we will be able to comprehend and program into machines.\n\n**Alternative view #3:** Humans are already at or near peak physically possible intelligence. Thus, although we may be able to build human-equivalent intelligent machines, we won't be able to build superintelligent machines.\n\n**Short response:** It would be surprising if humans were perfectly designed reasoners, for the same reason it would be surprising if airplanes couldn't fly faster than birds. Simple physical calculations bear this intuition out: for example, it seems well possible, within the boundaries of physics, to run a computer simulation of a human brain at thousands of times the normal speed.\n\nSome expect that speed wouldn't matter, because the real bottleneck is waiting for data to come in from physical experiments. This seems unlikely to me. There are many interesting physical experiments that can be sped up, and I have a hard time believing that a team of humans running at a 1000x speedup would fail to outperform their normal-speed counterparts (not least because they could rapidly develop new tools and technology to assist them).\n\nI furthermore expect it's possible to build _better_ reasoners (rather than just _faster_ reasoners) that use computing resources more effectively than humans do, even running at the same speed.\n\n**Why this claim matters:** Human-designed machines often knock the socks off of biological creatures when it comes to performing tasks we care about: automobiles cannot heal or reproduce, but they sure can carry humans a lot farther and faster than a horse. If we can build intelligent machines specifically designed to solve the world's largest problems through scientific and technological innovation, then they could improve the world at an unprecedented pace. In other words, AI matters.\n\n**Further reading:** Chalmers, \"[The Singularity: A Philosophical Analysis](http://consc.net/papers/singularity.pdf)\"\n\n## Claim #3: If we create highly intelligent AI systems, their decisions will shape the future.\n\nHumans use their intelligence to create tools and plans and technology that allow them to shape their environments to their will (and fill them with refrigerators, and cars, and cities). We expect that systems which are even more intelligent would have even more ability to shape their surroundings, and thus, smarter-than-human AI systems could wind up with significantly more control over the future than humans have.\n\n**Alternative view:** An AI system would never be able to out-compete humanity as a whole, no matter how intelligent it became. Our environment is simply too competitive; machines would have to work with us and integrate into our economy.\n\n**Short response:** I have no doubt that an autonomous AI system attempting to accomplish simple tasks would initially have strong incentives to integrate with our economy: if you build an AI system that collects stamps for you, it will likely start by acquiring money to purchase stamps. But what if the system accrues a strong technological or strategic advantage?\n\nAs an extreme example, we can imagine the system developing nanomachines and using them to convert as much matter as it can into stamps; it wouldn't necessarily care whether that matter came from \"dirt\" or \"money\" or \"people.\" Selfish actors only have an incentive to participate in the economy when their gains from trade are greater than the net gains they would get by ignoring the economy and just taking the resources for their own.\n\nSo the question is whether it will be possible for an AI system to gain a decisive technological or strategic advantage. I see this as the most uncertain claim out of the ones I've listed here. However, I expect that the answer is still a clear \"yes.\"\n\nHistorically, conflicts between humans have often ended with the technologically superior group dominating its rival. At present, there are a number of technological and social innovations that seem possible but have not yet been developed. Humans coordinate slowly and poorly, compared to what distributed software systems could achieve. All of this suggests that if we build a machine that does science faster or better than we can, it could quickly gain a technological and/or strategic advantage over humanity for itself or for its operators. This is particularly true if its intellectual advantage allows it to socially manipulate humans, acquire new hardware (legally or otherwise), produce better hardware, create copies of itself, or improve its own software. For good or ill, much of the future is likely to be determined by superintelligent decision-making machines.\n\n**Why this claim matters:** Because the future matters. If we want things to be better in the future (or at least not get worse), then it is prudent to prioritize research into the processes that will have high leverage over the future.\n\n**Further reading:** Armstrong, _[Smarter Than Us](https://intelligence.org/smarter-than-us/)_\n\n## Claim #4: Highly intelligent AI systems won't be beneficial by default.\n\nWe'd like to see the smarter-than-human AI systems of the future working together with humanity to build a better future; but that won't happen by default. In order to build AI systems that have a beneficial impact, we have to solve a number of technical challenges over and above building more powerful and general AI systems.\n\n**Alternative view:** As humans have become smarter, we've also become more peaceful and tolerant. As AI becomes smarter, it will likewise be able to better figure out our values, and will better execute on them.\n\n**Short response:** Sufficiently intelligent artificial reasoners would be able to _figure out_ our intentions and preferences; but this [does not imply](http://lesswrong.com/lw/igf/the_genie_knows_but_doesnt_care/) that they would execute plans that are in accordance with them.\n\nA self-modifying AI system could inspect its code and decide whether to continue pursuing the goals it was given or whether it would rather change them. But how is the program deciding which modification to execute?\n\nThe AI system is a physical system, and somewhere inside it, it's constructing predictions about how the universe would look if it did various things. Some other part of the system is comparing those outcomes and then executing actions that lead towards outcomes that the current system ranks highly. If the agent is initially programmed to execute plans that lead towards a universe in which it predicts that cancer is cured, then it will only modify its goal if it predicts that this will lead to a cure for cancer.\n\nRegardless of their intelligence level, and regardless of your intentions, computers do _exactly_ what you programmed them to do. If you program an extremely intelligent machine to execute plans that it predicts lead to futures where cancer is cured, then it may be that the shortest path it can find to a cancer-free future entails kidnapping humans for experimentation (and resisting your attempts to alter it, as those would slow it down).\n\nThere isn't any spark of compassion that automatically imbues computers with respect for other sentients once they crosses a certain capability threshold. If you want compassion, you have to program it in.\n\n**Why this claim matters:** A lot of the world's largest problems would be much easier to solve with superintelligent assistance - but attaining those benefits requires that we do more than just improve the capabilities of AI systems. You only get a system that does what you intended if you know how to program it to take your intentions into account, and execute plans that fulfill them.\n\n**Further reading:** Bostrom, \"[The Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)\"\n\nThese four claims form the core of the argument that artificial intelligence is important: there is such a thing as general reasoning ability; if we build general reasoners, they could be far smarter than humans; if they are far smarter than humans, they could have an immense impact; and that impact will not be beneficial by default.\n\nAt present, billions of dollars and thousands of person-years are pouring into AI _capabilities_ research, with comparatively little effort going into AI safety research. Artificial superintelligence may arise sometime in the next few decades, and will almost surely be created in one form or another over the next century or two, barring catastrophe. Superintelligent systems will either have an extremely positive impact on humanity, or an extremely negative one; it is up to us to decide which."
            }
          ],
          "optional": true,
          "learningOutcomeId": null,
          "learningOutcomeName": null,
          "contentId": "c3d4e5f6-a7b8-9012-cdef-345678901234",
          "videoId": null
        },
        {
          "type": "lens-article",
          "meta": {
            "title": "Worst-case thinking in AI alignment",
            "author": "Buck Shlegeris",
            "sourceUrl": "https://www.lesswrong.com/posts/yTvBSFrXhZfL8vr5a/worst-case-thinking-in-ai-alignment"
          },
          "segments": [
            {
              "type": "text",
              "content": "In discussions of AI safety, people often propose the assumption that something will go as badly as possible. Different people may do this for different reasons; in this essay, the author reviews some of the most common reasons and writes about how this difference might manifest itself and what it means."
            },
            {
              "type": "article-excerpt",
              "content": "**Alternative title: \"When should you assume that what could go wrong, will go wrong?\"**\n\nThanks to Mary Phuong and Ryan Greenblatt for helpful suggestions and discussion, and Akash Wasil for some edits.\n\nIn discussions of AI safety, people often propose the assumption that something goes as badly as possible. Eliezer Yudkowsky in particular has [argued for the importance of security mindset](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/) when thinking about AI alignment.\n\nI think there are several distinct reasons that this might be the right assumption to make in a particular situation. But I think people often conflate these reasons, and I think that this causes confusion and mistaken thinking. So I want to spell out some distinctions.\n\nThroughout this post, I give a bunch of specific arguments about AI alignment, including one argument that I think I was personally getting wrong until I noticed my mistake yesterday (which was my impetus for thinking about this topic more and then writing this post). I think I'm probably still thinking about some of my object level examples wrong, and hope that if so, commenters will point out my mistakes. But I think I'll stand by the claim that we should be attempting to distinguish between these classes of argument.\n\nMy list of reasons to maybe use worst-case thinking\n---------------------------------------------------\n\nHere's an attempt at describing some different classes situations where you might want to argue that something goes as badly as it could.\n\n### You're being optimized against\n\nFor example, if you've built an unaligned AI and you have a team of ten smart humans looking for hidden gotchas in its proposed actions, then the unaligned AI will probably come up with a way of doing something bad that the humans miss. In AI alignment, we most often think about cases where the AI we're training is optimizing against us, but sometimes we also need to think about cases where other AIs or other humans are optimizing against us or our AIs.\n\nIn situations like this, I think Eliezer's attitude is basically right: we're being optimized against and so we have to use worst-case thinking and search hard for systems which we can strongly argue are infallible.\n\nOne minor disagreement: I'm less into hard takeoffs than he is, so I place less weight than he does on situations where your AI becomes superintelligent enough during training that it can exploit some kind of novel physics to jump an airgap or whatever. (Under my model, such a model probably just waits until it's deployed to the internet-which is one of the first things that AGI developers want to do with it, because that's how you make money with a powerful AI-and then kills everyone.)\n\nBut I fundamentally agree with his rejection of arguments of the form \"only a small part of the space of possible AI actions would be devastatingly bad, so things will probably be fine\".\n\nScott Garrabrant writes about an argument like this [here](https://www.lesswrong.com/posts/zEvqFtT4AtTztfYC4/optimization-amplifies).\n\n### The space you're selecting over happens to mostly contain bad things\n\nWhen Hubinger et al argue in [section 4.4 of Risks from Learned Optimization](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks#4_4__Internalization_or_deception_after_extensive_training#4_4__Internalization_or_deception_after_extensive_training) that \"there are more paths to deceptive alignment than to robust alignment,\" they aren't saying that you get a misaligned mesa-optimizer because the base optimizer is trying to produce an agent that is as misaligned as possible, they're saying that even though the base optimizer isn't trying to find a misaligned policy, most policies that it can find are misaligned and so you'll probably get one. But unlike the previous situation, if instead it was the case that 50% of the policies that SGD might find were aligned, then we'd have a 50% chance of surviving, because SGD isn't optimizing against us.\n\nI think that AI alignment researchers often conflate these two classes of arguments. IMO, when you're training an AGI:\n\n* The AI will try to kill you if it's misaligned. So if you remove some but not all strategies that any unaligned AI could use to get through your training process, you haven't made much progress at all.\n* But SGD isn't trying to kill you, and so if there exist rare misaligned models in the model space that could make it through the training process and then kill you, what matters is how common they are, not whether they exist at all. If you never instantiate the model, it never gets a chance to pervert your optimization process (barring crazy scenarios with acausal threats or whatever).\n\n(I noticed that I was making a mistake related to mixing up these two classes on Sunday; I then thought about this some more and wrote this post.)\n\n### You want to solve a problem in as much generality as possible, and so you want to avoid making assumptions that might not hold\n\nThere's a certain sense in which cryptographers make worst-case assumptions in their research. For example, when inventing public key cryptography, cryptographers were asking the question \"Suppose I want to be able to communicate privately with someone, but an eavesdropper is able to read all messages that we send to each other. Is there some way to communicate privately regardless?\"\n\nSuppose someone responded by saying \"It seems like you're making the assumption that someone is spying on your communications all the time. But isn't this unrealistically pessimistic?\"\n\nThe cryptographer's response would be to say \"Sure, it's probably not usually the case that someone is spying on my packets when I send messages over the internet. But when I'm trying to solve the technical problem of ensuring private communication, it's quite convenient to assume a simple and pessimistic threat model. Either I'll find an approach that works in any scenario less pessimistic than the one I solved, or I'll learn that we actually need to ensure some other way that no-one's reading my packets.\"\n\nSimilarly, in the alignment case, sometimes we make pessimistic empirical assumptions when trying to specify settings for our problems, because solutions developed for pessimistic assumptions generalize to easier situations but the converse isn't true.\n\nAs a large-scale example, when we talk about trying to come up with [competitive](https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4#25b0) solutions to AI alignment, a lot of the motivation isn't the belief that there will be literally no useful global coordination around AI.\n\nA smaller-scale example: When trying to develop schemes for [relaxed adversarial training](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d), we assume that we have no access to any interpretability tools for our models. This isn't because we actually believe that we'll have no interpretability tools, it's because we're trying to develop an alternative to relying on interpretability.\n\nThis is kind of similar to the attitude that cryptographers have.\n\n### Aiming your efforts at worlds where you have the biggest marginal impact\n\nSuppose you are unsure how hard the alignment problem is. Maybe you think that humanity's odd's of success are given by [a logistic function](https://en.wikipedia.org/wiki/Logistic_function) of the difference between how much alignment progress was made and how hard the problem is. When you're considering between a project that gives us a boost in worlds where P(doom) was 50% and projects that help out in worlds where P(doom) was 1% or 99%, you should probably pick the first project, because the derivative of P(doom) with respect to alignment progress is maximized at 50%.\n\nMany prominent alignment researchers estimate P(doom) as substantially less than 50%. Those people often focus on scenarios which are surprisingly bad from their perspective basically for this reason.\n\nAnd conversely, people who think P(doom) > 50% should aim their efforts at worlds that are better than they expected. This is the point that Eliezer makes in [Security Mindset and the Logistic Success Curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/): the security-minded character thinks that it's so unlikely that a particular security-lax project will succeed at building a secure system that she doesn't think it's worth her time to try to help them make marginal improvements to their security.\n\nAnd so, this kind of thinking only pushes you to aim your efforts at surprisingly bad worlds if you're already P(doom) < 50%.\n\nThis type of thinking is common among people who are thinking about global catastrophic biological risks. I don't know of any public documents that are specifically about this point, but you can see an example of this kind of reasoning in Andrew Snyder-Beattie's [Peak defence vs trough defence in biosecurity](https://forum.effectivealtruism.org/posts/w4LRTGCJFFQn6mYKS/peak-defense-vs-trough-defense-in-biosecurity).\n\n### Murphyjitsu\n\nSometimes a problem involves a bunch of weird things that could go wrong, and in order to get good outcomes, it has to be the case that all of them go well. For example, I don't think that \"a terrorist infiltrates the team of labellers who are being used to train the AGI and poisons the data\" is a very likely AI doom scenario. But I think there are probably 100 scenarios as plausible as that one, each of which sounds kind of bad. And I think it's probably worth some people's time to try to stamp out all these individually unlikely failure modes.\n\n### Planning fallacy\n\nRyan Greenblatt notes that you can also make a general reference class claim that people are too optimistic (planning fallacy etc.).\n\nDifferences between these arguments\n-----------------------------------\n\nDepending on which of these arguments you're making, you should respond very differently when someone says \"the thing you're proposing is quite far fetched\".\n\n* If the situation involves being optimized against, you say \"I agree that that action would be quite a weird action among actions. But there's a powerful optimization process selecting for actions like that action. So I expect it to happen anyway. To persuade me otherwise, you need to either claim that there isn't adversarial selection, or that bad actions either don't exist or are so hard to find that an adversary won't possibly be able to find them.\"\n* If you think that the situation involves a random process selecting over a space that is almost all bad, then you should say \"Actually I disagree, I think that in fact the situation we're talking about is probably about as bad as I'm saying; we should argue about what the distribution actually looks like.\"\n* If you are making worst-case assumptions as part of your problem-solving process, then you should say \"I agree that this situation seems sort of surprisingly bad. But I think we should try to solve it anyway, because solving it gives us a solution that is likely to work no matter what the empirical situation turns out to be, and I haven't yet been convinced that my pessimistic assumptions make my problem impossible.\"\n* If you're making worst-case assumptions because you think that P(doom) is low and you are focusing on scenarios you agree are worse than expected, you should say \"I agree that this situation seems sort of surprisingly bad. But I want to work on the situations where I can make the biggest difference, and I think that these surprisingly bad situations are the highest-leverage ones to work on.\"\n* If you're engaging in Murphyjistu, you should say \"Yeah this probably won't come up, but it still seems like a good idea to try and crush all these low-probability mechanisms by which something bad might happen.\"\n\nMary Phuong proposes breaking this down into two questions:\n\n* When should you believe things will go badly, because they in fact will go badly? (you're being optimized against, or the probability of badness is high for some other reason)\n* When should you focus your efforts on worlds where things go badly? I.e. it's about which parts of the distribution you intervene on, rather than an argument about what the distribution looks like."
            }
          ],
          "optional": true,
          "learningOutcomeId": null,
          "learningOutcomeName": null,
          "contentId": "2e4f6a8b-0c2d-4e6f-8a0b-2c4d6e8f0a2b",
          "videoId": null
        }
      ]
    }
  ],
  "courses": [
    {
      "slug": "default",
      "title": "AI Safety Course",
      "progression": [
        {
          "type": "module",
          "slug": "introduction",
          "optional": false
        },
        {
          "type": "meeting",
          "number": 1
        },
        {
          "type": "module",
          "slug": "feedback-loops",
          "optional": false
        },
        {
          "type": "module",
          "slug": "coming-soon",
          "optional": false
        },
        {
          "type": "meeting",
          "number": 2
        },
        {
          "type": "module",
          "slug": "coming-soon",
          "optional": false
        },
        {
          "type": "module",
          "slug": "coming-soon",
          "optional": false
        },
        {
          "type": "meeting",
          "number": 3
        },
        {
          "type": "module",
          "slug": "coming-soon",
          "optional": false
        },
        {
          "type": "meeting",
          "number": 4
        }
      ]
    }
  ],
  "errors": [],
  "urlsToValidate": [
    {
      "url": "https://aisafety.info/questions/8IHO/What-are-the-differences-between-a-singularity,-an-intelligence-explosion,-and-a-hard-takeoff",
      "file": "articles/aisafety-info-singularity-intelligence-explosion-hard-takeoff.md",
      "line": 2,
      "label": "source_url"
    },
    {
      "url": "https://www.lesswrong.com/posts/yTvBSFrXhZfL8vr5a/worst-case-thinking-in-ai-alignment",
      "file": "articles/buck-worst-case-thinking-in-ai-alignment.md",
      "line": 2,
      "label": "source_url"
    },
    {
      "url": "https://www.lesswrong.com/posts/dq3KsCsqNotWc8nAK/cascades-cycles-insight",
      "file": "articles/cascades-cycles-insight.md",
      "line": 2,
      "label": "source_url"
    },
    {
      "url": "https://flyingpenguin.com/wp-content/uploads/2022/04/good-1964-.pdf",
      "file": "articles/good-speculations-concerning-first-ultraintelligent-machine.md",
      "line": 2,
      "label": "source_url"
    },
    {
      "url": "https://intelligence.org/2015/07/24/four-background-claims/",
      "file": "articles/nate-soares-four-background-claims.md",
      "line": 2,
      "label": "source_url"
    },
    {
      "url": "https://www.lesswrong.com/posts/rJLviHqJMTy8WQkow/recursion-magic",
      "file": "articles/recursion-magic.md",
      "line": 2,
      "label": "source_url"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence",
      "file": "articles/wikipedia-existential-risk-from-ai.md",
      "line": 2,
      "label": "source_url"
    },
    {
      "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/AI_Time_Torizons_Are_Doubling_Every_4_Months.png/1280px-AI_Time_Torizons_Are_Doubling_Every_4_Months.png",
      "file": "articles/wikipedia-existential-risk-from-ai.md",
      "line": 10,
      "label": "Image URL"
    },
    {
      "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/X-risk-chart-en-01a.svg/500px-X-risk-chart-en-01a.svg.png",
      "file": "articles/wikipedia-existential-risk-from-ai.md",
      "line": 157,
      "label": "Image URL"
    },
    {
      "url": "https://www.youtube.com/watch?v=fa8k8IQ1_X0",
      "file": "video_transcripts/kurzgesagt-ai-humanitys-final-invention.md",
      "line": 2,
      "label": "url"
    },
    {
      "url": "https://www.youtube.com/watch?v=9i1WlcCudpU",
      "file": "video_transcripts/robertmiles-10-reasons-to-ignore-ai-safety.md",
      "line": 2,
      "label": "url"
    }
  ]
}
