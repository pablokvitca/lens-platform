---
id: 01f6df31-099f-48ed-adef-773cc4f947e4
---
### Article: Existential Risk from AI
source:: [[../articles/wikipedia-existential-risk-from-ai]]

#### Text
content::
We continue with an overview of the main concepts regarding AI as a source of existential threat: what capabilities of this technology are considered first and foremost, and why the task of eliminating AI risks differs from similar tasks for other impressive technologies created by humanity.

#### Article-excerpt
to:: "the "evasion of human control"."

#### Text
content::
In your own words, what is instrumental convergence?
#### Chat: Discussion on X-Risk
instructions::
TLDR of what the user just read:
AI x-risk is the hypothesis that AGI/superintelligence could cause human extinction
or irreversible collapse. The risk combines: capability advantages, recursive
self-improvement, and emerging dangerous capabilities (manipulation, cyberattacks,
bioweapons)—amplified by competitive "race to the bottom" dynamics. The core crux
is alignment: specifying goals, ensuring corrigibility, handling instrumental
convergence. Intelligence and values are orthogonal—moral behavior isn't automatic.

Discussion topics to explore:
- What is "instrumental convergence"? Why would any smart AI seek self-preservation and resources?
- What is the "Gorilla Problem" (Stuart Russell's analogy)?
- How do "Monkey's Paw" or "King Midas" effects apply to goal specification?
- What do skeptics like Yann LeCun argue, and what are the counter-arguments?
- Why might "kill switches" fail against superintelligence?

Ask what they found surprising or new. Check if they can explain instrumental
convergence in their own words—it's a key concept.

The user has just answered the following question: "In your own words, what is instrumental convergence?"