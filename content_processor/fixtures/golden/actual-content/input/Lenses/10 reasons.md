---
id: 3f1fc921-560f-4d47-8eb6-3c4dab872b9d
---
### Video: 10 Reasons to Ignore AI Safety
source:: [[../video_transcripts/robertmiles-10-reasons-to-ignore-ai-safety]]

#### Video-excerpt
from:: 0:00
to:: 15:37


#### Text
content::
Which objection stood out most to you?
#### Chat: Discussion on Objections
instructions::
TLDR of what the user just watched:
The video presents 10 common objections to AI safety concerns and refutes each one. Key rebuttals include: instrumental convergence explains why "just don't add bad goals" fails; implicit goals make systems LESS safe; the "asteroid analogy" shows why early preparation matters; being "for AI safety" is not the same as being "against AI."

Discussion topics to explore:
- Which objection did they find most initially convincing? Did their view change?
- How does instrumental convergence counter "just don't put in bad goals"?
- Why do implicit goals make systems less safe, not more?
- What's the flaw in "human-AI teams will keep things safe"?
- How does the video respond to the "overpopulation on Mars" analogy?

This is a good stage to surface any remaining skepticism they have. Engage with their doubts constructively rather than dismissing them.