---
title: "...Recursion, Magic â€” LessWrong"
author: Eliezer Yudkowsky
date: 2008-11-25
source_url: https://www.lesswrong.com/posts/rJLviHqJMTy8WQkow/recursion-magic
---

**Followup to**: [Cascades, Cycles, Insight...](https://www.lesswrong.com/lw/w5/cascades_cycles_insight)

_...4, 5 sources of discontinuity._

## Recursion

Recursion is probably the most difficult part of this topic. We have historical records aplenty of _cascades,_ even if untangling the causality is difficult. _Cycles_ of reinvestment are the heartbeat of the modern economy. An _insight_ that makes a hard problem easy, is something that I hope you've experienced at least once in your life...

But we don't have a whole lot of experience redesigning our own neural circuitry.

We have these wonderful things called "optimizing compilers". A compiler translates programs in a high-level language, into machine code (though these days it's often a virtual machine). An "optimizing compiler", obviously, is one that improves the program as it goes.

So why not write an optimizing compiler _in its own language_, and then _run it on itself?_ And then use the resulting _optimized optimizing compiler,_ to recompile itself yet _again,_ thus producing an _even more optimized optimizing compiler -_

Halt! Stop! Hold on just a minute! An optimizing compiler is not supposed to change the logic of a program - the input/output relations. An optimizing compiler is only supposed to produce code that does _the same thing, only faster._ A compiler isn't remotely near understanding what the program is _doing_ and why, so it can't presume to construct _a better input/output function_. We just presume that the programmer wants a fixed input/output function computed as fast as possible, using as little memory as possible.

So if you run an optimizing compiler on its own source code, and then use the product to do the same again, it should produce the _same output_ on both occasions - at most, the first-order product will run _faster_ than the original compiler.

If we want a computer program that experiences _cascades_ of self-improvement, the path of the optimizing compiler does not lead there - the "improvements" that the optimizing compiler makes upon itself, do not _improve its ability to improve itself_.

Now if you are one of those annoying nitpicky types, like me, you will notice a flaw in this logic: suppose you built an optimizing compiler that searched over a sufficiently wide range of possible optimizations, that it did not ordinarily have _time_ to do a full search of its own space - so that, when the optimizing compiler ran out of time, it would just implement whatever speedups it had already discovered. Then the optimized optimizing compiler, although it would only implement the same logic faster, would do more optimizations in the same time - and so the second output would not equal the first output.

Well... that probably doesn't buy you much. Let's say the optimized program is 20% faster, that is, it gets 20% more done in the same time. Then, unrealistically assuming "optimization" is linear, the 2-optimized program will be 24% faster, the 3-optimized program will be 24.8% faster, and so on until we top out at a 25% improvement. [_k_< 1](https://www.lesswrong.com/lw/w5/cascades_cycles_insight).

So let us turn aside from optimizing compilers, and consider a more interesting artifact, EURISKO.

To the best of my inexhaustive knowledge, EURISKO may _still_ be the most sophisticated self-improving AI ever built - in the 1980s, by Douglas Lenat before he started wasting his life on Cyc. EURISKO was applied in domains ranging from the [Traveller war game](http://web.archive.org/web/20100123135422/http://www.aliciapatterson.org/APF0704/Johnson/Johnson.html) (EURISKO became champion without having ever before fought a human) to VLSI circuit design.

EURISKO used "heuristics" to, for example, design potential space fleets. It also had _heuristics for suggesting new heuristics_, and metaheuristics could apply to any heuristic, including metaheuristics. E.g. EURISKO started with the heuristic "investigate extreme cases" but moved on to "investigate cases close to extremes". The heuristics were written in RLL, which stands for Representation Language Language. According to Lenat, it was figuring out how to represent the heuristics in such fashion that they could usefully modify themselves without always just breaking, that consumed most of the conceptual effort in creating EURISKO.

But EURISKO did not go foom.

EURISKO could modify even the metaheuristics that modified heuristics. EURISKO was, in an important sense, more recursive than either humans or natural selection - a new thing under the Sun, a cycle more closed than anything that had ever existed in this universe.

Still, EURISKO ran out of steam. Its self-improvements did not spark a sufficient number of new self-improvements. This should not really be too surprising - it's not as if EURISKO started out with human-level intelligence _plus_ the ability to modify itself - its self-modifications were either [evolutionarily blind](https://www.lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway), or produced by the simple procedural rules of some heuristic or other. That's not going to navigate the search space very fast on an atomic level. Lenat did not stand dutifully apart from his creation, but stepped in and helped EURISKO prune its own heuristics. But in the end EURISKO ran out of steam, and Lenat couldn't push it any further.

EURISKO lacked what I called "insight" - that is, the type of abstract knowledge that lets humans fly through the search space. And so its recursive _access_ to its own heuristics proved to be for nought.

Unless, y'know, you're counting becoming world champion at Traveller without ever previously playing a human, as some sort of accomplishment.

But it is, thankfully, a little harder than that to destroy the world - as Lenat's experimental test informed us.

Robin previously asked why [Douglas Engelbart did not take over the world](http://www.overcomingbias.com/2008/11/engelbarts-uber.html), despite his vision of a team building tools to improve tools, and his anticipation of tools like computer mice and hypertext.

One reply would be, "Sure, a computer gives you a 10% advantage in doing various sorts of problems, some of which include computers - but there's still a lot of work that the computer _doesn't_ help you with - and the mouse doesn't run off and write better mice entirely on its own - so _k_< 1, and it still takes large amounts of human labor to advance computer technology as a whole - plus a lot of the interesting knowledge is nonexcludable so it's hard to capture the value you create - and that's why Buffett could manifest a better take-over-the-world-with-sustained-higher-interest-rates than Engelbart."

But imagine that Engelbart had built a computer mouse, and discovered that each click of the mouse raised his IQ by one point. Then, perhaps, we would have had a _situation_ on our hands.

Maybe you could diagram it something like this:

1. Metacognitive level: [Evolution](https://www.lesswrong.com/lw/kr/an_alien_god) is the metacognitive algorithm which produced the wiring patterns and low-level developmental rules for human brains.
2. Cognitive level: The brain processes its knowledge (including procedural knowledge) using algorithms that quite mysterious to the user within them. Trying to program AIs with the sort of instructions humans give each other usually proves not to do anything: [the machinery activated by the levers is missing](https://www.lesswrong.com/lw/sp/detached_lever_fallacy).
3. Metaknowledge level: Knowledge and skills associated with e.g. "science" as an activity to carry out using your brain - instructing you _when_ to try to think of new hypotheses using your mysterious creative abilities.
4. Knowledge level: Knowing how gravity works, or how much weight steel can support.
5. Object level: Specific actual problems, like building a bridge or something.

This is a _causal_ tree, and changes at levels _closer to root_ have greater impacts as the effects cascade downward.

So one way of looking at it is: "A computer mouse isn't recursive enough."

This is an issue that I need to address at further length, but for today I'm out of time.

## Magic

Magic is the final factor I'd like to point out, at least for now, in considering sources of discontinuity for self-improving minds. By "magic" I naturally do not refer to [this](https://www.lesswrong.com/lw/tv/excluding_the_supernatural). Rather, "magic" in the sense that if you asked 19th-century Victorians what they thought the future would bring, they would have talked about flying machines or gigantic engines, and a very few true visionaries would have suggested space travel or Babbage computers. Nanotechnology, not so much.

The future has a reputation for accomplishing feats which the past thought impossible. Future civilizations have even broken what past civilizations thought (incorrectly, of course) to be the laws of physics. If prophets of 1900 AD - never mind 1000 AD - had tried to bound the powers of human civilization a billion years later, some of those impossibilities would have been accomplished before the century was out; transmuting lead into gold, for example. Because we remember future civilizations surprising past civilizations, it has become cliche that we can't put limits on our great-grandchildren.

And yet everyone in the 20th century, in the 19th century, and in the 11th century, was human. There is also the sort of magic that a human gun is to a wolf, or the sort of magic that human genetic engineering is to natural selection.

To "improve your own capabilities" is an instrumental goal, and if a smarter intelligence than my own is focused on that goal, [I should expect to be surprised](https://www.lesswrong.com/lw/v7/expected_creative_surprises). The mind may find ways to produce _larger jumps_ in capability than I can visualize myself. Where higher creativity than mine is at work and looking for shorter shortcuts, the discontinuities that _I_ imagine may be dwarfed by the discontinuities that _it_ can imagine.

And remember how _little_ progress it takes - just a hundred years of human time, with everyone still human - to turn things that would once have been "unimaginable" into heated debates about feasibility. So if you build a mind smarter than you, and it thinks about how to go FOOM quickly, and it goes FOOM _faster than you imagined possible,_ you really have no right to complain - based on the history of mere human history, you should have expected a significant probability of being surprised. Not, surprised that the nanotech is 50% faster than you thought it would be. Surprised the way the Victorians would have been surprised by nanotech.

Thus the last item on my (current, somewhat ad-hoc) list of reasons to expect discontinuity: Cascades, cycles, insight, recursion, magic.
