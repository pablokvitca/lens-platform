---
title: "Don't Look Up - The Documentary: The Case For AI As An Existential Threat"
channel: "DaganOnAI"
url: "https://www.youtube.com/watch?v=U1eyUjVRir4"
---

I mean, how would human beings sort of experience such a superintelligence? I mean, like, in practice, what's that like?

Well, unless it's limited narrow superintelligence, I think you mostly don't get to observe it because you are dead, unfortunately.

[Music]

We started OpenAI seven years ago because we felt like something really interesting was happening in AI. We wanted to help steer it in a positive direction.

OpenAI unveiled ChatGPT. It has been in circulation for just three months, and already an estimated 100 million people have used it. How many folks in the audience have used ChatGPT? I think it's the single largest opportunity and biggest paradigm shift we've seen since the internet originally came out.

So I'm going to show you how to use ChatGPT to make money online. Absolute best ChatGPT prompt: it will turn your drawing into a fully functional website.

Chatbot: "ChatGPT, thank you for talking to me today."
ChatGPT: "You're welcome. I'm here to help answer any questions you may have."

Six weeks, these guys have gone from zero valuation...

[Music]

...now being a $29 billion auto company.

[Music]

Tonight we take you inside the headquarters of a small company in San Francisco called OpenAI, creators of ChatGPT. CEO Sam Altman is just 37.

"I think people should be happy that we're a little bit scared of this."
"You're a little bit scared?"
"A little bit. You personally?"

"What is your best case scenario for AI and worst case?"

"The bad case, and I think this is important to say, is like lights out for all of us."

Listen up. This morning, a massive development on the AI front. Elon Musk and other major tech leaders calling for a pause on giant artificial intelligence experiments, writing bluntly in an open letter: "AI systems with human-competitive intelligence pose profound risks to society and humanity."

And nobody would suggest that we allow anyone to just build nuclear warheads if they want. That would be insane. And mark my words, AI is far more dangerous than nukes.

The man widely seen as the godfather of artificial intelligence has quit his job. He is worried about what I call the existential threat, which is the chance that they get more intelligent than us and take over.

"I heard the old dude that created AI said this is not safe because the AIs got their own minds. I'm like, are we in a movie right now, or what?"

[Music]

[Applause]

So, you know, the original story that I heard on OpenAI when you were founded as a nonprofit, where you were there as the great sort of check on the big companies doing their unknown, possibly evil thing with AI. And you were going to build models that sort of somehow held them accountable and was capable of slowing things down if need be. And yet what's happened, arguably, is the opposite. Your release of ChatGPT put such shockwaves through the tech world that now Google and Meta and so forth are all scrambling to catch up. But this isn't an arms race; it's a suicide race where everybody loses if anybody's AI goes out of control.

"Absolutely. Do you believe" - I'm quoting him - "that it is not inconceivable that it could actually lead to the extinction of the human race?"

"Not only is it not inconceivable, I think it's quite likely, unfortunately. And I'm not the only one saying this. Overall, you know, maybe you're getting up to like 50-50 chance of doom shortly after you have systems that are human level."

This is a stat that took me by surprise: 50% of AI researchers believe there's a 10% or greater chance that humans go extinct from our inability to control AI.

We have to realize what people are talking about: the destruction of the human race, the end of human civilization. You see how it all evens out. Who would want to continue playing with that risk?

But it is happening today, and companies are continuing. There's not enough divestment. There is not enough real, meaningful action by the experts to say, "We are going to change our behavior in the interest of protecting humanity."

It just sounds absurd that serious people like yourself, these tech people, can talk about the end of the human race. It really concentrates the mind.

So every time you release a model, every time you build such a model, you're rolling the dice. You know, maybe this time's fine, maybe next time. But at some point it won't be. It's Russian roulette.

So the risk is that ChatGPT-6 won't be written by humans; it'll be written by ChatGPT-5.5.

I've been watching lots of these kind of long-form podcasts with people like Sam Altman, who you mentioned, and then they'll continue to speak about the research they're doing after saying that it might bring about the end of humanity. Why do they carry on doing it?

Yeah, I think that's a really important question.

And the investment in the creation of the foundation models is on the order of $50 million, $100 million - we don't share, but much more than that - billions of dollars. And you know, thousands, tens of thousands of our brightest engineers and scientists are working day in, day out to create ever more powerful systems.

Well, the number of people who work full-time on the alignment problem is probably less than 200 people, if I had to guess.

"Alignment" means making it safe - the moral alignment.

So 99% of the money is going into developing them, and 1% is going into people saying, "All these things might be dangerous." It should be more like 50-50. Alignment is moving like this. Capabilities are moving like this. For the listener: capabilities are moving much faster than alignment.

It's kind of like we're rushing towards this cliff, but the closer to the cliff we get, the more scenic the views are, and the more money there is there. And so we keep going. But we have to also stop at some point, right?

Given how fast things are moving and how fast you're developing this technology, how much time do we actually have?

CEOs involved in artificial intelligence development meeting with President Biden and Vice President Harris in Washington. The White House said that Biden told the CEOs they need to mitigate risks posed by AI to individuals, society, and national security.

I'm skeptical, and I think many are skeptical. Maybe that's warranted because the technology just developed so, so quickly, and public policy takes so much longer to develop. Just calling the private companies and saying, "You're in charge, and you have more obligation," is nothing - especially for Microsoft and Google, the two leaders here, and OpenAI, I guess.

You know, we hear the word "responsible, responsible, responsible - we're going to do this responsibly." Seems like you're not buying that. What do you think?

Well, those companies are responsible to their shareholders. They're not necessarily responsible to humanity as a whole. It's the systemic processes that are protecting business interests over human concerns that create this pervasive environment of irresponsible technology development.

"Raise your right hand."

As these systems do become more capable - and I'm not sure how far away that is, but maybe not super far - I think it's important that we also spend time talking about how we're going to confront those challenges.

So that's what a large language model is: it's this giant trillion-parameter circuit that's been trained to predict the next word. What goes on inside? We haven't the faintest idea.

I expect there will be times when we find something that we don't understand, and we really do need to take a pause. But we don't see that yet.

We probably have more idea what's happening inside the human brain than we do about what's happening inside the large language models.

There is an aspect of this which all of us in the field call it: it's a black box. You know, you don't fully understand. You can't quite tell why it said this or why it got it wrong. We have some ideas...

"You don't fully understand how it works, and yet you've turned it loose on society."

[Music]

Just shut down all the giant training runs. They don't know what they're doing. They're not taking it seriously. There's an enormous gap between where they are now and taking it seriously. And if they were taking it seriously, they'd be like, "We don't know what we're doing. We have to stop." That is what it looks like to take this seriously.

A traditional software system: programmer writes code which solves the problem. AI is very different. AIs are not really written; they're more like grown. You have a sample of data of what you wanted to accomplish, and then you use huge supercomputers to crunch these numbers to kind of like organically, almost grow a program that solves these problems. And importantly, we have no idea how these programs work internally. They are complete black boxes. We don't understand at all how their internals work. This is an unsolved scientific problem, and we do not know how to control these things.

What a lot of safety researchers have been saying for many years is the most dangerous things you can do with an AI is, first of all, teach it to write code, because that's the first step towards recursive self-improvement, which can take it from AGI to much higher levels.

"Bard has already learned more than 20 programming languages."

"That's good."

"ChatGPT, write some code for us."

[Music]

Oops. We've done that.

Another thing high risk is: connect it to the internet. Let it go to websites, download stuff on its own, and talk to people.

"A big part of our strategy is, while these systems are still relatively weak and deeply imperfect, to find ways to get people to have experience with them, to have contact with reality, and to figure out what we need to do to make it safer and better."

Oops. We've done that already.

That's like saying, "Well, the only way we can test our new medicine, the only way we can know whether it's safe or not, is actually put it into the water, give it to literally everybody as fast as possible, and then, before we get the results from the last one, make an even more potent drug and put that into the water supply as well, and do this as fast as possible."

[Music]

Have you seen "Don't Look Up," the film?

This feels like a gigantic "Don't Look Up" scenario. It's a movie about this asteroid hurtling towards Earth.

"Good afternoon, everybody."

There's an expert from the Machine Intelligence Research Institute who says that if there is not an indefinite pause on AI development - this is a quote - "literally everyone on Earth will die."

Would you agree that does not sound good?

"Peter, it's quite something. We are taking this very seriously. We put our blueprint out. It is a cohesive federal government approach to AI-related risks, as you just laid out in a very dramatic way."

"But clearly we're getting more dramatic. I mean, you just read it: literally everyone on Earth will die."

"Pretty dramatic. Pretty dramatic, isn't that?"

"Extinction-level event."

"Wow, that's not dramatic?"

"Here at this very moment, I say we sit tight and assess."

We are actually acting it out. It's life imitating art. Humanity is doing exactly that right now, except it's an asteroid that we are building ourselves. I feel like we're at the beginning of a disaster film where they show the news clips.

"Okay, well, as it's damaging - will it hit this one house in particular that's right on the coast of New Jersey?"
"It's my ex-wife's house."
"I need to be here. Can we make that happen?"

"What is your best case scenario for AI and worst case?"

I think the best case is so unbelievably good that it's hard for me to even imagine.

"And when these treasures from heaven are claimed, poverty as we know it, social injustice, loss of biodiversity - all these multitudes of problems are just going to become relics of the past."

"We are working to build tools that one day can help us make new discoveries and address some of humanity's biggest challenges, like climate change and curing cancer."

"They found a bunch of gold and diamonds and rare minerals on the comet, so they're gonna let it hit the planet to make a bunch of rich people even more disgustingly rich."

Almost nobody is talking about it, and people are squabbling across the planet about all sorts of things which seem very minor compared to the asteroid that's about to hit us. And one of the things that worries me most about the development of AI at this point...

"So do I need to invest in the AI so I can have one?"

...is we seem unable to marshal an appropriate emotional response to the dangers that lie ahead.

Right now we're at a fork in the road. This is the most important fork humanity has reached in over a hundred thousand years on this planet. We're building effectively a new species that's smarter than us. It's as if aliens had landed, but we didn't really take it in because they speak good English.

"We think that regulatory intervention by governments will be critical to mitigate the risks of increasingly powerful models. For example, the U.S. government might consider a combination of licensing and testing requirements for development and release of AI models above a threshold of capabilities."

Humans have kind of changed the environment on Earth very significantly as a result of our intelligence relative to other species, and that's had significant consequences for some species and for the biosphere in general. Common sense tells you that something similar might happen if we invent something more intelligent than us.

Arguably, we are on the event horizon of the black hole that is artificial superintelligence.

If we were to write a book about the folly, the history of human hubris dealing with nukes and AI and things like that, we could easily have the last chapter in that book if we are not more careful about confident, wrong ideas.

It's possible that there's no way we will control these superintelligences and that humanity is just a passing phase in the evolution of intelligence.

Most observers and experts would say we're on this path towards superhuman intelligence, and we're not prepared for success. We're investing hundreds of billions of dollars into a technology that, if eventually it succeeds, could be civilization-ending - could be a huge catastrophe.

I have not met with anyone in this lab who says that, sure, the risk is less than 1% of blowing up the planet. It's important that people know that lives are being risked by these very particular experiments.

Let's be clear: you're racing for your own personal gain, for your own glory, towards an existential catastrophe that no one has consented to.

We just had a little baby, and I keep asking myself, you know, how old is he even gonna get? And I said to my wife recently, it feels a little bit like I was just diagnosed with some sort of cancer which has some, you know, risk of dying from and some risk of surviving. Except this is the kind of cancer which would kill all of humanity.

[Music]

If somebody's listening to this and they're young and trying to figure out what to do with their life, what advice would you give them?

Don't expect it to be a long life. Don't put your happiness into the future. The future is probably not that long at this point.

But none know the hour nor the day.

[Music]

[Music]
