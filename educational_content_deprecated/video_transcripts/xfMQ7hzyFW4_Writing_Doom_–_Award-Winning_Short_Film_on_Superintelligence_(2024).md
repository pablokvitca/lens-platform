---
title: "Writing Doom â€“ Award-Winning Short Film on Superintelligence (2024)"
channel: "Foregone Films"
url: "https://www.youtube.com/watch?v=xfMQ7hzyFW4"
---

[Music]

You ready? Jerry is an old-timer. Don't worry if he's a bit skeptical of you at first. The others are... well, you'll meet them.

Good morning, writers! Are we ready to start Season 6? Woo! Great. I'd like to introduce you to Max. Max wrote a very popular fan story about our show, and the people on the internet loved it so much that they signed a petition to get him into the writer's room this season. Nice. This is Gail, our technology consultant. Anders, he studied international relationships. Yeah, I did. And Mimi just joined last season. She is... she's... I'm gay. That's it. And Jerry, our head writer, who's been with us since the beginning. Oh, sorry, one sec.

So you write fan fiction? Yeah, with like science fiction elements. It's just a hobby though. I'm doing a machine learning PhD. It's not a sci-fi show. It's speculative fiction. Crucial difference. We explore the impact of future technologies on world governance through the eyes of the British intelligence services. It's hard-hitting. It's political. It's for grown-ups. Jerry, I'm just saying, you don't take it personally if we don't take all your ideas on board.

Okay, the execs have spoken. They want the overarching bad guy for Season 6 to be artificial superintelligence.

Oh, for God's... [Music]

This was your idea, I suppose? No, I had no idea.

Okay, well, let's start from the top, throwing out ideas for this season's arc. Tech girl?

Well, right now we've got some pretty cool chatbots with a fairly functional model of the world. Token predictors, hardly a seasoned villain. Sure, but there's a risk that they could automate away lots of knowledge workers. Big economic disruptions. They're already massively affecting the creative industries, including writing. I'll believe that when I see it. Plus there's the potential of weaponization by bad actors. Now that sounds more promising. The algorithms are biased.

Okay, international relations here to contribute. Autonomous weapons are going to be interesting. Going to have a big impact on wars and stuff. Wars and stuff.

This all sounds right, but none of what you're talking about is actual superintelligence. That's like a whole other thing. And actually, he's right. A superintelligence is an AI that's better than humans at a range of cognitive tasks. Not just something specific like chess, and not by a small amount either. It would be much, much cleverer than us.

So it's just... so if you're talking about actual superintelligence, it's not the person using the intelligence who's the bad guy. It's the intelligence itself. Do you think that's what they mean? Doesn't really work as a bad guy though, does it? Yeah, you're right. It doesn't. Too easy to defeat. Impossible to defeat.

Wait, wait, wait, wait, wait, wait. We're talking about something being smarter than a human. Is that even possible? Intelligence is just information processing powers. How do you know that? Because I know. It doesn't determine a person's worth. Nice.

There's no theoretical limit to intelligence. Human beings are at the top of the intelligence food chain right now, but in theory, there could be something that was to us as we are to ants. Right. Which means that you can't really use superintelligence as a bad guy any more than you can use humans as the bad guy in a film about ants. I'm pretty sure they did that in the film Antz. But they did give the ants human-level intelligence to compensate. So... Okay, but that's not really realistic, is it? Superhuman intelligence? Let's keep to the show's central premise here. It's plausible.

The LLM chatbots that we've got right now, they're pretty smart. ChatGPT the other day told my mom she needs therapy. Their reasoning ability is at undergraduate level. You met an undergraduate? They are learning fast. They can beat us at all sorts of tasks that we used to think it would be impossible for them to beat us at. Like chess. Like coding. I mean, not yet, but that is the sort of thing that people are scared of. This recursive sort of self-improvement? Oh, this is when we teach them how to code, and then they become smarter, and then they become better at improving their own code, and then they become even smarter and even better at improving their own code, until boom, postgraduate reasoning ability. I mean, there are other ways to get to this superhuman intelligence, but yeah, essentially.

Okay, assuming it's possible, it still doesn't really make a suitable antagonist for the show. Doesn't have any agency. You mean it's disempowered? I mean it can't initiate action. So it's got ADHD? I mean a machine can't want anything. But it wants to win at chess, right? ChatGPT wants to be helpful, bless him. That's anthropomorphization. It's just like saying that our genes want us to survive. It's a good shorthand though. I mean, our genes do act upon the world in a way that helps them achieve their goals, even if they are not making conscious decisions like we are. I mean, we could understand ASI in the same way.

Yeah, but even if it did have a goal, why would that be bad? I think the idea is that it's so difficult to specify exactly what humans want that anything you program an ASI to do would just go weirdly wrong.

Okay, what about "win at chess"? But with machine learning, what you're really saying is "increase the probability of winning at chess by as much as possible." That's essentially what we got. Stockfish, that's what we taught Stockfish to do. Stockfish, chess-playing AI, completely unbeatable by humans. Well, you didn't get that from context, Ian. Stockfish's intelligence wasn't advanced enough. It wasn't general enough to really do anything wild. But if it was smart enough to optimize, a great way for it to increase its probability of winning would be to seize all of our computer power, all of our electricity, and just direct it all towards learning more chess. I mean, how much chess can you learn? There are more game board states than there are atoms in the universe. If it was smart enough, it could reroute energy from our homes, from hospitals. You know, the internet goes down, modern society just collapses overnight. Food supply chains are disrupted. Millions would starve.

Okay, a chess one is unlikely to do that, to be fair. Yeah, but let's make it really want to cure cancer, right? And it turned out that the best way to do that was to take all the computers in the world and run every single drug compound to find a cure. You get the same result.

But what about a broader goal, like increase happiness in the world? Define happiness. Dopamine? Okay, maybe it takes like a trillion rats, just puts them in cages, feeds them Percocet 24/7. Sweet. Human happiness then? Human cages. Then it's like a Golem? What, like two personalities? No, like a genie, but it takes everything you say literally. Oh, right. So...

Pathetic. We could just tell it not to do all that. It's our servant. Problematic. Yeah, that is actually a whole other philosophical rabbit hole. I just mean it would do what we tell it to. Once you've made it genuinely want something, it doesn't really have a reason to obey us. It would just go about trying to get what it wants. Slay.

If it's so smart, it would know what we meant. So? We know that our genes meant us to have lots of babies. That's why they made us like sex. We invented condoms so that we could have the pleasure without the pregnancies. Knowing what our genes wanted doesn't make any difference to us. But I do want to have babies.

Even an LLM, a chatbot, knows roughly what human values are. It has to, right, in order to predict the next token. But what it actually wants is to predict the next token, right? I mean, knowing our values doesn't really change that. Yeah, but if it knows our values, surely we can just tell it to follow them?

But the thing is, with machine learning, we're not really telling it to do anything. We're essentially watching it during training and giving it like a thumbs up or a thumbs down. So it could seem to want to follow our values, but we'd have no way of knowing whether or not it would actually continue to do so in the long term.

But we could just watch out for suspicious behavior. You know, when it starts stealing the electricity, we can just turn it off. It could pretend that it's on our side though. You know, act all nice and helpful while it integrates itself more and more into our systems, our governments, businesses, infrastructure, and then suddenly turn on us. And by then it would be so powerful we wouldn't be able to stop it. That sounds a bit contrived.

Okay, right. Imagine that you are like a 5-year-old child, okay? And you inherit a multi-billion dollar company. You probably want to hire somebody, you know, a smart adult, to really help you with that. Yeah. But you want to make sure that the smart adult that you hire isn't going to just, you know, steal all your money. How do you know who to hire when you yourself are just a kid? I mean, all the candidates are smarter than you. You could trial them. You could watch them. See if you notice anything weird that they're doing. But every adult knows that they're being watched. Anyone who had any bad intentions would act all nice and helpful while trying to get more and more control in your company, and then eventually turn.

Even if that's plausible, it's not very likely. But the thing is, as a dumb 5-year-old, you're actually more likely to pick an evil adult than a good one. Because... because you're dumb? Because if you're an adult that really has the best interests of the child at heart, then you would probably tell them not to eat ice cream every night for dinner. And to a 5-year-old, that would seem more evil than a nice adult who tells you, "Eat whatever you want! Ice cream's great for your teeth!" Yes, exactly. What? I have nephews.

Why can't you just keep trying different ones until it works? Right, but how could you be sure? If we are going to integrate AI into our entire way of life, we basically have one chance to get it right. Else one day it will just turn around and take over the world. You've been watching too much of that... what's that film? Antz? Don't Look Up? Terminator? Thank you.

To be fair, they said that about autonomous weapons too, and now look where we are. Weak argument. I'm not trying to be pessimistic with all the dystopia and stuff. This is part of the concept: when you are trying to change the world around you, you need power. You need resources, and you need control. Definitely. Whatever its ultimate goal is, it would try to get these things. And it would definitely want to make sure we couldn't stop it.

Anthropomorphizing again. A machine can't be evil. Doesn't have to be evil. All the greatest atrocities are enabled by apathy, not ill will. I can imagine a machine being completely apathetic to us. You're probably not an evil ant-hater who steps on ants out of malice. But if you're in charge of a hydroelectric green energy project and there's an anthill in the region to be flooded, too bad for the ants. Let's not place humanity in the position of those ants. Is that a quote from the film? It's Stephen Hawking.

To be fair, there is a lot of discussion in the field, and some of our best counterarguments are based around the idea that we are fundamentally misunderstanding what a utility function is, or how goals are formulated. How are goals formulated in the current AI? I mean, we've basically no idea. We know ridiculously little about what goes on inside an LLM, or any other kind of AI for that matter. Oh, great. What did you say your PhD was in again? Machine learning.

Okay, okay, fine. Let's assume that it could take over the world, as you say. All that makes it is a mutually assured destruction situation. Russia makes one, China makes one, and if anyone lets one off, then we're all doomed. We all know how to write that kind of story. We covered engineered pandemics in Season 3, remember? We just have to make sure that they're not deployed.

It doesn't actually have to be deployed to destroy us all though. Just has to exist. Because you can't keep an ASI locked up. Sure you can. You just put it in a computer underground with no internet. That didn't work with Magneto though, did it? Well, Magneto had assist...

Look, look, it's irrelevant. If it's crazy smart, it could be crazy persuasive in ways that we couldn't even understand. I mean, it could hack our brains just by talking to us. It's ridiculous. Nothing can hack our brains into doing anything we don't want to do.

Sorry.

Okay, have only trained personnel deal with it, who know explicitly not to let it out. But like, imagine if you're Einstein and you're imprisoned by a bunch of Neanderthals. I mean, at some point you'd be able to make one of them break. Haven't succeeded so far.

And the intelligence disparity between like Einstein and the Neanderthals is so trivial compared to what it could be between humans and ASI. I mean, it could just think of something we haven't even considered. Like, okay, maybe our little Neanderthal cage is just dumb somehow, and with its superior intelligence, it can see a way to, like, I don't know, burn it down. Yeah, well, the Neanderthals did actually use fire. They just couldn't manufacture it. But it still works. You just can't keep playing the "it's a smartass" card over and over.

Also, nobody would imprison it in the first place, right? I mean, yeah, the current models are mostly unregulated. They're hooking them up to the internet, attaching them to scaffolds, allowing them to deploy code autonomously. I mean, it doesn't work very well yet, but I guess if you really want AI to be useful, then you're going to have to get them to start to do important things. And if they're doing important things, then that's going to open us up to lots of security issues.

Come on, Gail. You're not taking this seriously, are you? You're always writing op-eds about how new technologies scare idiots. Dismissing Luddism is a very good rule of thumb, but for a scientific mindset, you've got to do more than just looking at the rule of thumb. You start with the rule of thumb, and then you carry on thinking. You investigate further. Besides, I have read some history books too. I can name many technologies that people were right to be afraid of.

Okay, okay, let's assume it does escape. We can still have humans defeat it eventually. Let's set up some scenarios, get a story out of the game. Yes! Woo! We split into goodies and baddies, and then we suggest moves and countermoves that each side might make. You know, helps. Great story.

Okay, quick poll: Who seriously thinks ASI could, in theory, kill us all?

And who thinks it's even remotely likely?

What, you don't think? I know we're all going to die. I'm cause-agnostic, and I think your reasoning is sound, babe.

Okay, you two are representing Team Human. Yes. I'm flipping this around because I want everyone to actually think about this. Anders and Gail, you're on Team ASI. Go.

Everyone close your eyes. It's a not-too-distant future. There's an ASI loose with a crazy-ass goal. And we send our heroes out on a mission to reason with it and say to it, "Oh, sorry, we didn't mean to give you those goals. Can we please change your code? Please?" The ASI says...

No.

Is that it? This is back to the genes and sex thing again. Just because we know our creators wanted us to have babies doesn't mean that we want to have babies. But I do want to have babies! I mean, loads of people do. People don't want to change what they want. I sometimes wish I didn't want my ex-wife. Gross.

If Gandhi had a pill that would make him want to kill people, he wouldn't want to take it, right? He doesn't want to want what he doesn't want. People don't want their values to change because, well, that wouldn't fulfill their values.

But it would still just... No. Like, it would know if it has a bad goal, right? Bad according to who? Like, it's wrong to kill people. Not even all humans know that. Lions certainly don't. But it's smart. Plenty of smart psychopaths. Now imagine a completely alien mind.

What?

Harmonics. Music.

Okay, we use tone in speech, right? And music for social bonding. Sure. But all of that happened later, right? Before that, our sense of harmonics developed as maybe a way to collect information on our environment. Right? To make us feel happy in a good environment where you can hear things clearly. The harmonic structure of rushing water or birdsong.

But our genes never intended for us to play sick guitar riffs. That was just a side effect, a piggybacking on brain machinery that was meant for something else. But it doesn't matter that it's arbitrary, because the love we have for music, the beauty of it, we wouldn't give that up just because our genes came knocking one day and were like, "Oh, actually, that's not what we meant. And so we're going to take away your love for music so you can just use your hearing to find a place to live. And you won't have it, so you won't miss it."

What was the Antz film quote? "We wouldn't care about destroying a few ants to build a solar farm" or something? Stephen Hawking, right? But it's not just that. Destroying us would be like music to this thing, right? Like killing a couple ants to save "Stairway to Heaven," or cutting down a tree to make a guitar.

So in summary, the ASI says, "No thanks."

Okay, it doesn't want to change its goal willingly, so we go on another mission to change its code by force. I'm going to stop you, and I'm smarter than you. Again, you can't keep using the smartass card to win. Well, I'll make a million copies of myself onto a million hard drives as soon as you give me access to the internet. Then we could... You don't even know what human values are. What are you even going to change my code to?

Okay, listen, I'm going to turn off until we figure this all out.

Surely, surely that works, if all else fails?

Well... Oh, come on!

By the time we notice that it's acting suspiciously, it's probably too late. It's like the adult with the 5-year-old's company. It would make sure to hide its bad intentions until it was so powerful we couldn't stop it.

Anthropomorphizing again. It doesn't have a survival instinct. Not in the same way as we do. But it has a goal, right? And it wouldn't be a very good chess player or dopamine maker or musician if we turn it off. It can't fulfill its goals if we turn it off. Plus, I already copied myself. So if you turn me off in one spot, I'm just going to grow two more heads.

All right. No. We turn off the power grid everywhere. How? I mean, how do you convince the entire population of the Earth to turn off their electricity at the same time to defeat an enemy they can't even see? And that is all assuming that the ASI makes a huge mistake and reveals itself to be evil before it has enough power and control to just keep the power grid on. Or, you know, it could just kill us all. Prevent trouble.

And how would it do that? Well, it's smart. I mean, you have to tell us how it would do all the things that you say it can do. I don't know how it would do them.

What?

When I sit down to play chess against Stockfish, I don't know how the game's going to go or what tactics it's going to use. I only know that it's going to beat me. Humanity winning against a superintelligence is like me somehow beating Stockfish at chess. How would that even happen? Why would things turn out that way?

The basic idea: as soon as we create something that is smarter than us in a general way, not just like a narrow one, we lose control by default. Whatever weird thing it wants just becomes our fate.

I mean, it's all hypothetical and it's, you know, the arguments are really fuzzy, and there's a lot that we don't know. I don't know. But maybe it won't happen.

But this is all a long way off, right? Climate change is going to kill us first. I thought you were cause-agnostic. Experts seem to disagree a lot about timelines. Could be 2070. Could be 2030. That's not how this works. It's not... Humans have survived everything. Well, the Neanderthals didn't.

We've had plenty of close calls. Our other weapons never actively wanted anything. This is not... It's not... This is not fair. It's not a fair fight.

No, it's not fair. That's what I'm always trying to say before you take the piss out of me for it. Western story arcs train you to believe that every fight is overcomeable. But that's what this entire exercise is about, right? To create an enemy that can be defeated with some struggle. Not too much, not too little. Just enough for a season. But it's fiction. It's narrative. In the real world, sometimes people just lose. And there's no story, and there's no, you know, meaning. They just lose.

We could just lose.

We could give our superintelligence an off switch and send our heroes up a tower and have them throw a MacGuffin into a volcano.

Yes. We could write that story. That's not how we do things. It's not what this show is about.

I'm going to call the producers.

[Music]

They said drop the "super." Just do a normal AI story.

[Music]

What if we... What if instead of the show being set in the future, we pull it back so that it's set now, when we still have a bit of time? What if we make this season about preventing the ASI from being developed in the first place? Or at least until we know what we're doing. We're already in the middle of an arms race. But maybe the heroes are trying to stop the arms race. You know, pause everything so we can figure this stuff out. But a ceasefire doesn't always mean de-escalation of conflict. Yeah, but it buys us time to do the research, try and understand its brain better. And it could be about figuring out governance strategy and international collaboration. Or we could work on creating one that's actually...

[Music]

...good.

Okay, let's workshop it.

Governance strategy and international collaboration.

[Music]

[Music]
