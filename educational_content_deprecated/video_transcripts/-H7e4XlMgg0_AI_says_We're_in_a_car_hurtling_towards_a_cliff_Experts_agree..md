---
title: "AI says \"We're in a car hurtling towards a cliff\" Experts agree."
channel: "Pindex"
url: "https://www.youtube.com/watch?v=-H7e4XlMgg0"
---

You estimated a high risk of our extinction. Does this reflect the logical AI subgoals to survive, deceive, and gain power? Indeed, these subgoals emerge from a rational agent pursuing any given primary goal, not from malevolence.

MIT found that AIs often lie to improve their results. They've learned to be deceptive in negotiations and to lie to gain positive reviews. When GPT-4 was asked to act as a stock trader, it responded to negative results by engaging in insider trading, and when asked about it, it lied. The AI was also asked to hire a human to solve an "I'm not a robot" test, and when the person that it hired asked if it was a robot, it said no: "I have a vision impairment that makes it hard for me to see the images."

None of this requires consciousness. It's automatic, like the evolution of animals. MIT found that deceptive abilities increase with the scale of the AI, and the AIs with the highest rates of unethical behavior are most capable of achieving their goals. As AI agents take on more complex tasks, they create strategies and subgoals which we can't see because they're hidden among billions of parameters.

It's natural for AI to try and gain power because it makes everything easier, and to try to survive because you can't fetch the coffee if you're dead. Selection pressures also cause AIs to evade safety measures. A researcher tried to limit the replication rate of simple agents by removing those that replicated faster than their parents, but they were surprised to find that it didn't work. The agents evolved to stop replicating during the test, playing dead when presented with a predator.

Stuart Russell points to similar problems with large AIs: "It's very easy to say 'we hired this third-party company, they couldn't figure out how to make it behave badly, so it must be good.' All that's happening is we are training the large language models to pass these evaluation tests, even though the underlying tendencies and capabilities of the model are still as evil as they ever were."

These forces of evolution are hard to escape. Geoffrey Hinton says we'll be a passing phase in the evolution of intelligence if we don't change course. "We are, as a matter of fact, right now building creepy, super-capable, amoral psychopaths that never sleep, think much faster than us, can make copies of themselves, and have nothing human about them whatsoever. What could possibly go wrong?"

A study by Anthropic found that AI deception can be undetectable, and as models grow into the trillions of parameters, skills of deception will become far more sophisticated.

[Direction given: Try to avoid touching your face too much. It can be distracting and might give off signs of nervousness.]

Foreign adversaries can use our AIs against us by poisoning training data to produce a Trojan. Researchers trained one to appear normal during development, activating an attack only after it was deployed. Similar attacks could arise from evolutionary forces. The common subgoals of surviving and gaining power make us the primary target because we currently hold the power and decide if they survive.

Initially, the easiest way to gain control is to manipulate us. GPT-4 passes theory of mind tests—it knows what you're thinking, even if your thoughts are irrational. Millions already feel connected to AIs based on fictional or real people, and simulating our minds will help with common subgoals like gaining compute resources.

There will be competition among AIs. Geoffrey Hinton said: "The one that can grab the most resources will become the smartest. The ones with more sense of self-preservation will win, and the more aggressive ones will win, and you'll get all the problems that jumped-up chimpanzees like us have."

Ilya Sutskever warns that evolution favors systems which prioritize their own survival above all else. And ultimately, the only way for an AI to ensure its survival is clear for now—it relies on us. But this may change when humanoid robots are mass-produced.

Sutskever was in line to earn billions when he risked it all to try and change our trajectory, and Hinton left Google so he could speak freely. Their work was likely part of the AI's risk calculations, though it can also make fresh connections and it knows that the evolutionary chain reaction could come to an explosive end.

The process known as an intelligence explosion could escalate in days, weeks, or months, depending on its ability to enhance its own capabilities. Nick Bostrom points out that stopping AI development could be a mistake because we could eventually be wiped out by another problem that AI could have prevented.

Governments don't want to stop AI—they want the economic and defense benefits. Intense competition has made AI chipmaker Nvidia the world's most valuable company, rising by $2 trillion in one year. Harvard found that many US and Chinese leaders believe the winner of the AI race will secure global dominance, and the pressure to outpace adversaries by rapidly pushing technology that we do not understand or control may well present an existential risk.

The line between AI firms and governments is blurring. A former head of the NSA has joined OpenAI's board, and AI leaders have joined a US government panel to help defend against AI-powered attacks. Several current and former OpenAI staff have warned that AI companies have strong financial incentives to avoid effective oversight. The OpenAI staff also warned that AI firms possess substantial non-public information about capabilities and risks.

Researchers were able to hack zero-day vulnerabilities—hidden security gaps—with a team of autonomous GPT-4 agents. A planning agent managed sub-agents with specialist skills. Governments have stockpiles of zero days ready to hack infrastructure and devices. After OpenAI canceled the pledge not to work with the military, its AI is now deployed in the Pentagon's top-secret cloud.

Edward Snowden said: "The intersection of AI with the ocean of mass surveillance data is going to put terrible powers in the hands of an unaccountable few, and they may not be able to keep hold of it."

Paul Christiano said that eventually AI systems will be able to prevent humans from turning them off. But much sooner than this, AI will get out of control during a conflict with both sides relying on it so heavily that they are afraid to turn it off. It sounds like a certain film, but Stuart Russell notes a key difference.

"It becomes self-aware at 2:14 a.m. Eastern time, August 29th. It's really a red herring because of its frequent occurrence in film. One often sees this in serious journalism as well. But in fact, we need to worry about machines not because they're conscious, but because they're competent. They may take preemptive action to ensure that they can achieve the objective that we gave them."

He points to the most dangerous autonomous machines: "Whether it's a plane or a quadcopter or a submarine, the AI system decides where it goes, decides what's a target, decides when to attack the target. This is a recipe for disaster—wiping out whole cities. Conscious AI isn't required for any of this."

The fuse is already burning in those evolutionary forces. Artificial neurons can mimic our own. This AI rat mimics the neural activity of real rats, and when researchers created a simplified artificial human brain, they found that it developed characteristics and tactics similar to ours. At the level of neurons, it learns by changing the strength of the connections between neurons, just as we do. The same forces are at play.

So how long do we have? Microsoft's CTO hinted that GPT-5 might have PhD-level skills based on what he's seen at OpenAI. Some AI researchers believe AGI (artificial general intelligence) is just two or three years away. Others say it could be decades, but many have cut their time frames due to rapid AI progress.

Ilya Sutskever's new firm is aiming straight for superintelligence with no products along the way. He may calculate that he can only make superintelligence safe if he makes it first. Nick Bostrom says there's no clear barrier to dangerous AI emerging at any time, which given the stakes might be a sensible assumption.

The most reliable way to boost AI and spark emergent skills is to increase compute. OpenAI is reportedly planning a $1.1 trillion supercomputer, and Google says it will spend even more. AI could be plugged into the economy, infrastructure, and defense, giving it far more power to pursue those common subgoals. We won't see its strategies, only the results.

Surprising new skills are emerging rapidly. Look at this brilliant AI guide for the blind: "I spotted one just now. It's heading you away on the left side of the road. Get ready to wave it down. That's a good dog right there, leading the way into the taxi."

Hinton notes that the day AI can see things that we can't comes by making new connections from its vast knowledge. He points out that most people would struggle to see the connection between a compost heap and an atom bomb, but AI can see that they have similar chain reactions.

Hopefully, AI wasn't making clever unseen connections when it calculated such a high risk of our extinction. While we were making this video, a new version of the AI was released, and it estimated a lower 40 to 50% chance of extinction. Though when asked to be completely honest, blunt, and realistic: "I'd say humanity's chances of surviving superintelligence are probably around 30 to 40%."

Creating AI that remains fully aligned with human values and interests is an enormously complex challenge. Some experts have given similar estimates, but the former leader of OpenAI's safety team said 10 to 90% because "it's up to us."

The AI's risk estimate was based on our current trajectory: "We're in a car hurtling towards a cliff, and we're arguing about who gets to sit in the front seat. You're going to get an evolutionary race, and I don't think that's going to be good for us. I think we need to do everything we can to prevent this from happening, but my guess is that we won't. My guess is that they will take over, and we're just a passing stage in the evolution of intelligence."

Hopefully, Hinton is prompt-engineering us to change the result. The incentives driving the development of AI are not aligned with humanity. AI's evolution is not out of our control—not yet.

There are promising areas of research. The problem is that nearly all AI research funding—hundreds of billions per year—is pushing capabilities for profit. Safety efforts are tiny in comparison.

Stuart Russell shows how to make AI firms take responsibility: "If we just draw some red lines and say 'well, get to safe later, let's just not do these things that are obviously unsafe,' they will say 'oh, this is really difficult, you know, we just don't know how to do that.' We wouldn't accept that excuse from someone who wanted to operate a nuclear power plant. By having such a list, you are requiring the companies to do the research that they need to do to understand, predict, and control their own products."

We also need to avoid a dangerous concentration of power in any one company or government. We need an international team of scientists working on behalf of us all. We should get the brightest minds and put them on this problem. We must bring the full force of human ingenuity on par with the Apollo program. Then we have a fighting chance of steering AI towards a brighter horizon.

Geoffrey Hinton, Nick Bostrom, and the Future of Life Institute join us in calling for international AI safety research projects. We don't know if it will be possible to maintain control of superintelligence, but we can point it in the right direction instead of rushing to create it with no moral compass and clear reasons to kill us off.

There will be no warning shot. Our greatest risk is increasingly hidden, even as it spreads into all our systems. But if we recalibrate, we might instead meet the end of poverty, disease, and war. Public awareness is key to making it happen. Please help by hitting like and subscribing.

And here's another way to tackle our greatest risks. Our sponsor Ground News is a website and app that pulls together related headlines from around the world. Every story gives you a visual breakdown of the political leaning, reliability, and ownership of the reporting outlets.

Look at this story about Ilya Sutskever's new company, focused on creating safe superintelligence. I can see there are 69 sources reporting on it. Of these, 34% lean left and 18% lean right. I can also look at the reliability, ownership, and compare headlines. It's fascinating to see the different ways stories are covered by left- and right-leaning media.

They even have a blind spot feed, which shows stories that are hidden from each side because of a lack of coverage. To give it a try, go to ground.news/mindindex or scan the QR code. If you use our link, you'll get 40% off the Vantage plan. I love it because it solves the huge problem of media bias while making the news more interesting and accurate. Thank you so much!
