---
phase: 11-answer-feedback-chat
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - core/modules/feedback.py
  - core/__init__.py
  - core/tables.py
  - core/tests/test_feedback.py
  - web_api/routes/feedback.py
  - web_api/tests/test_feedback.py
  - main.py
autonomous: true

must_haves:
  truths:
    - "Backend can build a feedback-specific system prompt from question context, student answer, and mode"
    - "Backend can stream AI feedback responses via SSE and persist conversation using chat_sessions"
    - "Feedback chat sessions use content_type='feedback' with question-specific content_id derived via UUID5"
    - "GET /api/chat/feedback/history returns existing conversation for a question"
    - "POST /api/chat/feedback/archive archives a stale feedback session"
  artifacts:
    - path: "core/modules/feedback.py"
      provides: "Feedback prompt builder and streaming handler"
      exports: ["build_feedback_prompt", "send_feedback_message"]
    - path: "core/tests/test_feedback.py"
      provides: "Tests for build_feedback_prompt mode switching and prompt content"
      contains: "TestBuildFeedbackPrompt"
    - path: "web_api/routes/feedback.py"
      provides: "POST /api/chat/feedback, GET /api/chat/feedback/history, POST /api/chat/feedback/archive endpoints"
      exports: ["router"]
    - path: "web_api/tests/test_feedback.py"
      provides: "Tests for feedback endpoint contracts -- UUID5 derivation, SSE streaming, history retrieval, archival"
      contains: "TestFeedbackEndpoints"
    - path: "core/tables.py"
      provides: "Updated CHECK constraint allowing content_type='feedback'"
      contains: "feedback"
  key_links:
    - from: "web_api/routes/feedback.py"
      to: "core/modules/feedback.py"
      via: "send_feedback_message import"
      pattern: "from core\\.modules\\.feedback import"
    - from: "web_api/routes/feedback.py"
      to: "core/modules/chat_sessions.py"
      via: "get_or_create_chat_session with content_type='feedback'"
      pattern: "content_type.*feedback"
    - from: "core/modules/feedback.py"
      to: "core/modules/llm.py"
      via: "stream_chat for streaming feedback"
      pattern: "from core\\.modules\\.llm import.*stream_chat"
---

<objective>
Build the backend feedback module and API endpoints using test-driven development. Tests first for prompt builder mode logic, UUID5 content_id derivation, and endpoint request/response contracts, then implement to pass.

Purpose: TDD forces clean interfaces for the feedback prompt builder (pure function with clear I/O) and endpoint contracts (defined request shapes and response codes). These are the same kinds of testable behaviors that made Phase 9's scoring module robust. The feedback module follows the exact same patterns.
Output: Working, tested backend -- core/modules/feedback.py + web_api/routes/feedback.py + Alembic migration + passing test suites.
</objective>

<execution_context>
@/home/penguin/.claude/get-shit-done/workflows/execute-plan.md
@/home/penguin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-answer-feedback-chat/11-RESEARCH.md
@core/scoring.py
@core/tests/test_scoring.py
@core/modules/llm.py
@core/modules/chat_sessions.py
@core/tables.py
@web_api/routes/module.py
@web_api/tests/test_chat_module.py
@web_api/tests/test_chat_history.py
@web_api/tests/conftest.py
@main.py
</context>

<features>

<feature>
  <name>Feature 1: Feedback prompt builder (core/modules/feedback.py)</name>
  <files>core/modules/feedback.py, core/tests/test_feedback.py</files>
  <behavior>
build_feedback_prompt() is a pure function that returns a system prompt string.

Test cases (mirror test_scoring.py TestBuildScoringPrompt pattern):
- mode="socratic" -> prompt contains "supportive"/"tutor"/"Socratic", does NOT contain "assessor"
- mode="assessment" -> prompt contains "assessor"/"rubric"/"evaluate", does NOT contain "supportive tutor"
- learning_outcome_name="Understanding X" -> "Understanding X" appears in prompt
- learning_outcome_name=None -> "Learning Outcome:" does NOT appear in prompt
- assessment_prompt="Check for X" -> "Rubric" and "Check for X" appear in prompt, in correct order
- assessment_prompt=None -> "Rubric" does NOT appear in prompt
- answer_text and user_instruction always appear in the returned prompt
- Returns a string (not a tuple like scoring -- feedback is simpler, just the system prompt)
  </behavior>
  <implementation>
Follow the exact structure from the research file's Pattern 2. The function signature:

```python
def build_feedback_prompt(
    *,
    answer_text: str,
    user_instruction: str,
    assessment_prompt: str | None,
    learning_outcome_name: str | None,
    mode: str,
) -> str:
```

Socratic mode: supportive tutor, Socratic questions, encouraging.
Assessment mode: educational assessor, evaluate against rubric, concrete improvements.
Both modes: append question, optional LO, optional rubric, student answer.
  </implementation>
</feature>

<feature>
  <name>Feature 2: Feedback endpoint contracts (web_api/routes/feedback.py)</name>
  <files>web_api/routes/feedback.py, web_api/tests/test_feedback.py, core/tables.py, main.py</files>
  <behavior>
Three endpoints on the feedback router, tested following test_chat_module.py and test_chat_history.py patterns:

**POST /api/chat/feedback** (SSE streaming):
- Accepts JSON: {questionId: str, moduleSlug: str, answerText: str, message: str}
- Returns 200 with content-type text/event-stream
- Streams data: JSON events with type "text" and "done"
- Uses UUID5(NAMESPACE_URL, questionId) for deterministic content_id
- Saves user message and assistant response to chat_sessions
- Returns 401 when not authenticated
- Returns 404 for unknown module (when _resolve_question_details returns {})

**GET /api/chat/feedback/history?questionId=X**:
- Returns 200 with {sessionId: int, messages: [{role, content}]}
- Returns {sessionId: 0, messages: []} when no session exists
- Uses same UUID5(NAMESPACE_URL, questionId) content_id derivation
- Returns 401 when not authenticated

**POST /api/chat/feedback/archive**:
- Accepts JSON: {questionId: str}
- Returns 200 with {ok: true} always (best-effort archival)
- Archives the active feedback session for the user/question pair

**UUID5 content_id derivation** (tested directly):
- uuid5(NAMESPACE_URL, "test-module:0:0") produces a deterministic, reproducible UUID
- Same input always produces same output
- Different inputs produce different UUIDs

**DB constraint**: core/tables.py CHECK constraint updated to include 'feedback'.
  </behavior>
  <implementation>
Create web_api/routes/feedback.py following the exact SSE pattern from web_api/routes/module.py:

1. Router with prefix="/api/chat/feedback"
2. Pydantic models: FeedbackChatRequest(questionId, moduleSlug, answerText, message), FeedbackArchiveRequest(questionId)
3. POST endpoint: resolve question details via _resolve_question_details, compute content_id = uuid5(NAMESPACE_URL, questionId), get_or_create_chat_session with content_type="feedback", save user message, stream via send_feedback_message, save assistant response. Return StreamingResponse.
4. GET /history endpoint: compute content_id, look up session, return messages.
5. POST /archive endpoint: compute content_id, archive session if exists, return {ok: true}.

Register router in main.py. Update CHECK constraint in core/tables.py. Create Alembic migration for constraint change.

Also create send_feedback_message in core/modules/feedback.py (delegates to stream_chat with the built prompt).
Export from core/__init__.py.
  </implementation>
</feature>

</features>

<tdd_protocol>
## RED-GREEN-REFACTOR Cycle

### RED Phase: Write failing tests

**Step 1: Create core/tests/test_feedback.py** with TestBuildFeedbackPrompt class.
Follow the exact pattern from core/tests/test_scoring.py TestBuildScoringPrompt:

```python
"""Tests for AI feedback module (prompt building and mode switching)."""

from core.modules.feedback import build_feedback_prompt


class TestBuildFeedbackPrompt:
    """Tests for build_feedback_prompt -- pure function returning a system prompt string."""

    def test_socratic_mode_prompt_is_supportive(self):
        """mode='socratic' produces prompt with supportive/tutor/Socratic language."""
        prompt = build_feedback_prompt(
            answer_text="Some answer",
            user_instruction="Explain X",
            assessment_prompt=None,
            learning_outcome_name=None,
            mode="socratic",
        )
        assert any(word in prompt.lower() for word in ["supportive", "tutor", "socratic"])
        assert "assessor" not in prompt.lower()

    def test_assessment_mode_prompt_is_evaluative(self):
        """mode='assessment' produces prompt with assessor/evaluate/rubric language."""
        prompt = build_feedback_prompt(
            answer_text="Some answer",
            user_instruction="Explain X",
            assessment_prompt=None,
            learning_outcome_name=None,
            mode="assessment",
        )
        assert any(word in prompt.lower() for word in ["assessor", "evaluate"])
        assert "supportive tutor" not in prompt.lower()

    def test_includes_learning_outcome_when_provided(self):
        """learning_outcome_name='Understanding X' appears in prompt."""
        prompt = build_feedback_prompt(
            answer_text="Some answer",
            user_instruction="Explain X",
            assessment_prompt=None,
            learning_outcome_name="Understanding X",
            mode="socratic",
        )
        assert "Understanding X" in prompt

    def test_excludes_learning_outcome_when_none(self):
        """learning_outcome_name=None means no 'Learning Outcome:' in prompt."""
        prompt = build_feedback_prompt(
            answer_text="Some answer",
            user_instruction="Explain X",
            assessment_prompt=None,
            learning_outcome_name=None,
            mode="socratic",
        )
        assert "Learning Outcome:" not in prompt

    def test_includes_rubric_when_provided(self):
        """assessment_prompt='Check for X' appears in prompt after 'Rubric'."""
        prompt = build_feedback_prompt(
            answer_text="Some answer",
            user_instruction="Explain X",
            assessment_prompt="Check for X",
            learning_outcome_name=None,
            mode="assessment",
        )
        assert "Rubric" in prompt
        assert "Check for X" in prompt

    def test_excludes_rubric_when_none(self):
        """assessment_prompt=None means no 'Rubric' in prompt."""
        prompt = build_feedback_prompt(
            answer_text="Some answer",
            user_instruction="Explain X",
            assessment_prompt=None,
            learning_outcome_name=None,
            mode="socratic",
        )
        assert "Rubric" not in prompt

    def test_includes_answer_text_and_question(self):
        """Both answer_text and user_instruction appear in prompt."""
        prompt = build_feedback_prompt(
            answer_text="My detailed answer about alignment",
            user_instruction="Explain the alignment problem",
            assessment_prompt=None,
            learning_outcome_name=None,
            mode="socratic",
        )
        assert "My detailed answer about alignment" in prompt
        assert "Explain the alignment problem" in prompt

    def test_returns_string(self):
        """Return value is a string."""
        result = build_feedback_prompt(
            answer_text="Answer",
            user_instruction="Question",
            assessment_prompt=None,
            learning_outcome_name=None,
            mode="socratic",
        )
        assert isinstance(result, str)
        assert len(result) > 50  # Should be a substantial prompt
```

**Step 2: Create web_api/tests/test_feedback.py** with TestFeedbackEndpoints class.
Follow the exact patterns from test_chat_module.py and test_chat_history.py:

```python
"""Tests for feedback chat endpoints."""

import uuid
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from fastapi import HTTPException
from fastapi.testclient import TestClient

from main import app
from web_api.auth import get_user_or_anonymous


@pytest.fixture
def client():
    return TestClient(app)


@pytest.fixture
def mock_auth():
    app.dependency_overrides[get_user_or_anonymous] = lambda: (1, None)
    yield (1, None)
    app.dependency_overrides.clear()


@pytest.fixture
def mock_auth_anonymous():
    anon_token = uuid.uuid4()
    app.dependency_overrides[get_user_or_anonymous] = lambda: (None, anon_token)
    yield (None, anon_token)
    app.dependency_overrides.clear()


class TestUUID5ContentId:
    """Tests for deterministic UUID5 content_id derivation."""

    def test_uuid5_is_deterministic(self):
        """Same questionId always produces same UUID."""
        id1 = uuid.uuid5(uuid.NAMESPACE_URL, "test-module:0:0")
        id2 = uuid.uuid5(uuid.NAMESPACE_URL, "test-module:0:0")
        assert id1 == id2

    def test_uuid5_differs_for_different_questions(self):
        """Different questionIds produce different UUIDs."""
        id1 = uuid.uuid5(uuid.NAMESPACE_URL, "test-module:0:0")
        id2 = uuid.uuid5(uuid.NAMESPACE_URL, "test-module:0:1")
        assert id1 != id2


class TestPostFeedbackChat:
    """Tests for POST /api/chat/feedback."""

    def test_returns_sse_stream(self, client, mock_auth):
        """Should return 200 with SSE content type."""
        async def mock_stream(*args, **kwargs):
            yield {"type": "text", "content": "Here is feedback."}
            yield {"type": "done"}

        mock_conn = MagicMock()
        mock_conn.__aenter__ = AsyncMock(return_value=mock_conn)
        mock_conn.__aexit__ = AsyncMock(return_value=None)

        with (
            patch("web_api.routes.feedback.get_connection", return_value=mock_conn),
            patch("web_api.routes.feedback.get_or_create_chat_session", new_callable=AsyncMock, return_value={"session_id": 1, "messages": []}),
            patch("web_api.routes.feedback.add_chat_message", new_callable=AsyncMock),
            patch("web_api.routes.feedback._resolve_question_details", return_value={"user_instruction": "Explain X", "assessment_prompt": None, "learning_outcome_name": None, "mode": "socratic"}),
            patch("web_api.routes.feedback.send_feedback_message", side_effect=lambda *a, **kw: mock_stream()),
        ):
            response = client.post("/api/chat/feedback", json={
                "questionId": "test-module:0:0",
                "moduleSlug": "test-module",
                "answerText": "My answer",
                "message": "What did I miss?",
            })
            assert response.status_code == 200
            assert response.headers["content-type"] == "text/event-stream; charset=utf-8"

    def test_streams_text_and_done_events(self, client, mock_auth):
        """Should stream data: events with text content and done signal."""
        async def mock_stream(*args, **kwargs):
            yield {"type": "text", "content": "Good "}
            yield {"type": "text", "content": "answer!"}
            yield {"type": "done"}

        mock_conn = MagicMock()
        mock_conn.__aenter__ = AsyncMock(return_value=mock_conn)
        mock_conn.__aexit__ = AsyncMock(return_value=None)

        with (
            patch("web_api.routes.feedback.get_connection", return_value=mock_conn),
            patch("web_api.routes.feedback.get_or_create_chat_session", new_callable=AsyncMock, return_value={"session_id": 1, "messages": []}),
            patch("web_api.routes.feedback.add_chat_message", new_callable=AsyncMock),
            patch("web_api.routes.feedback._resolve_question_details", return_value={"user_instruction": "Q", "mode": "socratic"}),
            patch("web_api.routes.feedback.send_feedback_message", side_effect=lambda *a, **kw: mock_stream()),
        ):
            response = client.post("/api/chat/feedback", json={
                "questionId": "test-module:0:0",
                "moduleSlug": "test-module",
                "answerText": "My answer",
                "message": "",
            })
            lines = list(response.iter_lines())
            data_lines = [line for line in lines if line.startswith("data: ")]
            assert len(data_lines) >= 2

    def test_saves_user_and_assistant_messages(self, client, mock_auth):
        """Should save both user message and streamed assistant response."""
        async def mock_stream(*args, **kwargs):
            yield {"type": "text", "content": "Feedback here"}
            yield {"type": "done"}

        add_mock = AsyncMock()
        mock_conn = MagicMock()
        mock_conn.__aenter__ = AsyncMock(return_value=mock_conn)
        mock_conn.__aexit__ = AsyncMock(return_value=None)

        with (
            patch("web_api.routes.feedback.get_connection", return_value=mock_conn),
            patch("web_api.routes.feedback.get_or_create_chat_session", new_callable=AsyncMock, return_value={"session_id": 1, "messages": []}),
            patch("web_api.routes.feedback.add_chat_message", add_mock),
            patch("web_api.routes.feedback._resolve_question_details", return_value={"user_instruction": "Q", "mode": "socratic"}),
            patch("web_api.routes.feedback.send_feedback_message", side_effect=lambda *a, **kw: mock_stream()),
        ):
            response = client.post("/api/chat/feedback", json={
                "questionId": "test-module:0:0",
                "moduleSlug": "test-module",
                "answerText": "My answer",
                "message": "Tell me more",
            })
            list(response.iter_lines())  # consume to trigger generator

            calls = add_mock.call_args_list
            user_calls = [c for c in calls if c.kwargs.get("role") == "user"]
            assistant_calls = [c for c in calls if c.kwargs.get("role") == "assistant"]
            assert len(user_calls) >= 1
            assert user_calls[0].kwargs["content"] == "Tell me more"
            assert len(assistant_calls) >= 1
            assert assistant_calls[0].kwargs["content"] == "Feedback here"

    def test_returns_401_when_not_authenticated(self, client):
        """Should return 401 for unauthenticated requests."""
        async def raise_401():
            raise HTTPException(status_code=401, detail="Authentication required")

        app.dependency_overrides[get_user_or_anonymous] = raise_401
        try:
            response = client.post("/api/chat/feedback", json={
                "questionId": "test-module:0:0",
                "moduleSlug": "test-module",
                "answerText": "My answer",
                "message": "Hello",
            })
            assert response.status_code == 401
        finally:
            app.dependency_overrides.clear()


class TestGetFeedbackHistory:
    """Tests for GET /api/chat/feedback/history."""

    def test_returns_existing_conversation(self, client, mock_auth):
        """Should return session messages when history exists."""
        mock_conn = MagicMock()
        mock_conn.__aenter__ = AsyncMock(return_value=mock_conn)
        mock_conn.__aexit__ = AsyncMock(return_value=None)

        with (
            patch("web_api.routes.feedback.get_connection", return_value=mock_conn),
            patch("web_api.routes.feedback.get_or_create_chat_session", new_callable=AsyncMock, return_value={
                "session_id": 42,
                "messages": [
                    {"role": "assistant", "content": "Here is feedback"},
                    {"role": "user", "content": "Tell me more"},
                ],
            }),
        ):
            response = client.get("/api/chat/feedback/history?questionId=test-module:0:0")
            assert response.status_code == 200
            data = response.json()
            assert data["sessionId"] == 42
            assert len(data["messages"]) == 2

    def test_returns_empty_when_no_session(self, client, mock_auth):
        """Should return sessionId=0 and empty messages when no session exists."""
        mock_conn = MagicMock()
        mock_conn.__aenter__ = AsyncMock(return_value=mock_conn)
        mock_conn.__aexit__ = AsyncMock(return_value=None)

        with (
            patch("web_api.routes.feedback.get_connection", return_value=mock_conn),
            patch("web_api.routes.feedback.get_or_create_chat_session", new_callable=AsyncMock, return_value={"session_id": 0, "messages": []}),
        ):
            response = client.get("/api/chat/feedback/history?questionId=test-module:0:0")
            assert response.status_code == 200
            data = response.json()
            assert data["sessionId"] == 0
            assert data["messages"] == []

    def test_returns_401_when_not_authenticated(self, client):
        """Should return 401 for unauthenticated requests."""
        async def raise_401():
            raise HTTPException(status_code=401, detail="Authentication required")

        app.dependency_overrides[get_user_or_anonymous] = raise_401
        try:
            response = client.get("/api/chat/feedback/history?questionId=test-module:0:0")
            assert response.status_code == 401
        finally:
            app.dependency_overrides.clear()


class TestPostFeedbackArchive:
    """Tests for POST /api/chat/feedback/archive."""

    def test_returns_ok_when_session_exists(self, client, mock_auth):
        """Should return {ok: true} after archiving."""
        mock_conn = MagicMock()
        mock_conn.__aenter__ = AsyncMock(return_value=mock_conn)
        mock_conn.__aexit__ = AsyncMock(return_value=None)

        with (
            patch("web_api.routes.feedback.get_connection", return_value=mock_conn),
            patch("web_api.routes.feedback.archive_chat_session", new_callable=AsyncMock, return_value=True),
            patch("web_api.routes.feedback.find_active_feedback_session", new_callable=AsyncMock, return_value={"session_id": 1}),
        ):
            response = client.post("/api/chat/feedback/archive", json={"questionId": "test-module:0:0"})
            assert response.status_code == 200
            assert response.json() == {"ok": True}

    def test_returns_ok_when_no_session(self, client, mock_auth):
        """Should return {ok: true} even when no session to archive (best-effort)."""
        mock_conn = MagicMock()
        mock_conn.__aenter__ = AsyncMock(return_value=mock_conn)
        mock_conn.__aexit__ = AsyncMock(return_value=None)

        with (
            patch("web_api.routes.feedback.get_connection", return_value=mock_conn),
            patch("web_api.routes.feedback.find_active_feedback_session", new_callable=AsyncMock, return_value=None),
        ):
            response = client.post("/api/chat/feedback/archive", json={"questionId": "test-module:0:0"})
            assert response.status_code == 200
            assert response.json() == {"ok": True}
```

**Step 3: Run tests -- ALL MUST FAIL (RED).**
```bash
pytest core/tests/test_feedback.py web_api/tests/test_feedback.py -x -q
```
Tests fail because core/modules/feedback.py and web_api/routes/feedback.py do not exist yet.

Commit: `test(11-02): add failing tests for feedback prompt builder and endpoint contracts`

### GREEN Phase: Implement to pass

**Step 4: Implement core/modules/feedback.py** with build_feedback_prompt and send_feedback_message. Follow the code from the plan action section (research Pattern 2). Export from core/__init__.py.

**Step 5: Update core/tables.py** CHECK constraint to include 'feedback'. Create Alembic migration.

**Step 6: Implement web_api/routes/feedback.py** with all three endpoints. Register in main.py. Follow the SSE pattern from web_api/routes/module.py.

Note on archive endpoint: The tests mock `find_active_feedback_session` -- this is a small helper function in feedback.py that queries chat_sessions for the active session matching (user_id, content_id, content_type='feedback', archived_at IS NULL). If found, call archive_chat_session. If not found, return None. The endpoint always returns {ok: true}.

**Step 7: Run tests -- ALL MUST PASS (GREEN).**
```bash
pytest core/tests/test_feedback.py web_api/tests/test_feedback.py -x -q
```

Commit: `feat(11-02): implement feedback module and endpoints`

### REFACTOR Phase (if needed)

**Step 8: Review and clean up.** Run full test suite to confirm no regressions:
```bash
pytest core/tests/ web_api/tests/ -x -q
ruff check . && ruff format --check .
```

Commit (if changes made): `refactor(11-02): clean up feedback module`
</tdd_protocol>

<verification>
1. `pytest core/tests/test_feedback.py -v` -- all TestBuildFeedbackPrompt tests pass
2. `pytest web_api/tests/test_feedback.py -v` -- all endpoint contract tests pass
3. `pytest core/tests/ web_api/tests/ -x -q` -- no regressions
4. `ruff check . && ruff format --check .` -- Python linting passes
5. Alembic migration file exists with upgrade/downgrade for CHECK constraint
6. Feedback router registered in main.py
</verification>

<success_criteria>
- core/tests/test_feedback.py has 8+ tests covering prompt mode switching, optional fields, and return type
- web_api/tests/test_feedback.py has 10+ tests covering POST streaming, GET history, POST archive, auth, UUID5 derivation
- All tests pass
- core/modules/feedback.py exports build_feedback_prompt and send_feedback_message
- web_api/routes/feedback.py provides all three endpoints
- CHECK constraint updated, Alembic migration ready
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/11-answer-feedback-chat/11-02-SUMMARY.md`
</output>
