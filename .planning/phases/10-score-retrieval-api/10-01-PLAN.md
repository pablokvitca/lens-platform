---
phase: 10-score-retrieval-api
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - core/assessments.py
  - web_api/routes/assessments.py
  - web_api/tests/test_score_retrieval.py
autonomous: true

must_haves:
  truths:
    - "GET /api/assessments/scores?response_id=X returns score data (overall_score, reasoning, dimensions, key_observations) plus metadata (model_id, prompt_version, created_at)"
    - "Endpoint only returns scores for responses owned by the calling user (user_id or anonymous_token ownership check)"
    - "Returns 200 with empty scores list when response has no scores yet (async scoring may be in progress)"
    - "Returns 200 with empty scores list when response_id doesn't exist or doesn't belong to caller (no information leakage)"
  artifacts:
    - path: "core/assessments.py"
      provides: "get_scores_for_response() query function with ownership-checked JOIN"
      contains: "get_scores_for_response"
    - path: "web_api/routes/assessments.py"
      provides: "GET /scores endpoint, ScoreItem model, ScoreResponse model, _format_score_items helper"
      contains: "get_assessment_scores"
    - path: "web_api/tests/test_score_retrieval.py"
      provides: "Tests covering: scores returned correctly, empty list for no scores, JSONB field extraction with missing keys"
      min_lines: 40
  key_links:
    - from: "web_api/routes/assessments.py"
      to: "core/assessments.py"
      via: "import get_scores_for_response"
      pattern: "from core\\.assessments import.*get_scores_for_response"
    - from: "core/assessments.py"
      to: "core/tables.py"
      via: "JOIN assessment_scores to assessment_responses"
      pattern: "assessment_scores\\.c\\.response_id == assessment_responses\\.c\\.response_id"
---

<objective>
Add a GET endpoint to retrieve assessment scores for a given response_id, completing the CRUD layer for the assessment_scores table.

Purpose: Phases 6 and 9 built write paths (create responses, score them via AI). This plan adds the read path so scores can be queried back. Required for Phase 11 (answer feedback chat) and future analytics.
Output: Working GET /api/assessments/scores?response_id=X endpoint with TDD test coverage.
</objective>

<execution_context>
@/home/penguin/.claude/get-shit-done/workflows/execute-plan.md
@/home/penguin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-score-retrieval-api/10-RESEARCH.md
@core/assessments.py
@web_api/routes/assessments.py
@web_api/tests/test_assessments_scoring.py
@core/tables.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Write failing tests for score retrieval endpoint</name>
  <files>web_api/tests/test_score_retrieval.py</files>
  <action>
Create test file following the EXACT pattern from web_api/tests/test_assessments_scoring.py (same imports, same mock helpers, same fixture style).

Tests to write in a TestScoreRetrieval class:

1. `test_get_scores_returns_extracted_fields` -- Mock get_scores_for_response to return one score row with full score_data JSONB. Assert 200, assert response body has scores[0] with overall_score, reasoning, dimensions, key_observations, model_id, prompt_version, created_at all correctly extracted.

2. `test_get_scores_returns_empty_list_when_no_scores` -- Mock get_scores_for_response to return []. Assert 200, assert {"scores": []}.

3. `test_get_scores_handles_missing_jsonb_fields` -- Mock get_scores_for_response to return a row where score_data is {} (empty dict -- simulates incomplete/evolving schema). Assert 200, assert overall_score/reasoning/dimensions/key_observations are all None.

4. `test_get_scores_requires_response_id_param` -- Call GET /api/assessments/scores without ?response_id. Assert 422 (FastAPI validation error).

Mock constants:
- MOCK_USER = (UUID("00000000-0000-0000-0000-000000000001"), None) -- same as scoring tests
- MOCK_SCORE_ROW with score_id=1, response_id=42, score_data containing all 4 fields, model_id="gpt-4o-mini", prompt_version="v1", created_at="2026-01-01T00:00:00"
- MOCK_EMPTY_SCORE_ROW with score_data={} for the missing fields test

Mock targets:
- `web_api.routes.assessments.get_connection` -> mock_connection() (from same helper pattern)
- `web_api.routes.assessments.get_scores_for_response` -> AsyncMock

Run tests: `.venv/bin/pytest web_api/tests/test_score_retrieval.py -v`
Tests MUST FAIL (ImportError or AttributeError -- get_scores_for_response and ScoreResponse don't exist yet).

Commit: `test(10-01): add failing tests for score retrieval endpoint`
  </action>
  <verify>.venv/bin/pytest web_api/tests/test_score_retrieval.py -v 2>&1 | tail -20 -- should show ERRORS (import failures or AttributeError)</verify>
  <done>4 tests exist in test_score_retrieval.py and all fail with import/attribute errors (RED phase complete)</done>
</task>

<task type="auto">
  <name>Task 2: GREEN -- Implement score retrieval query, models, and endpoint</name>
  <files>core/assessments.py, web_api/routes/assessments.py</files>
  <action>
**core/assessments.py** -- Add get_scores_for_response function at the bottom of the file:

- Import assessment_scores from .tables (add to existing import or use local import like the research suggests)
- Function signature: async def get_scores_for_response(conn: AsyncConnection, *, response_id: int, user_id: int | None = None, anonymous_token: UUID | None = None) -> list[dict]
- Raises ValueError if neither user_id nor anonymous_token provided
- Builds ownership filter on assessment_responses (user_id OR anonymous_token, same pattern as update_response)
- SELECT from assessment_scores JOIN assessment_responses ON response_id, WHERE response_id matches AND ownership filter passes
- Order by created_at DESC
- Returns [dict(row._mapping) for row in result.fetchall()] -- same pattern as get_responses

**web_api/routes/assessments.py** -- Add to existing file:

1. Add get_scores_for_response to the import from core.assessments

2. Add Pydantic models after existing models (before --- Endpoints --- comment):
   - ScoreItem(BaseModel): score_id (int), response_id (int), overall_score (int | None = None), reasoning (str | None = None), dimensions (dict | None = None), key_observations (list[str] | None = None), model_id (str | None = None), prompt_version (str | None = None), created_at (str)
   - ScoreResponse(BaseModel): scores (list[ScoreItem])

3. Add _format_score_items helper (after existing _format_response_items):
   - Takes list[dict] rows, returns list[ScoreItem]
   - For each row: extract score_data JSONB with .get() defaults (score_data = row.get("score_data", {}) or {})
   - Extract overall_score, reasoning, dimensions, key_observations from score_data using .get() (returns None for missing keys)
   - Handle created_at datetime->isoformat conversion (same pattern as _format_response_items)

4. Add GET endpoint (BEFORE the GET /responses/{question_id} route to avoid path conflicts -- though /scores vs /responses/{x} won't conflict, keeping it ordered logically):
   - @router.get("/scores", response_model=ScoreResponse)
   - async def get_assessment_scores(response_id: int, auth: tuple = Depends(get_user_or_anonymous))
   - Unpack auth, call get_scores_for_response with get_connection, format with _format_score_items, return ScoreResponse

5. Update module docstring to include the new endpoint.

Run tests: `.venv/bin/pytest web_api/tests/test_score_retrieval.py -v`
All 4 tests MUST PASS.

Also run full test suite: `.venv/bin/pytest` to ensure no regressions.

Commit: `feat(10-01): implement score retrieval endpoint with ownership-checked JOIN`
  </action>
  <verify>.venv/bin/pytest web_api/tests/test_score_retrieval.py -v -- all 4 tests pass. .venv/bin/pytest -- full suite passes with 0 failures.</verify>
  <done>GET /api/assessments/scores?response_id=X returns score data for owned responses, empty list otherwise. All tests green, no regressions.</done>
</task>

</tasks>

<verification>
1. `.venv/bin/pytest web_api/tests/test_score_retrieval.py -v` -- all 4 tests pass
2. `.venv/bin/pytest` -- full test suite passes (no regressions)
3. `ruff check core/assessments.py web_api/routes/assessments.py web_api/tests/test_score_retrieval.py` -- no lint errors
4. `ruff format --check core/assessments.py web_api/routes/assessments.py web_api/tests/test_score_retrieval.py` -- formatting OK
5. Verify get_scores_for_response import exists in web_api/routes/assessments.py
6. Verify assessment_scores JOIN assessment_responses in core/assessments.py
</verification>

<success_criteria>
- GET /api/assessments/scores?response_id=42 returns {"scores": [{score_id, response_id, overall_score, reasoning, dimensions, key_observations, model_id, prompt_version, created_at}]}
- Ownership check enforced via JOIN (user can only read scores for their own responses)
- Empty scores (no scores yet or wrong response_id) returns {"scores": []} not 404
- JSONB fields extracted with .get() defaults (missing keys become None, not errors)
- 4 tests covering: happy path, empty results, missing JSONB fields, missing query param
- Full test suite green, lint clean
</success_criteria>

<output>
After completion, create `.planning/phases/10-score-retrieval-api/10-01-SUMMARY.md`
</output>
