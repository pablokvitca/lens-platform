---
phase: 07-answer-box
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - web_frontend/src/hooks/useVoiceRecording.ts
  - web_frontend/src/components/module/NarrativeChatSection.tsx
  - web_frontend/src/components/module/AnswerBox.tsx
autonomous: false

must_haves:
  truths:
    - "Student can click a microphone button on the answer box to record voice input"
    - "Voice recording shows volume bars, timer, and 60s warning — same UX as chat voice input"
    - "After recording stops, audio is transcribed and text appears in the textarea for review/editing"
    - "When enforceVoice is true, the mic button is visually prominent and primary"
    - "Chat voice input still works exactly as before (no regression from hook extraction)"
  artifacts:
    - path: "web_frontend/src/hooks/useVoiceRecording.ts"
      provides: "Reusable voice recording hook extracted from NarrativeChatSection"
      min_lines: 120
    - path: "web_frontend/src/components/module/NarrativeChatSection.tsx"
      provides: "Refactored chat section using useVoiceRecording hook instead of inline recording logic"
    - path: "web_frontend/src/components/module/AnswerBox.tsx"
      provides: "Answer box with mic button, recording UI, and enforceVoice support"
  key_links:
    - from: "web_frontend/src/hooks/useVoiceRecording.ts"
      to: "@/api/modules (transcribeAudio)"
      via: "import and call for audio transcription"
      pattern: "transcribeAudio"
    - from: "web_frontend/src/components/module/NarrativeChatSection.tsx"
      to: "web_frontend/src/hooks/useVoiceRecording.ts"
      via: "useVoiceRecording hook call"
      pattern: "useVoiceRecording"
    - from: "web_frontend/src/components/module/AnswerBox.tsx"
      to: "web_frontend/src/hooks/useVoiceRecording.ts"
      via: "useVoiceRecording hook call"
      pattern: "useVoiceRecording"
---

<objective>
Extract voice recording logic into a reusable hook, refactor NarrativeChatSection to use it, and add voice input to the AnswerBox component with enforceVoice support.

Purpose: Students need voice input for answer boxes (same UX as existing chat voice input). The recording logic (~200 lines) must be extracted into a shared hook rather than duplicated, and the existing chat must continue working.

Output: `useVoiceRecording` hook shared by both chat and answer box. AnswerBox gains a mic button with full recording UI. NarrativeChatSection refactored to use the hook (no behavior change). enforceVoice makes mic prominent.
</objective>

<execution_context>
@/home/penguin/.claude/get-shit-done/workflows/execute-plan.md
@/home/penguin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-answer-box/07-CONTEXT.md
@.planning/phases/07-answer-box/07-RESEARCH.md
@.planning/phases/07-answer-box/07-01-SUMMARY.md

Key source files to reference:
@web_frontend/src/components/module/NarrativeChatSection.tsx (lines 100-430: all recording state, refs, and functions to extract)
@web_frontend/src/components/module/AnswerBox.tsx (from Plan 07-01 — add voice input to this)
@web_frontend/src/api/modules.ts (transcribeAudio function)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract useVoiceRecording hook and refactor NarrativeChatSection</name>
  <files>
    web_frontend/src/hooks/useVoiceRecording.ts
    web_frontend/src/components/module/NarrativeChatSection.tsx
  </files>
  <action>
    **1. Create `web_frontend/src/hooks/useVoiceRecording.ts`:**

    Extract the voice recording logic from NarrativeChatSection.tsx (lines ~100-430) into a standalone hook.

    Interface:
    ```typescript
    interface UseVoiceRecordingOptions {
      onTranscription: (text: string) => void;  // Called with transcribed text
      onError?: (message: string) => void;       // Optional external error handler
      maxRecordingTime?: number;                  // Default 120 seconds
      warningTime?: number;                       // Default 60 seconds
      minRecordingTime?: number;                  // Default 0.5 seconds
    }

    interface UseVoiceRecordingReturn {
      recordingState: "idle" | "recording" | "transcribing";
      recordingTime: number;
      volumeBars: number[];
      errorMessage: string | null;
      showRecordingWarning: boolean;
      startRecording: () => Promise (async, resolves void);
      stopRecording: () => Promise (async, resolves void);
      handleMicClick: () => void;
      formatTime: (seconds: number) => string;
    }

    export function useVoiceRecording(options: UseVoiceRecordingOptions): UseVoiceRecordingReturn;
    ```

    What to extract (all from NarrativeChatSection.tsx):
    - **State:** `recordingState`, `recordingTime`, `volumeBars`, `errorMessage`, `showRecordingWarning` (lines 128-132)
    - **Refs:** `mediaRecorderRef`, `audioChunksRef`, `audioContextRef`, `sourceRef`, `analyserRef`, `timerIntervalRef`, `animationFrameRef`, `streamRef`, `isRecordingRef`, `recordingTimeRef`, `smoothedVolumeRef`, `pcmDataRef` (lines 135-146)
    - **Constants:** `MAX_RECORDING_TIME`, `WARNING_TIME`, `MIN_RECORDING_TIME` — make these configurable via options with defaults
    - **Cleanup effect:** The useEffect that cleans up on unmount (lines 193-208)
    - **Error auto-dismiss effect:** The useEffect that clears errors after 3s (lines 211-216)
    - **updateAudioLevel callback:** Volume meter logic (lines 238-278)
    - **startRecording function:** MediaRecorder setup, AudioContext, timer (lines 280-337)
    - **stopRecording function:** Stop recording, transcribe, call onTranscription (lines 339-394)
    - **cleanupRecording function:** Reset all refs and state (lines 396-417)
    - **handleMicClick function:** Toggle start/stop (lines 419-425)
    - **formatTime function:** Format seconds as mm:ss (lines 427-430)

    Key difference from NarrativeChatSection: Instead of directly calling `setInput()` with transcribed text, the hook calls `options.onTranscription(text)`. The consumer decides what to do with the text.

    The `transcribeAudio` import stays in the hook (import from `@/api/modules`).

    **2. Refactor `NarrativeChatSection.tsx` to use the hook:**

    Remove all the extracted state, refs, effects, and functions (lines ~100-430). Replace with:
    ```typescript
    import { useVoiceRecording } from "@/hooks/useVoiceRecording";

    // Inside the component:
    const {
      recordingState,
      recordingTime,
      volumeBars,
      errorMessage,
      showRecordingWarning,
      handleMicClick,
      formatTime,
    } = useVoiceRecording({
      onTranscription: (text) => {
        setInput((prev) => (prev ? `${prev} ${text}` : text));
      },
    });
    ```

    Keep the `RecordingState` type import from the hook (or define it there).

    The JSX in NarrativeChatSection that renders the recording UI (volume bars, timer, warning, mic button) stays in the component — only the logic is extracted.

    **Verify no regression:** The chat recording UI must look and behave identically after refactoring. Same volume bars, same timer, same warning at 60s, same max at 120s, same transcription flow.

    **Run checks:**
    - `cd web_frontend && npm run lint` must pass
    - `cd web_frontend && npm run build` must pass
  </action>
  <verify>
    - `cd web_frontend && npm run lint && npm run build` passes clean
    - `web_frontend/src/hooks/useVoiceRecording.ts` exists with 120+ lines
    - `grep -c "mediaRecorderRef\|audioChunksRef\|audioContextRef" web_frontend/src/components/module/NarrativeChatSection.tsx` returns 0 (all recording refs moved to hook)
    - `grep "useVoiceRecording" web_frontend/src/components/module/NarrativeChatSection.tsx` shows the hook is used
    - No recording state variables remain directly in NarrativeChatSection (all come from the hook)
  </verify>
  <done>
    useVoiceRecording hook encapsulates all MediaRecorder, AudioContext, volume analysis, timer, and transcription logic. NarrativeChatSection uses the hook and all recording behavior is unchanged. The hook is reusable by AnswerBox.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add voice input to AnswerBox with recording UI and enforceVoice support</name>
  <files>
    web_frontend/src/components/module/AnswerBox.tsx
  </files>
  <action>
    **1. Add useVoiceRecording to AnswerBox:**

    ```typescript
    import { useVoiceRecording } from "@/hooks/useVoiceRecording";

    // Inside AnswerBox component:
    const {
      recordingState,
      recordingTime,
      volumeBars,
      errorMessage,
      showRecordingWarning,
      handleMicClick,
      formatTime,
    } = useVoiceRecording({
      onTranscription: (text) => {
        // Append transcribed text to existing answer, then trigger save
        const newText = currentText ? `${currentText} ${text}` : text;
        setText(newText);
        setMetadata({ voice_used: true }); // from useAutoSave return
      },
    });
    ```

    **2. Add mic button to the AnswerBox UI:**

    Place the mic button to the right of the textarea (or in the footer area alongside the Finish button). The mic button should:
    - Show a microphone icon when idle (use an inline SVG — keep it simple, match the existing mic icon style from NarrativeChatSection)
    - Show a stop icon when recording
    - Show a loading/spinner state when transcribing
    - Be disabled when the answer is completed

    **3. Add recording UI overlay/inline:**

    When recording is active, show (inline below or overlaying the textarea area):
    - Volume bars (5 bars, same as chat) — animated height based on `volumeBars` array
    - Timer display — `formatTime(recordingTime)` in small text
    - Recording warning — "60s warning" text when `showRecordingWarning` is true
    - Error message — shown briefly when `errorMessage` is non-null

    Style the recording UI to match the chat recording experience but adapted for the answer box context (inline, not as a separate panel). Keep it minimal.

    **4. Implement enforceVoice support:**

    When `segment.enforceVoice` is true:
    - Make the mic button larger/more prominent (e.g., primary color, larger size)
    - Add a small label like "Voice input" or "Speak your answer" near the mic button
    - The textarea is still available (enforceVoice is not strictly enforced per context decisions — user can still type, but the voice option is prominent)

    **5. Track voice usage in answer_metadata:**

    When transcription completes, call `setMetadata({ voice_used: true })` from the useAutoSave hook (Plan 07-01 adds `setMetadata` to the hook's return interface). This merges `voice_used` into the metadata ref, which gets included in the next debounced save automatically. No additional plumbing needed — just call `setMetadata` inside the `onTranscription` callback alongside `setText`.

    **Run checks:**
    - `cd web_frontend && npm run lint` must pass
    - `cd web_frontend && npm run build` must pass
  </action>
  <verify>
    - `cd web_frontend && npm run lint && npm run build` passes clean
    - `grep "useVoiceRecording" web_frontend/src/components/module/AnswerBox.tsx` shows the hook is used
    - `grep "enforceVoice" web_frontend/src/components/module/AnswerBox.tsx` shows enforceVoice logic
    - `grep "handleMicClick" web_frontend/src/components/module/AnswerBox.tsx` shows mic button handler
    - `grep "volumeBars\|recordingState" web_frontend/src/components/module/AnswerBox.tsx` shows recording UI
  </verify>
  <done>
    AnswerBox has a mic button that triggers voice recording using the shared useVoiceRecording hook. Recording shows volume bars, timer, and warnings. Transcribed text appears in the textarea for review/editing before marking complete. When enforceVoice is true, the mic button is visually prominent. Voice usage is tracked in answer_metadata.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Full answer box with voice input, tested in context of module content:
    1. AnswerBox renders inline for question segments in module content
    2. Auto-expanding textarea with auto-save (saves after ~2.5s of inactivity)
    3. Finish button marks answer complete
    4. Mic button records voice, transcribes, puts text in textarea
    5. Volume bars and timer during recording
    6. Chat voice input still works (no regression)
  </what-built>
  <how-to-verify>
    Prerequisites: The database migration from 07-01 Task 1 must be run first. Ask the user to review and run it if not already done.

    To test, you need module content that contains a `#### Question` segment. If no existing module has one, you can temporarily add test content via the Lens Relay content, or test by inspecting the component in isolation.

    1. **Navigate to a module with a question segment** at `http://dev.vps:3300/course/{courseId}/module/{moduleId}`
    2. **Verify answer box renders**: Should see the question prompt text and a textarea below it
    3. **Type an answer**: Type several words — verify "Saving..." appears briefly, then "Saved"
    4. **Refresh the page**: Navigate away and back — verify the typed text is still there
    5. **Click Finish**: Verify the textarea becomes non-editable and shows "Completed"
    6. **Test voice input**: Click the mic button, speak, verify volume bars appear, stop recording, verify transcribed text appears in textarea
    7. **Test chat voice** (regression check): Navigate to a section with chat, click mic button, verify voice recording still works exactly as before
    8. **Check character count**: If the question has maxChars set, verify the counter appears

    If no module has question segments yet, verify:
    - The AnswerBox component exists and the build passes
    - The PATCH endpoint works via curl:
      ```bash
      curl -X POST http://localhost:8300/api/assessments/responses \
        -H "Content-Type: application/json" \
        -H "X-Anonymous-Token: test-token-123" \
        -d '{"question_id":"test:0:0","module_slug":"test","answer_text":"hello"}'

      curl -X PATCH http://localhost:8300/api/assessments/responses/1 \
        -H "Content-Type: application/json" \
        -H "X-Anonymous-Token: test-token-123" \
        -d '{"answer_text":"updated answer"}'
      ```
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:
1. `cd web_frontend && npm run lint && npm run build` passes (TypeScript + build)
2. `ruff check . && ruff format --check .` passes (Python — from Plan 01 changes)
3. `pytest` passes
4. useVoiceRecording hook exists and is used by both NarrativeChatSection and AnswerBox
5. NarrativeChatSection has no inline recording logic (all via hook)
6. AnswerBox has mic button, recording UI, and enforceVoice support
7. Chat voice input works identically to before (no regression)
</verification>

<success_criteria>
- Voice recording works in the answer box — click mic, record, see volume bars, stop, see transcribed text
- Chat voice input is unchanged (regression-free refactor)
- enforceVoice makes the mic button visually prominent
- Voice usage is tracked in answer_metadata
- All code passes lint and build checks
</success_criteria>

<output>
After completion, create `.planning/phases/07-answer-box/07-02-SUMMARY.md`
</output>
