[
  {
    "text": "In the previous video,",
    "start": "0:01.02"
  },
  {
    "text": "we introduced the thought experiment of the outcome pump,",
    "start": "0:02.34"
  },
  {
    "text": "a device that lets you change",
    "start": "0:05.76"
  },
  {
    "text": "the probability of events at will.",
    "start": "0:07.11"
  },
  {
    "text": "In that thought experiment,",
    "start": "0:09.57"
  },
  {
    "text": "your aged mother is trapped in a burning building.",
    "start": "0:11.04"
  },
  {
    "text": "You wish for your mother to end up",
    "start": "0:14.04"
  },
  {
    "text": "as far as possible from the building,",
    "start": "0:15.54"
  },
  {
    "text": "and the outcome pump makes the building explode,",
    "start": "0:17.55"
  },
  {
    "text": "flinging your mother's body away from it.",
    "start": "0:20.58"
  },
  {
    "text": "That clearly wasn't what you wanted,",
    "start": "0:23.07"
  },
  {
    "text": "and no matter how many amendments you make to that wish,",
    "start": "0:25.59"
  },
  {
    "text": "it's really difficult to make it actually safe",
    "start": "0:28.83"
  },
  {
    "text": "unless you have a way of specifying",
    "start": "0:31.68"
  },
  {
    "text": "the entirety of your values.",
    "start": "0:33.27"
  },
  {
    "text": "In this video, we explore how similar failures",
    "start": "0:35.85"
  },
  {
    "text": "affect machine learning agents today.",
    "start": "0:38.46"
  },
  {
    "text": "You can think of such agents as less powerful outcome pumps,",
    "start": "0:40.98"
  },
  {
    "text": "or little genies.",
    "start": "0:44.19"
  },
  {
    "text": "They have goals and take actions in their environment",
    "start": "0:45.99"
  },
  {
    "text": "to further their goals.",
    "start": "0:48.84"
  },
  {
    "text": "The more capable these models are,",
    "start": "0:50.70"
  },
  {
    "text": "the more difficult it is to make them safe.",
    "start": "0:52.62"
  },
  {
    "text": "The way we specify their goals is always leaky in some way.",
    "start": "0:55.29"
  },
  {
    "text": "That is, we often can't perfectly describe",
    "start": "0:59.52"
  },
  {
    "text": "what we want them to do,",
    "start": "1:01.98"
  },
  {
    "text": "so we use proxies that deviate",
    "start": "1:03.42"
  },
  {
    "text": "from the intended objective in certain cases.",
    "start": "1:05.49"
  },
  {
    "text": "In the same way that",
    "start": "1:08.19"
  },
  {
    "text": "getting your mother out of the building",
    "start": "1:09.02"
  },
  {
    "text": "was only an imperfect proxy for actually saving your mother,",
    "start": "1:11.34"
  },
  {
    "text": "there are plenty of similar examples in ordinary life.",
    "start": "1:15.48"
  },
  {
    "text": "The goal of exams is to evaluate",
    "start": "1:18.48"
  },
  {
    "text": "a student's understanding of the subject,",
    "start": "1:20.61"
  },
  {
    "text": "but in practice, students can cheat,",
    "start": "1:22.89"
  },
  {
    "text": "cram,",
    "start": "1:25.19"
  },
  {
    "text": "and study just exactly what will be on the test",
    "start": "1:26.75"
  },
  {
    "text": "and nothing else.",
    "start": "1:29.25"
  },
  {
    "text": "So passing exams is a leaky proxy for actual knowledge.",
    "start": "1:30.54"
  },
  {
    "text": "If we paid a bot farm to boost view counts for this video,",
    "start": "1:34.92"
  },
  {
    "text": "that would generate plenty of views,",
    "start": "1:38.46"
  },
  {
    "text": "but it would be pointless.",
    "start": "1:40.44"
  },
  {
    "text": "View count is a leaky proxy for our real objective",
    "start": "1:42.21"
  },
  {
    "text": "of teaching important subjects to a wide audience.",
    "start": "1:45.78"
  },
  {
    "text": "In machine learning, depending on the context,",
    "start": "1:49.32"
  },
  {
    "text": "there are many names used for situations",
    "start": "1:51.75"
  },
  {
    "text": "in which this general phenomenon occurs,",
    "start": "1:53.79"
  },
  {
    "text": "such as specification gaming, reward misspecification,",
    "start": "1:56.25"
  },
  {
    "text": "reward hacking, outer misalignment,",
    "start": "2:00.93"
  },
  {
    "text": "and Goodhart's Law.",
    "start": "2:03.81"
  },
  {
    "text": "These terms are used to describe situations",
    "start": "2:05.67"
  },
  {
    "text": "in which a behavior satisfies",
    "start": "2:07.77"
  },
  {
    "text": "the literal specification of an objective",
    "start": "2:09.63"
  },
  {
    "text": "without leading to the outcome that was actually intended.",
    "start": "2:12.18"
  },
  {
    "text": "DeepMind, in an article from 2020,",
    "start": "2:16.11"
  },
  {
    "text": "describes some of these failures",
    "start": "2:18.60"
  },
  {
    "text": "in machine learning systems trained",
    "start": "2:19.98"
  },
  {
    "text": "using Reinforcement Learning.",
    "start": "2:21.51"
  },
  {
    "text": "In Reinforcement Learning, rather than a goal,",
    "start": "2:23.61"
  },
  {
    "text": "you specify a reward function,",
    "start": "2:26.40"
  },
  {
    "text": "which automatically gives feedback to the agent",
    "start": "2:28.62"
  },
  {
    "text": "after it performs an action.",
    "start": "2:31.08"
  },
  {
    "text": "Here's an example: researchers were trying to train an agent",
    "start": "2:33.37"
  },
  {
    "text": "to stack LEGO blocks in simulation.",
    "start": "2:36.24"
  },
  {
    "text": "The outcome they desired was for the agent",
    "start": "2:38.91"
  },
  {
    "text": "to place a red block on top of a blue one.",
    "start": "2:41.25"
  },
  {
    "text": "How would you design the reward function",
    "start": "2:44.04"
  },
  {
    "text": "to make the agent learn the task?",
    "start": "2:45.72"
  },
  {
    "text": "The researchers chose a reward function",
    "start": "2:48.06"
  },
  {
    "text": "that looked at the height of the bottom face",
    "start": "2:49.80"
  },
  {
    "text": "of the red block when the agent is not touching the block.",
    "start": "2:51.66"
  },
  {
    "text": "If the bottom face of the red block ends up",
    "start": "2:55.47"
  },
  {
    "text": "at the same height as the top face of the blue block,",
    "start": "2:57.90"
  },
  {
    "text": "then that means the red block",
    "start": "3:00.93"
  },
  {
    "text": "is on top of the blue one, right?",
    "start": "3:02.43"
  },
  {
    "text": "Well, yes, but not always.",
    "start": "3:04.71"
  },
  {
    "text": "Instead of learning to stack the blocks,",
    "start": "3:07.77"
  },
  {
    "text": "the agent learned to simply flip the red block over",
    "start": "3:10.11"
  },
  {
    "text": "so that the bottom face would find itself",
    "start": "3:13.62"
  },
  {
    "text": "at the height of one block.",
    "start": "3:15.48"
  },
  {
    "text": "You see how the reward specified by the researchers",
    "start": "3:17.52"
  },
  {
    "text": "was leaky with respect to the original goal?",
    "start": "3:19.98"
  },
  {
    "text": "It didn't account for the unintended ways",
    "start": "3:22.71"
  },
  {
    "text": "in which the agent could solve the problem,",
    "start": "3:24.60"
  },
  {
    "text": "in the same way that",
    "start": "3:26.97"
  },
  {
    "text": "\"put your mother as far as possible from the building\"",
    "start": "3:28.23"
  },
  {
    "text": "allowed unintended outcomes",
    "start": "3:30.84"
  },
  {
    "text": "in which your mother ends up dead.",
    "start": "3:32.31"
  },
  {
    "text": "So the LEGO-stacking agent example",
    "start": "3:34.50"
  },
  {
    "text": "can be thought of as a version",
    "start": "3:37.11"
  },
  {
    "text": "of the outcome pump thought experiment,",
    "start": "3:38.46"
  },
  {
    "text": "but much simpler to solve and with much lower stakes.",
    "start": "3:40.62"
  },
  {
    "text": "One proposed solution to the problem of specification gaming",
    "start": "3:44.10"
  },
  {
    "text": "is to simply provide reward using human feedback.",
    "start": "3:47.43"
  },
  {
    "text": "A human would know that flipping the red block over",
    "start": "3:50.64"
  },
  {
    "text": "doesn't count as putting it on top of the blue block,",
    "start": "3:53.01"
  },
  {
    "text": "so that strategy wouldn't get reward.",
    "start": "3:55.59"
  },
  {
    "text": "If humans are in the loop",
    "start": "3:58.26"
  },
  {
    "text": "and can tell the agent when it makes mistakes,",
    "start": "3:59.61"
  },
  {
    "text": "then it can't go wrong, right?",
    "start": "4:02.10"
  },
  {
    "text": "Well, consider this other case.",
    "start": "4:04.20"
  },
  {
    "text": "An agent trained using human feedback",
    "start": "4:06.75"
  },
  {
    "text": "to grasp a ball ended up doing this.",
    "start": "4:08.97"
  },
  {
    "text": "That looks okay, right?",
    "start": "4:12.63"
  },
  {
    "text": "But if you look closely,",
    "start": "4:14.19"
  },
  {
    "text": "you can see that the hand",
    "start": "4:15.96"
  },
  {
    "text": "isn't actually gripping the ball.",
    "start": "4:17.04"
  },
  {
    "text": "It's in front of the ball,",
    "start": "4:18.90"
  },
  {
    "text": "between the ball and the virtual camera.",
    "start": "4:20.49"
  },
  {
    "text": "The agent learned that by putting its hand there,",
    "start": "4:23.40"
  },
  {
    "text": "the humans evaluating it would reward it",
    "start": "4:25.92"
  },
  {
    "text": "as though it was grasping the ball.",
    "start": "4:28.02"
  },
  {
    "text": "It learned to fool the humans.",
    "start": "4:30.18"
  },
  {
    "text": "So again, human feedback doesn't always work",
    "start": "4:32.91"
  },
  {
    "text": "because it's only a proxy for the true objective.",
    "start": "4:36.00"
  },
  {
    "text": "We don't want the agent to make the human evaluator think",
    "start": "4:39.09"
  },
  {
    "text": "it's grasping the ball\u2014we",
    "start": "4:42.21"
  },
  {
    "text": "want the agent to actually grasp the ball.",
    "start": "4:43.86"
  },
  {
    "text": "If fooling the evaluator is simpler to learn",
    "start": "4:46.77"
  },
  {
    "text": "than actually performing the task,",
    "start": "4:49.23"
  },
  {
    "text": "the agent will fool the evaluator.",
    "start": "4:51.54"
  },
  {
    "text": "Human feedback is also only a leaky proxy",
    "start": "4:53.79"
  },
  {
    "text": "for our true objectives.",
    "start": "4:56.43"
  },
  {
    "text": "You can see a bunch more examples",
    "start": "4:58.56"
  },
  {
    "text": "of this phenomenon in the video",
    "start": "4:59.97"
  },
  {
    "text": "\"9 Examples of Specification Gaming\" by Robert Miles.",
    "start": "5:01.72"
  },
  {
    "text": "That's me, by the way, the narrator.",
    "start": "5:05.79"
  },
  {
    "text": "I just picked nine particularly interesting examples",
    "start": "5:08.28"
  },
  {
    "text": "for that video, but this effect happens all the time.",
    "start": "5:10.68"
  },
  {
    "text": "Making machine learning agents pursue",
    "start": "5:14.28"
  },
  {
    "text": "our intended objectives can be really tricky,",
    "start": "5:16.08"
  },
  {
    "text": "and as machine learning improves, the stakes become higher.",
    "start": "5:19.44"
  },
  {
    "text": "It's not dangerous if a Reinforcement Learning model learns",
    "start": "5:23.37"
  },
  {
    "text": "to flip a block instead of stacking it in a simulation.",
    "start": "5:26.28"
  },
  {
    "text": "But what about models that affect the real world,",
    "start": "5:29.49"
  },
  {
    "text": "such as when they're used in medical settings,",
    "start": "5:32.04"
  },
  {
    "text": "recommender systems, or acting in the stock market?",
    "start": "5:34.47"
  },
  {
    "text": "Or what if they're at least as capable",
    "start": "5:37.53"
  },
  {
    "text": "and generally intelligent as humans,",
    "start": "5:39.27"
  },
  {
    "text": "with freedom to act on the internet",
    "start": "5:41.28"
  },
  {
    "text": "or in the real world more broadly?",
    "start": "5:42.90"
  },
  {
    "text": "If the capability is high enough",
    "start": "5:45.24"
  },
  {
    "text": "and if the core objective of those systems",
    "start": "5:46.92"
  },
  {
    "text": "is even slightly misaligned,",
    "start": "5:48.72"
  },
  {
    "text": "then their behavior may prove dangerous or even deadly",
    "start": "5:50.76"
  },
  {
    "text": "for individuals or the entirety of human civilization.",
    "start": "5:54.54"
  },
  {
    "text": "Think about the outcome pump again.",
    "start": "5:59.76"
  },
  {
    "text": "In a context in which the stakes are high,",
    "start": "6:02.10"
  },
  {
    "text": "a misspecification of the objective",
    "start": "6:03.81"
  },
  {
    "text": "can produce tragic outcomes.",
    "start": "6:06.42"
  },
  {
    "text": "In the following videos about this topic,",
    "start": "6:09.18"
  },
  {
    "text": "we'll explore more speculative scenarios",
    "start": "6:11.16"
  },
  {
    "text": "in which task misspecification may prove dangerous",
    "start": "6:13.83"
  },
  {
    "text": "for the entirety of human civilization.",
    "start": "6:16.53"
  },
  {
    "text": "We'll examine some ways in which many AI systems",
    "start": "6:18.93"
  },
  {
    "text": "could collectively contribute to civilizational collapse",
    "start": "6:21.69"
  },
  {
    "text": "if they're aimed at misspecified goals,",
    "start": "6:24.27"
  },
  {
    "text": "or how a single system",
    "start": "6:26.55"
  },
  {
    "text": "may bring about the end of human civilization.",
    "start": "6:28.11"
  },
  {
    "text": "So stay tuned.",
    "start": "6:30.78"
  },
  {
    "text": "In the meantime, if you'd like to skill up on AI Safety,",
    "start": "6:32.94"
  },
  {
    "text": "we highly recommend the free AI Safety Fundamentals courses",
    "start": "6:35.91"
  },
  {
    "text": "by BlueDot Impact at aisafetyfundamentals.com.",
    "start": "6:39.42"
  },
  {
    "text": "You can find three courses: AI Alignment, AI Governance,",
    "start": "6:43.62"
  },
  {
    "text": "and AI Alignment 201.",
    "start": "6:48.21"
  },
  {
    "text": "You can follow the AI Alignment and AI Governance courses",
    "start": "6:51.00"
  },
  {
    "text": "even without a technical background in AI.",
    "start": "6:53.64"
  },
  {
    "text": "The AI Alignment 201 course",
    "start": "6:56.16"
  },
  {
    "text": "assumes you've completed the AI Alignment course first",
    "start": "6:58.20"
  },
  {
    "text": "and also university-level courses on deep learning",
    "start": "7:01.53"
  },
  {
    "text": "and Reinforcement Learning or equivalent understanding.",
    "start": "7:04.44"
  },
  {
    "text": "The courses consist of a very well thought-out selection",
    "start": "7:07.98"
  },
  {
    "text": "of course materials you can find online.",
    "start": "7:10.62"
  },
  {
    "text": "They're available to everyone, so you can simply read them",
    "start": "7:13.14"
  },
  {
    "text": "without formally enrolling in the courses.",
    "start": "7:15.75"
  },
  {
    "text": "If you want to enroll,",
    "start": "7:18.30"
  },
  {
    "text": "BlueDot Impact accepts applications on a rolling basis.",
    "start": "7:19.56"
  },
  {
    "text": "The courses are remote and free of charge.",
    "start": "7:22.80"
  },
  {
    "text": "They consist of a few hours of effort per week",
    "start": "7:25.11"
  },
  {
    "text": "to go through the readings,",
    "start": "7:27.30"
  },
  {
    "text": "plus a weekly call with a facilitator",
    "start": "7:28.77"
  },
  {
    "text": "and a group of people learning from the same material.",
    "start": "7:31.05"
  },
  {
    "text": "At the end of each course,",
    "start": "7:33.84"
  },
  {
    "text": "you can complete a personal project,",
    "start": "7:35.25"
  },
  {
    "text": "which may help you kickstart your career in AI Safety.",
    "start": "7:37.32"
  },
  {
    "text": "BlueDot Impact receives many more applications",
    "start": "7:40.59"
  },
  {
    "text": "than they can accept. So if you'd still like",
    "start": "7:42.87"
  },
  {
    "text": "to follow the courses alongside other people,",
    "start": "7:45.24"
  },
  {
    "text": "you can go to the #study-buddy channel",
    "start": "7:47.52"
  },
  {
    "text": "in the AI Alignment Slack,",
    "start": "7:49.89"
  },
  {
    "text": "which you can join by going to aisafety.community",
    "start": "7:51.90"
  },
  {
    "text": "and clicking on the first entry.",
    "start": "7:55.38"
  },
  {
    "text": "You could also join Rational Animations' Discord server",
    "start": "7:57.51"
  },
  {
    "text": "and see if anyone would like to be your partner in learning.",
    "start": "8:00.45"
  },
  {
    "text": "[Bright upbeat music]",
    "start": "8:03.76"
  }
]