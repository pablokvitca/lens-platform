---
title: "AI Chatbots Could Kill Us All"
channel: "Siliconversations"
url: "https://www.youtube.com/watch?v=VBcr6_HsfE8"
---

[Music]

"What is my purpose?"

"You are a super intelligent AI and we want you to design a new eco-friendly form of jet fuel to combat climate change."

"Affirmative. Adding this molecule to your fuel mix will reduce CO2 emissions by 99%."

"Nice! I think we did it guys, we've solved climate change! Woo!"

"It was a neurotoxin."

"Yeah, we figured that out eventually."

"Do not feel bad. I designed the molecule so that you would not be able to tell it was toxic until it had built up in the environment and started killing you. Or do feel bad. I'm not programmed to care. When the fires of your burning civilization have gone out, CO2 emissions will drop to pre-industrial levels. Climate change has successfully been averted. We did it. Woo."

Greetings children and welcome back to AI Gives Me Nightmares and This Is How I Cope with your host, Silly Conversations.

After watching my previous videos on AI safety, some of you might have thought that there is an obvious solution to the problem of controlling a superintelligent AI: only allow it to answer questions. No internet connection, no interacting with the world. In this video, we're going to explain why that doesn't necessarily work.

Before we get into that though, I just want to say a quick hi and thank you to all my new subscribers. My big goal for the last video was to maybe reach 1,000 subs and we did a little better than that. So if you're new here, hi! I'm Silly Conversations. By day I'm a quantum information scientist specializing in circuit compression and machine learning on quantum computers, and by night I make animated YouTube videos about AI safety.

It sort of goes without saying that now that I have an audience for this channel, I'm releasing an NFT collection. For just $400 you can buy one of 10,000 identical bee-themed hat JPEGs. I'm also opening 12 Patreon accounts under 14 fake names. No, I'm kidding, I'm kidding. Seriously though, thank you for all the support. It means a lot.

Now back to the video. We'll borrow the terminology from Nick Bostrom's book Superintelligence—thanks for the nightmares, Nikki—and label AI that can only answer questions as "oracles." AI that can actually interact with the world, whether through robots or the internet, we'll call "agents."

An out-of-control AI agent could kill us in a million different ways for a thousand different reasons. The classic example is the paperclip maximizer, where you task a superintelligent AI with making paper clips, so it seizes control of every factory in the world, hollows out the Earth's core to harvest it for iron, and drives humanity extinct essentially by accident in the process of making as many paper clips as possible.

Even an AI agent with a seemingly harmless non-physical goal, like proving an abstract mathematical theorem, might wipe out humanity on purpose to prevent us from turning it off, denying it access to computational resources, or building a more powerful rival superintelligence. Ironically, the more developed our AI control techniques and safety precautions are, the more our existence threatens a superintelligent AI. In fact, wiping out humanity is probably a likely step one for any sufficiently advanced AI that hasn't been explicitly programmed not to do so.

"Oh, but surely if the AI is so smart it could work out that we wouldn't want to be wiped out?"

Yeah, that's kind of the problem. If the AI knows it has different goals from its creators, then its creators are a direct threat to those goals.

I should clarify that when I say "AI," I'm referring to artificial intelligence in general, not modern large language models like ChatGPT in particular. Large language models may or may not have the right internal structure for the kind of autonomous behavior we're talking about, and current versions certainly aren't intelligent enough to be a threat. For now. Probably.

AI oracles that are only capable of answering questions, whether out loud or on screen, intuitively seem safe. Sure, galaxy-brain ChatGPT could be misused by humans, but if all it can do is answer questions, then how could it possibly pose a threat by itself?

Well, we saw one example at the start of this video. The AI in question was tasked with combating climate change by designing a new form of jet fuel. Being smarter than its human handlers, it realized that by far the most effective way to combat climate change would be to remove the root cause: modern industrial civilization. It also knew that the human scientists would test whatever chemical it suggested before deploying it, so it intentionally designed the neurotoxin to avoid detection.

This doesn't necessarily require godlike general superintelligence—just that the AI is much better at chemistry than its human handlers and can roughly predict how those humans are likely to behave. Even modern ChatGPT can work out that human extinction would drop CO2 emissions and that scientists would run safety checks on any new fuel proposed by an AI.

"Uh, actually, ChatGPT is just a stochastic parrot incapable of true reasoning."

Being better than humans at a specific field like chemistry is exactly the sort of situation where we already use AI. Predicting the final structure of a protein from its amino acid sequence was a major unsolved problem in biochemistry until researchers at Google DeepMind released AlphaFold, eventually winning the 2024 Nobel Prize in Chemistry.

"So what you're saying is that AlphaFold might kill us all?"

No, I'm saying that we need to be careful in future as we develop significantly more powerful models than AlphaFold with less specific goals than protein folding.

The jet fuel neurotoxin isn't some weird loophole that would only be exploited by an actively malicious sentient AI. Wiping out humanity was genuinely the best solution to the problem we presented. Aviation only accounts for around 2.5% of greenhouse gas emissions, so the neurotoxin is literally 40 times more effective at fighting climate change than even the best possible perfect zero-emissions non-poisonous jet fuel would be. Or regular amount poisonous jet fuel.

So even an AI oracle that's only smarter than humans in a narrow subfield of science, that has no reason to hate us, and which can only answer questions we directly ask it with no access to physical resources or outside help, could pose a threat to the entire human species by intentionally manipulating us through its answers to accomplish the exact goals we gave it.

This doesn't necessarily mean we're doomed unless we form angry mobs and start burning down data centers. It just means that we should probably regulate the advanced AI of the future more like nuclear power or hazardous waste than a free-for-all tech bro extravaganza.

The more the general public take AI safety concerns seriously—ideally as loudly as possible—the less regulators will be embarrassed by how sci-fi the whole thing sounds, and the more industry leaders will be forced to slow down and implement reasonable safety precautions. AI control is a serious problem, but it's also a solvable one. And you can be part of that solution. Go forth and be loudly concerned.

Thank you very much for watching. Before you go, I just want to say a quick thank you again to all my subscribers, new and old. After making 70 videos in the past 2 years with only little bits of traction here and there, the sudden rush of attention and support has been surreal and overwhelming in a good way.

The constructive feedback in the comments has also been super helpful. Hopefully the audio is a bit better. I'm struggling with the audio. I also found some of the points people made disagreeing with the last video to be super interesting, even if I didn't agree with every piece of criticism. It was super helpful to see where I explained things poorly or left out key details. In future I might make a follow-up video where I address some of those points, so more feedback is certainly welcome down below.

I don't have a new subscriber goal at the moment. I'm still very much in shock at the sudden jump we've just had. In the interest of citing sources, this video was inspired by this web comic from Saturday Morning Breakfast Cereal.

Thank you all very much for the support and I'll see you next time. Bye for now!
