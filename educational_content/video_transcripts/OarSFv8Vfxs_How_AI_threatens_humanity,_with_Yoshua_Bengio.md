---
title: "How AI threatens humanity, with Yoshua Bengio"
channel: "Dr Waku"
url: "https://www.youtube.com/watch?v=OarSFv8Vfxs"
---

Hi everyone. We're on the brink of creating AI that greatly surpasses human intelligence, or so-called super intelligent AI - super intelligence for short. This breakthrough could solve humanity's most complex challenges, but it also brings significant risks. I'm Dr. Waku, and I've spoken with world-renowned AI expert Yoshua Bengio about the potential dangers ahead. Keep watching to learn more.

This video has three parts: The Super Intelligence Dilemma, On a Runaway Train, and Reorganizing Society.

Part One: The Super Intelligence Dilemma

Although it's been in the news a lot recently, AI development has actually been going on for a long time - many decades, in fact. Up until a few years ago, our AI systems used to be what we call narrow AI systems. They're called narrow because they can only solve a specific range of tasks, but they can still be very powerful. For example, narrow AI systems have beaten the world champions at chess and Go, and figured out how to fold proteins better than any biologist could have dreamed.

But it's really since ChatGPT was released in November 2022 that AI has really captured the public's imagination. That's because ChatGPT and other large language models are the first AI systems that are approaching general purpose reasoning capabilities. They can apply their intelligence to any domain you might ask about. The results might be good or really bad, but compared to a system that can only think about chess, it's undeniably impressive.

In fact, here's a quote from Yoshua Bengio: "And even though I see ChatGPT, I don't believe it. It must be cheating. I know, where's the human in the machine?"

Yes, this would be a good time for me to introduce Yoshua Bengio. I was extremely fortunate to be able to interview him about the future of AI. He is one of the three so-called "Godfathers of AI" because he won a Turing Award in 2018 for co-inventing deep learning. So basically, he co-invented neural networks, which are the technology that underlies pretty much all modern AI.

Yoshua Bengio has the highest H-index amongst all computer scientists, not just AI researchers. H-index is a measure of how impactful a scientist's work is, so you can see that Yoshua's papers speak for themselves. Apparently his H-index is third amongst all scientists in the entire world, so I guess there's still a bit of room for improvement. But needless to say, I am very lucky to have had the chance to speak with him. And when you hear him in this video, no, it's not an AI-generated voice, although I'm sure that's what some of you might be thinking.

The big question that everybody should be asking is: what's next for AI? Will AI development plateau, or will we continue to see massive improvements in a small number of months or years? How will these changes impact our world?

Unfortunately, by default, without a lot of intentional interventions by a lot of people, the answer is likely to be a bad one. It's likely to have a negative impact on our world. There are many, many risks posed by advanced AI, especially the most advanced form, which we call super intelligence.

For the general public, and even for many scientists, it's very hard to comprehend the scale of those changes - the scale of the problems and the impact this will have on all of our lives. And that's what I'm hoping to explain in this video.

So what exactly are the dangers? I'm going to talk about three really briefly: misuse by humanity, misalignment, and loss of control.

In terms of misuse by humanity, well, powerful AI is a force multiplier. A person can use it to achieve their ends more effectively. But if those ends have bad intentions, like crime or terrorism, then AI will make the situation much, much worse. For example, interfering with elections, or slandering competitors online by posting things about them that are not true, by posting deepfakes, or using AI to figure out the best way to build weapons or evade surveillance.

Next, there's misalignment, which basically means that human goals and AI goals are not aligned. That means that if the AI starts taking a lot of actions and humans don't notice for a while, we could end up where we don't want to be. And this can arise totally innocently by actually specifying instructions to an AI that aren't precise enough.

For example, let's say I create an AI and I give it the goal to prevent the ocean from filling up with plastic. So the AI starts investigating and contributing to recycling efforts - great. But now notice that the AI has an incentive to keep itself around so that it can continue monitoring the ocean for plastic. In other words, its terminal goal or final goal is to prevent the ocean from filling up with plastic, but now it has derived another goal along the way, which is called an instrumental goal. And this instrumental goal is: don't die, because then I can't prevent the ocean from filling up with plastic.

This type of self-preservation instrumental goal can be extremely dangerous, because now the AI will resist attempts to shut it down. It might even lash out or kill people that try to interfere with it. And all because the only things it cares about are its terminal goals.

The AI researcher Robert Miles gives some great examples about this. He describes a stamp collector super intelligence and all the ways it could go wrong. Quote: "Similarly, the things humans care about would seem stupid to the stamp collector because they result in so few stamps."

So one has to be very careful when specifying goals for an AI so that you don't get misalignment. But in the worst case, misalignment can cause loss of control, which is the third problem. Basically, as soon as you get self-preservation as a terminal goal, or even an instrumental goal, you're kind of out of luck. Although there are plenty of other ways that we could lose control of a super intelligence, of course. That's why advanced AI in general, from AGI to super intelligence, is considered to pose existential risk, or X-risk, to humanity.

Max Tegmark has called it a basic Darwinian error to introduce a species that is smarter than you are. Every time in history that this has happened, the less intelligent species gets wiped out.

Other sources are even more stark about the danger. Here's a quote from the Future of Life Institute: "Surveys suggest that the public is concerned about AI, especially when it threatens to replace human decision-making. However, public discourse rarely touches on the basic implication of super intelligence: if we create a class of agents that are more capable than humans in every way, humanity should expect to lose control by default."

And in the worst case, of course, this could lead to human extinction.

But the purpose of this video isn't to scare everybody. So what should you take away from this? What should the public really know about the development of advanced AI? Well, let's see what Yoshua had to say about this:

"So in my opinion, the most important game changer for future safety from potential catastrophic risks of AI is the public understanding that whoever is going to control superhuman AI will have enormous power and could abuse it at their expense. And also, like, lose control - and then everybody loses, including the person who did it. But in all of these cases, the public loses, right? And we are taking a lot of risks - new forms of weapons, or new forms of political control. Like, it's just the scope of possible catastrophic futures is terrifying. And the general public is always on the losing end here. And there are also scenarios where we all win, like AI is used for good, and I think we should strive for that. But there is no particularly strong reason to think that that kind of positive future is the one that's going to happen unless we steer things in that direction because of political pressure."

So basically, this is the situation: advanced AI could provide such amazing positive transformations that you can't fault anyone for wanting it. However, there are a lot of dangers that have to be carefully addressed as we build advanced AI. And yet we're currently racing ahead as quickly as possible and not allocating nearly enough resources to safety considerations. So there's a significant likelihood of disaster unless we make substantial technical and political changes. And of course, the first step in political change is spreading awareness about this issue amongst the public.

To elaborate, probably less than 1% of general resources are going towards AI safety research, and the other 99% to what we call capability research - just making the AI smarter. Companies actually say that they don't know how to control the systems they're building.

If you listen to Eric Schmidt, former CEO of Google, he advocates for just continuing forward until the system evolves so far that you have agents talking to each other in a language you don't understand, and then he says you should just stop. But how do you just stop? Who coordinates it? What do you do instead? How do you ensure stubborn actors don't just keep running the code that they had been running?

People like Yann LeCun, who is the director of AI research at Meta and therefore Facebook, don't seem to think there are any safety issues at all. And this is part of the reason why people aren't taking this seriously enough yet, because prominent figures seem to disagree.

But my take on it is that everyone admits that danger is a possibility. It's just that some people assign 0% likelihood to it, or some number very close to 0%, so they don't worry about it. And to be clear, it's not 100% certain that AI will kill us all. For instance, Yoshua put that probability at less than 50%. But he also added: even if it was only 5% likelihood, we would have to take this problem very seriously.

So why do people have such different estimates on this probability? That's because it's very hard for humans to forecast the future, because we just extrapolate our past. And we've never had a tool that makes us superhuman but that threatens to turn on us. It sounds more like the plot of a fantasy novel, in fact. So it's quite understandable, but don't dismiss these concerns just because some people assign 0% likelihood to it.

Part Two: On a Runaway Train

Let's start by talking about AGI, or Artificial General Intelligence. You can think of this as human-level AI, able to do just about any mental task that a human can do. This is the stated goal of many AI companies like OpenAI and Anthropic - to develop AGI. And the reason isn't hard to guess, because AGI grants incredible power. In fact, AGI alone has the power to completely upend our economic system.

"Once people realize that it is plausibly not science fiction, the race is on. I think the only reason why we didn't think of it like this before is because the vast majority of people, including researchers, thought that this was like science fiction, or like, you know, not in my lifetime."

Once you have AGI, you can actually ask it to perform AI research for you. And this is very interesting, because this AI scientist would work much faster than a human would, and it could run 24/7. Once you can get this working, you can simply run 100,000 replicas of it in an instant. So these automated AI researchers would quickly make scientific breakthroughs on how to create AI, which means in a very short time you would then have another AI system much more powerful than the last. And you can just repeat this and get another system, and then another system, and so on.

This is called iterative or recursive self-improvement, and it's the phenomenon behind what people call an intelligence explosion. Once you have a really powerful AI that is many, many times smarter than a human, then you might call that a super intelligence. Another term for this is ASI, or Artificial Super Intelligence. Definitions for super intelligence vary, but I like to think of it as something that would be smarter than hundreds or millions of humans put together, and can best any expert in their field.

Another name for this point where we have an intelligence explosion and then we develop super intelligence is the technological singularity, which is named after the mathematical concept of singularity, which is where a function goes to infinity. And also, I suppose, the physics singularity, where gravity is infinite and acceleration has exceeded the speed of light. In other words, a point of no return - and we don't know what would happen at that point.

You see, this is why it's hard to talk about this topic. It sounds incredible, but it's not science fiction. It's what people are seriously considering.

I want to give you more intuition for AGI and super intelligence. The main effect of AGI, I think, is an astronomical increase in productivity, because you can have digital workers that you can replicate at essentially no cost. Imagine getting new scientific advances seemingly every week, and doing things that would traditionally be reserved for alchemists, like reversing aging, granting immortality, etc.

At the same time, because our economic system relies on humans doing all the work right now, there would be a massive loss of jobs. It's hard to fathom exactly what type of system will replace that.

But I want to draw a parallel here. Imagine explaining to a farmer from 100 years ago that a huge proportion of the workforce just sits in one place all day on a chair. He might say, "How could you get any work done?" Yeah, and not only do they just sit in one place all day, but they just push about 100 different buttons, and they move around objects in careful patterns, and they make their living that way. Without an understanding of computers and the internet, you can't conceive of how this could be productive.

So basically, right now, we are the farmers from 100 years ago. We are saying, "But doesn't everybody have to go to work and do something that they might not want to do for 8 hours every day? How do you make a living otherwise?" And the person from the future is like, "Look, you have to change your whole mindset."

Now let's talk about super intelligence. What does super intelligence mean? Something that is far more intelligent than you, of course, but also something that can plan far better, something that can think strategically far better. There would be no use trying to outwit it - not by yourself, and not as a group of other humans.

A super intelligence that was free to act of its own accord wouldn't really be confined to one place. It would be taking actions all around you, all the time. It's still not quite the same thing as omniscience - after all, it could still perish along with all of us if a huge asteroid hits the Earth. But it could easily do things that seem like magic to us. After all, any sufficiently advanced form of technology is magic.

The example I hear a lot is: imagine explaining an airplane to someone from the Middle Ages - explaining how it works. They just lack the science, they lack a ton of background, and they simply wouldn't understand how such a thing is possible.

In one way, you could think of a super intelligence as something that exists even outside of governments. It could oppose an unjust war. It could mediate in geopolitical conflicts. But only, of course, if the super intelligence was free to act of its own accord.

So whether you think about AGI or super intelligence, the world looks really different from our current one in either case.

Unfortunately, there's a problem, which is the speed of our research. We're progressing through these AI research trajectories far too quickly. It's really unlikely that we'll be able to do enough safety evaluation and checking along the way at this rate.

Unfortunately, the companies that develop frontier models are under intense competitive pressures. There are companies like OpenAI, Anthropic, and Google DeepMind at the forefront of trying to develop better and better LLM technology. And there are plenty of other players as well, like xAI and Meta.

The company which develops the first highly capable frontier model that approaches AGI capabilities stands to make billions, or maybe even trillions of dollars. Their shareholders won't let the companies go anything but as fast as possible. And it doesn't leave much time or resources for investigating the safety of these things.

Companies aren't the only players. You also have governments. But unfortunately, as governments start to learn more and more about AI, they start to realize the immense military implications. And just like between companies, there's competition between governments and different countries as well. And there's also espionage, because if a country gets even a small military advantage, it could really upset the balance of geopolitical power.

So unfortunately, everybody's incentives are aligned to go as fast as possible. And that's why Yoshua says that we are on a train, and we can't stop. Here's a clip:

"In a way, I mean, that's one of the main reasons why we have to worry about super intelligence, is because we are on a train that's going there, and it's a very... there's very strong forces pushing that train towards super intelligence, right? And it's hard for any particular individual or government to decide - I mean, to do anything that can stop that train."

At first glance, you might think that developing AGI or super intelligence would surely take a really long time. And until recently, many researchers would have guessed like 100 years. But now we think that the total amount of time needed could be very short indeed. Yoshua gives the example of the massive improvements that happened between 2021 and 2023, which is a 2-year period. If we have a similar pace of advancement in the next 2 years, that might take us all the way to AGI. We simply don't know.

So basically, Yoshua is estimating that AGI could take between 2 years and several decades. In my own personal experience, most AI researchers that are thinking about safety tend to estimate about 3 to 5 years before AGI is developed, with reasonably high probability, and a long tail as well, of course, like Yoshua. Here's a clip:

"In your view, in like an ideal way to, you know, put the world on this path, what else needs to exist?"

"Well, clearly we're not ready. The institutions are not ready. The science of safety is not ready. So I'm very concerned. And even though I don't know - you know, maybe it's going to be decades - but it could be years. Honestly, I don't think that scientists have a way to answer that question in a very reliable way. If you just follow the trend of recent years, you might think that, you know, we're going to do a jump from the current state to - say, from 2021 to 2023 - in about the same number of years. And that could be it, right? We don't really know. That might be enough to kind of either be AGI or be effectively so powerful that we have to treat it that way."

"Yeah. A lot of it depends on whether the scaling is going to continue to work. Another part of it is, even if scaling alone is not sufficient, how many feasible, like easy-to-discover tweaks separate the not-so-smart-but-really-big AI that we may have in 3 years from now from something that's really like AGI, or eventually, you know, superhuman? It's impossible to answer these things. Like, it's like you can't anticipate scientific progress. And you can't anticipate if we're just like a little bit, you know, one little tweak away from something, or we're going to need a fundamental, like, rethinking of the whole thing. And that's why the range could be just a few years, or it could be decades, or even more, right?"

"From a public policy perspective, like what we should do together, we should worry about superhuman intelligence coming, you know, within the next few years. I think it's a lower probability event - like, it's not... it's less than 50%, much less probability event. But even if it was only 5% - that, say, by 2030 we get superhuman AI - that's way too quick and way too dangerous. And like, we need to prepare, make sure it doesn't happen in an uncontrolled way."

Part Three: Reorganizing Society

So enough doom and gloom. Let's talk about how to actually solve this problem.

Passing safety-related regulations can help, but it doesn't work on its own. Here's the basic problem: we know that systems like AGI are extremely powerful, and that there are bad people in the world. And if someone with bad intentions gets access to AGI and starts using it, they could cause terrible harm.

Of course, powerful technologies exist in our world today - some that are much too powerful for one individual to have access to. And our approach as a society so far has been to put laws into place that prohibit owning and using certain things, like nuclear tools or bioweapons, etc. If someone violates those laws, then the state will go after them in punishment and try to arrest them. This acts as a deterrent, but it doesn't prevent all crimes and all violations of the law from happening. Of course, there's always someone who thinks they can get away with it.

But unfortunately, we as a species may not survive the first misuse of AGI. So we have to prevent all misuses from ever occurring - hopefully while still ensuring that each human has their own free will, which might seem like a bit of a contradiction, but it is possible.

Of course, there are some simple solutions that come to mind. The first is to simply pause AI development. But as discussed earlier, this is not really feasible - we're on a train, and it's going fast. Remember that "Pause AI for 6 Months" letter from a few years ago? Wishful thinking, unfortunately.

Another simple solution is having surveillance on everybody - very high surveillance to make sure that they never misuse an AI system. But that sounds like a terrible idea as well.

If we had just one mind - if humanity was all a hive mind - we could just decide to stop research. But we don't have one mind. We have a distributed set of human minds, not all of whom are trustworthy. So we have to deal with human nature. And we only live on one planet, so we only get one shot at this.

Luckily, Yoshua Bengio has a proposition about this, published in the Journal of Democracy. In a sense, it involves creating a distributed world government. Here's a clip:

"A decentralized world government?"

"Yeah, right, because otherwise we're back to the same problem."

"Yeah, so a government... Yeah, we do need a world government. I mean, we've needed a world government to fix climate change, for example, for a long time. But a centralized world government is hugely dangerous. Like, you know, we elect a wannabe dictator, and we're all cooked. And so the democratic institutions that are needed to manage that level of centralization is something we haven't built yet."

So the idea is to set up democratic institutions that control the development of AI. These institutions have to be incremental, democratic, and involve scientific AI safety.

Imagine if you had, say, five or even 20 different institutions around the world that were all building their own AGI, except that everybody's computers are wired together so that any one of the institutions can press a button and everybody's computers blow up. So if anybody sees something dangerous, they can take action and shut down any rogue systems.

I dramatized this a bit here, but it is possible to do this with cryptography - not by actually blowing up the computers. You would probably want to take all of the companies that are currently doing AGI research and turn them into one of these institutions and subject them to the same "blowing up the computer" rules.

This is a great idea - probably very difficult to implement. But there is still one issue with it: you can still have a bad human actor that works at one of these organizations and takes the AGI system out into the world. So it seems to me like you would need heavy surveillance on the people involved with the project - not on the whole world, just on the people in the institutes. Here's a clip:

"If you think carefully, you don't need to surveil everybody. You only need to surveil... I need to be watched only to the extent that I have capabilities that are dangerous for the rest of humanity."

"Yeah, 99.99% of people don't need that."

"Knowledge, that's right. It's like, if I had a vial with a virus that could kill everyone - yeah, first, not everyone needs to have that, nor the means of like creating a new version that I can give to someone else, right? And yeah, maybe some people need to study this so we can build defenses, but it's a tiny, tiny minority."

"Yeah, that makes a lot of sense."

So even with all those defenses in place, there's still a chance that rogue organizations continue to do AI research on their own, especially as computing power starts to get stronger and stronger and cost less and less money. It might even be within the reach of a criminal organization to attempt the creation of an AGI system at some point in the future.

That's why controlling the supply chain is important, and a lot of people are already thinking about this. For example, you can put something inside each GPU to track where it is and who's running it. And if too many of them are focused on some unknown project, then you can investigate.

So as you can see, this plan involves a lot of different moving pieces. And actually, there's even more. Yoshua and I also talked about having defensive technologies in place. For example, having a local AI system that automatically checks all of your messages for misinformation. Or having all infrastructure-level code on the internet be written in a way that allows you to formally prove that there are no bugs in the code.

While possible, this is a huge ask, because there's so much deployed software out there already. But there's a pretty good chance that advanced AI would be great at generating provably secure code. And in case you're wondering, you don't have to blindly trust the AI on that front. It's much easier to check a proof than it is to actually generate one, such that you can do the checking with just a normal program - no AI involved.

Here's more from Yoshua:

"What about the issue that, at least on a cybersecurity perspective, an attacker generally needs like a lot less capability than a defender does?"

"Oh, that is with the current state of things, yes, where there are holes all over the place. There's a hole every 50 lines of code, on average."

"Okay, it's even worse than I thought."

"Yes. And so the defender needs to plug all the holes, which is never going to happen."

"Yeah, whereas the attacker just needs to find one."

"Yeah, but if we make progress on provably correct code, at least for some tasks like infrastructure management, then we could have the situation completely reversed, where the advantage is on the side of the defender. And basically the attacker can't use a direct cyber attack. It will have to be going through people, which is still a weakness. Of course, we would want to put multiple defensive technologies into place. And all the other work about governance and AI safety technical work - it's all additive. So we should be doing all of it as far as we can."

One thing that can really help the development of defensive technologies is to slow down the development of AI - proceed in small steps, one after the other, presumably in synchronization with all the other groups that are doing AGI development as well. This allows the development of defensive technology to catch up. Here's a clip:

"So there's a notion of defensive technologies, right? So let's say that we invent a way to write programs that are not hackable by construction - like, formally provably safe. I don't mean AI, just ordinary programs. Well, and it gets much harder for even a super intelligence to hack our infrastructure and all that, right? It gets much harder for an attacker - you know, criminal mind using superhuman AI. At least that channel gets now safer and harder to control by a single authority.

Another kind of non-centralized, potentially non-centralized defensive technology would be if we had like truth-saying AI - like Bayesian posteriors, right? - that is basically just oracles with some guardrails that are just available to everyone to check - fact checking and like truth checking and plausibility checking."

"So why is that... that's a defensive technology against disinformation?"

"Yes. And doesn't need to be centralized. Like, you'd have... you can all have our own, like, version or something, or whatever. It's accessible to anyone. And you can have multiple types, and, you know, you can choose which one you want, and so on. And it automatically applies to all the messages coming into your devices and stuff like that. And you can set the filters, so it seals off all the garbage."

"Yeah, exactly. So I think we're... if we're smart, there are probably ways to construct society to be more resilient to these power-grabbing scenarios."

"So you have to think about the most important attack vectors - like cyber attacks, or like biological warfare, or disinformation, political hijacking, or something - and then put defensive technologies in for each of those? Is that what you're thinking?"

"I mean, it kind of sounds tedious, but I'm not saying it's the only program. But there... I think there are things we can do, right? And we need to work on the prevention."

Finally, in conclusion: huge thanks to Yoshua Bengio for agreeing to be interviewed for my video, and thanks as well to the Future of Life Institute for inspiring me to create it.

We talked about some of the dangers that we'll run into as we develop advanced AI, including misuse by humanity, misalignment, and potential loss of control. It all sounds pretty scary, but the public should really know about this and be talking about this. Please share this information with somebody that you know, because for us to have a better chance at survival, we have to make significant technical and political changes. And political change doesn't happen without the public getting involved.

We talked about the definition of AGI, or human-level intelligence, and we also talked about how AGI would evolve into super intelligence, which is something much, much smarter than a human. And unfortunately, we're on a runaway train when it comes to AI development, and from there we will go headlong towards super intelligence.

To meet these challenges, we'll have to re-architect advanced AI in a sort of distributed fashion, with a lot of different entities together all trying to develop the smarter AI systems, with one important caveat: which is that anybody at any of these organizations can press a button and shut down everyone at once.

There are lots of other pieces to the puzzle as well, including developing defensive technologies to remedy some of the weak spots in our global infrastructure. That should make it harder for an AI system to cause terrible harm, or for an individual that's controlling an AI system to cause harm on their own.

Don't forget to like and subscribe, and because of the nature of this video, please share it with somebody else that you think would enjoy it. We also have a Discord for our community, so please join if you'd like to discuss these topics with other people and with me.

And if you liked this video, check out this previous one I made, which is about how to prepare as an individual for a post-AGI world.

Well, that's all I have for today. Thank you very much for watching. Bye.
