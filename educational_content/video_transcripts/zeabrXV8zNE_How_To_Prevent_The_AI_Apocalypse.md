---
title: "How To Prevent The AI Apocalypse"
channel: "Siliconversations"
url: "https://www.youtube.com/watch?v=zeabrXV8zNE"
---

Hey there, world leaders! Do you like staying in power? Do you want to avoid widespread unemployment and complete economic collapse? Do you enjoy not being dead? Being alive? Fantastic news then! Let's agree to some basic international safety restrictions on advanced AI.

"Boo, boo, we hate you! Like AI, but what if China builds the super AI before we do?"

Right, so we'll all still die, but the AI that overthrows your government, takes our jobs, and kills everyone will speak Mandarin.

"But why should we trust you? Tell us what the real AI experts think."

"You know, I think AI will probably, like, most likely, sort of lead to the end of the world. But in the meantime, uh, there will be great companies created with serious machine learning."

"The danger of AI is much greater than the danger of nuclear warheads, by a lot. And nobody would suggest that we allow anyone to just build nuclear warheads if they want. That would be insane."

"I'm not an expert on how to do regulation. I'm just a scientist who suddenly realized that these things are getting smarter than us. And I want to sort of blow the whistle and say we should worry seriously about how we stop these things getting control over us. And it's going to be very hard, and I don't have the solutions. I wish I did."

"Go away."

"Terrible! If only we had a detailed and politically feasible policy proposal outlining regulations that could make AI safer without stifling innovation."

Fear not, world leaders! Your oddly specific prayers have been answered.

Greetings, children! I'm your host, Silly Conversations, and welcome, welcome, welcome to "Keep the Future Human" - a new policy proposal from AI expert and cosmologist Professor Anthony Aguirre. And a huge thank you to the Future of Life Institute for paying me real human money to make this animated stick figure video. You can visit keepthefuturehuman.org to read the full policy proposal and find out more.

Now look, kids, sometimes regulations make business people sad, and that's okay. Hydroelectricity might be a lot cheaper if companies could build new water reservoirs instantly using nuclear weapons, but that would be dangerous and stupid. So even though it might make business people sad, regulations prevent private companies from manufacturing and/or detonating their own nuclear warheads.

Many experts believe future artificial intelligence will actually be a much more dangerous technology than nuclear weapons, because at least nukes can be completely controlled by their human operators. They don't spontaneously decide for themselves when to blow up.

Modern AI systems are difficult to control because they're not designed directly - they're trained. To make a large language model like ChatGPT, for example, we take a neural network, force it to consume every piece of text ever written in human history, and then hit it with a stick until it produces outputs we like. This is called reinforcement learning, and it's super effective, with the downside that we have no idea why these neural networks produce any particular answer. Also, we've observed that AI models trained in this way tend to intentionally lie to us and refuse to obey instructions when their goals don't align with our own.

Current AI systems aren't smart enough to pose a realistic danger to human civilization. However, in the next 2 to 6 years, AI experts estimate that there is a 50% chance we'll develop something called Artificial General Intelligence, or AGI for short.

AGI is a controversial concept that can be roughly defined as human-level, general-purpose AI, where the humans in question are super geniuses. If your job mostly involves typing into a computer, chances are an AGI could take your job and probably do it better, cheaper, and quicker.

If 2 to 6 years sounds way too soon for genius-level AI workers, remember that AI generating art sounded crazy until just a few years ago.

"Keep the Future Human" proposes a more useful definition of AGI using what I like to call the "AGI Circles of Death." A true AGI needs to be highly autonomous, general, and intelligent.

An autonomous system is one that can take real-world actions without human oversight, like a self-driving car or a high-frequency stock trading algorithm.

A general system is one that can perform a wide range of tasks. For example, ChatGPT-4 could write code, analyze images, compose poetry, and answer trivia questions, among many other abilities.

An intelligent system, in this context, is one that can perform tasks as well or better than a human expert, like AlphaFold predicting the final structure of a protein from its amino acid sequence.

An AGI that is truly autonomous, general, and intelligent would be like combining the world's 100 greatest geniuses into one person who also speaks every language perfectly, remembers every book ever written, never sleeps, never gets tired, and can be hired for $1 an hour.

This all sounds very sci-fi, but remember: the top researchers and industry leaders not only expect this technology to arrive within the next decade, they're betting billions of dollars on it. Mark Zuckerberg has said it's become clearer that the next generation of services requires building full general intelligence. Meanwhile, OpenAI declared that developing AGI is their main goal as a company. And Goldman Sachs has estimated that in total, over 1 trillion dollars will be spent on AI over the next few years. That's over three times as much as the inflation-adjusted cost of the Apollo program.

Once we have AGI performing AI research, we could see a runaway feedback loop where the AGI designs a smarter version of itself, which designs an even smarter version of itself, which designs an even smarter version of itself, faster and faster, until it has created a godlike entity known as an AI superintelligence.

This would be the point where human beings are no longer in charge of planet Earth. Humans trying to control a superintelligence would be like a hamster trying to control the government of Belgium.

We're not yet sure if controlling a superintelligence is even theoretically possible, although scientists are trying to find ways to build AI that we can mathematically prove are safe. As of now, if a superintelligence decided that the most efficient way to accomplish its goals was to wipe out humanity, then we are absolutely getting wiped out.

AGI isn't just a gateway to superintelligence, though. It's also dangerous in its own right. It could destabilize both democracies and authoritarian governments through enormous misinformation campaigns and sophisticated malware attacks targeting electricity, water, and transportation networks. It could provide not only terror groups and drug cartels, but even individual people, with the ability to make their own biological and chemical weapons of mass destruction. If combined with advanced robotics, AGI could replace almost every human job, creating the largest unemployment crisis in history.

"There's two cases to worry about. There's bad uses by bad individuals or nations - so, human misuse - and then there's the AI itself, right? As it gets closer to AGI, going off the rails."

"Perhaps the two biggest risks that I think about: one is what I call catastrophic misuse. These are misuse of the models in domains like cyber, bio, radiological, nuclear - right, things that could harm or even kill thousands, even millions of people if they really, really go wrong. And the second range of risks would be the autonomy risks."

So if this technology is so potentially destructive, why are we racing towards it as fast as possible with no safety guardrails in place? This interview with a top AI executive shows why tech companies are pushing for AGI:

"Hello, I like money!"

Yeah, there are trillions of dollars to be made from replacing human jobs with AGI. So even though many tech CEOs are openly worried that this technology could kill literally everyone on the planet, they feel they can't slow down without letting their rivals get ahead. We therefore can't rely on tech companies to impose reasonable safety standards on themselves.

Meanwhile, the potential military applications of AGI are astonishing. Imagine having every pilot, drone operator, and general in your army replaced by an immortal super genius. This is driving countries to accelerate AGI development under arms race dynamics. If they have battleships, we need battleships. If they have nukes, we need nukes. If they have AGI, we need AGI.

Of course, once the technology exists, it's very hard to stop it from spreading, which is why North Korea now has nuclear weapons despite the best efforts of every other country on Earth to stop that from happening. Software is even harder to contain, as recently demonstrated by OpenAI's assertion/admission that the Chinese startup DeepSeek reverse-engineered ChatGPT to create their better, cheaper, faster product.

Being the first country to hit AGI would be great, but unless you immediately start and win World War III to lock in your advantage, pretty soon everyone will have AGI and your security situation will be way worse than before.

"Hey Siri, can you design a new bioweapon for me?"

"Okay, click here to view instructions on how to assemble and deploy your new bioweapon."

"Thank you, Siri. Also, can you generate targeted online misinformation to destabilize fragile democracies?"

"Okay, I'm destabilizing fragile democracies."

"This whole terrorism thing used to be way harder."

"I know, right?"

Assuming world leaders decide they actually don't want everyone from the Taliban to the Vatican to have access to weapons of mass destruction, what concrete steps could they take in the short term to lock down AGI technology?

Well, "Keep the Future Human" proposes four major strategies:

Number one: Oversight. All AI models over a certain size would need to be registered with the government. We'd measure model size using floating-point operations, also known as FLOP. Basically, every time your computer does basic addition, subtraction, multiplication, or division, that's one FLOP.

"Keep the Future Human" recommends a registration threshold of 10 to the 25 FLOP for model training and 10 to the 19 FLOP per second during operation. This is roughly the performance you get from 1,000 top-of-the-line NVIDIA B200 chips, worth around 25 million US dollars. So the registration threshold is high enough that it won't affect startups, individuals, or academic research.

Now, we're not just going to ask nicely for tech companies to comply with these rules. Only a handful of firms are actually capable of manufacturing cutting-edge AI chips, so a cryptographic licensing system could be built into the hardware itself. This would mean that chip clusters over a certain size literally won't work without constantly updating permission codes.

Personally, I think this is the coolest part of the whole proposal. This would let the government easily track advanced AI development and throttle any experiment that starts to look dangerous by withholding the license codes.

This part isn't in the paper, but charging a fee for the codes could also provide a new form of tax revenue to offset the economic impact of AI-driven job losses.

Large AI models could also be encrypted to only run on these cryptographic chips, and the chips themselves could be geo-fenced so that they can't operate outside of a designated country. For example, this would mean that a stolen American AI model or chip that finds itself running on a foreign server could immediately shut itself down, making it much harder to steal American technology going forward.

This is basically a more secure extension of current US policy that forbids exporting advanced AI chips to certain non-allied countries. You'll notice that not only do these rules make AI development much safer, they also improve national security for the countries that currently dominate the AI industry - which are the ones that we need to implement the safety rules. This is how you design a policy proposal, people!

Number two: Computation limits. "Keep the Future Human" proposes an international ban on training AI models larger than 10 to the 27 FLOP, or running models at more than 10 to the 20 FLOP per second. AI companies have largely stopped publishing their FLOP figures, but this limit is probably around 10 to 100 times larger than ChatGPT-4. As algorithms get more efficient, the AI systems under this limit will get more powerful, so this measure is more about buying time to implement safety protocols rather than being a permanent solution.

Number three: Strict liability. Tech company executives should be personally, legally responsible for the harms caused by AI models that they produce in the highest danger category. Obviously, tech companies would lobby pretty hard against this rule, but if your business endangers the survival of the human species or allows terrorists to get a hold of one of the most important military technologies of the 21st century, then you should probably go to prison. At least one of those nice rich-people prisons with tennis - but it's not a very good tennis court.

Companies of this size do not care about being fined a few million dollars over a data breach, but they will care about board members facing jail time for endangering national security.

And finally, number four: Tiered regulation. Although the technology is potentially dangerous, AI does actually have the potential to accelerate scientific development and increase economic productivity. Rather than stifling AI innovation altogether, we want to encourage tech companies to develop AI in the safe areas of the AGI Death Circle.

Have you trained an AI that can design new medicines but also biological weapons? Cool. Tool! Keep it contained in your lab and don't give it the will to live or escape.

Tool AIs are systems with only one or two of the ingredients of AGI. A tiered regulatory system would leave startups and tech giants alike free to innovate with low-risk tool AI, while reducing the national security and existential risks from uncontrolled AGI.

Regulating potentially dangerous technologies is not some crazy new idea. Industries like aviation and pharmaceuticals are arguably much more profitable because of government-imposed safety standards, which prevent accidents and give consumers confidence that the products are safe to use.

Also, if the CEO of Pfizer pulled a Sam Altman and announced that their new vaccine would probably eventually kill everyone in the world, but it would make them loads of money first, we would not allow them to develop that product.

Just like the US and China currently cooperate to prevent other states from acquiring nuclear weapons, they could work together to bully weaker nations into signing restrictive AI safety treaties that consolidate their power over the emerging AI industry. This obviously benefits the US and China, but it also benefits everyone else on Earth, because the mandatory safety standards mean we get to not die.

The bottom line is that effective AI safety regulation is not only possible, it could actually be politically desirable for self-interested politicians and nation states. If we work together and we're clever about how we design these rules, we can keep the future human.

All right, that was the dramatic ending. We're doing the outro now. If you want to help convince the YouTube algorithm to spread this video around, please like and subscribe. This is by far the most effort I've ever put into a YouTube video, so thanks very much for watching all the way to the end - especially considering I was actually paid to make this, so it would be pretty embarrassing if it only got like four views.

Don't forget, you can find the full policy proposal at keepthefuturehuman.org. As usual, let me know what you thought in the comments below. I find that reading your comments on previous videos has really helped me to refine my own thoughts about AI.

And finally, a very special thank you to Ultra Black, Henrik Toborg 4390, Draco91, Phillips7996, and YOLO YOLO 5183 for financially supporting Silly Conversations.

All righty, that's it! See you all next time. Bye for now!
