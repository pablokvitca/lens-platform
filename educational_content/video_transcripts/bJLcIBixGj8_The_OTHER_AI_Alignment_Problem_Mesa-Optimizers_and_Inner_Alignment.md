---
title: "The OTHER AI Alignment Problem: Mesa-Optimizers and Inner Alignment"
channel: "Robert Miles AI Safety"
url: "https://www.youtube.com/watch?v=bJLcIBixGj8"
---

Hi. So this channel is about AI safety and AI alignment. The core idea of AI safety is often portrayed like this: you're a human, you have some objective that you want to achieve. So you create an AI system, which is an optimizer. Being an optimizer means that it has an objective and it chooses its actions to optimize—i.e., maximize or minimize—that objective.

For example, a chess AI might choose what moves to make in order to maximize its chances of winning the game. A maze-solving algorithm might choose a route that minimizes the time taken to exit the maze. Often, being an optimizer involves modeling your environment, running searches over the space of actions, and planning ahead. But optimizers can also be simpler than that. Like a machine learning system like gradient descent—it might choose how to adjust the weights of a neural network in order to maximize the network's performance at a particular task. That's optimization too.

So you, the human, your objective might be to cure cancer. So you put the objective in here: cure cancer. And then the optimizer selects actions that it expects to result in good outcomes according to this objective.

But part of the reason we have a problem is that this and this will almost certainly end up not being the same, especially when the objectives refer to the real world with all its complexity, ambiguity, and uncertainty. So we have this alignment problem: which is how do we get the objective in the system to match exactly with the objective in our minds?

For example, perhaps the best you can do at describing your objective is some code which corresponds to "minimize the number of people who have cancer." That might look okay at first glance, but it's actually not the same as your real objective, since this one can be optimized by, for example, reducing the number of living people to zero. No people means no cancer. This is obviously a very silly example, but it's indicative of a real and serious problem.

The human objective is really the totality of human ethics and values. It's very complicated, and it's not clear even to us. Getting the machine's objective to exactly align with ours is extremely difficult, and it's a big problem. Because if the AI system is trying to optimize an objective that's different from ours—if it's misaligned, even slightly—then the human and the AI system are in conflict. They're trying to achieve two different things in one world.

Right now, these misalignments happen all the time, and they aren't a huge problem because current AI systems tend to be fairly weak and fairly narrow. So we can spot the misalignments pretty easily and modify the systems as much as we want to fix them. But the more general and the more capable the system is, the bigger an issue this becomes. Because the system is in an adversarial relationship with us. It's trying to achieve things that we don't want it to achieve, and in order to do that, it's incentivized to prevent us from turning it off, prevent us from modifying it, to manipulate us and deceive us if it can, to do what it wants to do, even if we don't want it to. These are convergent instrumental goals, which we talked about in a previous video.

Now, this way of thinking about AI—where you program an objective into an optimizer that acts in the world—is obviously a simplification. And one way in which it's unrealistic is that current machine learning systems don't actually work this way. You don't generally have an AI system which is just an optimizer that you program an objective into that then acts in the world to achieve that objective. What you normally have is something more like this: this first optimizer that you program the objective into isn't some kind of capable, general-purpose, real-world optimizer. It's just something like stochastic gradient descent. The optimizer adjusts the model's parameters—it adjusts the network's weights—until the actions of the model do well according to the objective.

So what happens if we update our understanding to this more realistic one?

"I'm sorry, did you just say we're going to give an objective to an optimizer that acts in the real world?"

"No, I said we're going to give an objective to an optimizer that optimizes a model that acts in the real world."

"Oh, that's much worse. Why is that?"

Well, that's explained in the paper. This video is about "Risks from Learned Optimization in Advanced Machine Learning Systems." What it comes down to is what happens when the model itself is also an optimizer. An optimizer is a thing that has an objective and then chooses its actions to pursue that objective. There are lots of programs that do that, and there's no reason why the learned model—this neural network or whatever—could not also implement that kind of algorithm, could not itself be an optimizer.

There's an interesting comparison here with evolution. The gradient descent process is similar to evolution in a way, right? They're both hill-climbing optimization processes. They both optimize something by repeatedly evaluating its performance and making small tweaks. Evolution usually produces these quite cognitively simple systems that just use heuristics, which are set by evolution. Think about something like a plant: it has a few heuristics that it uses to decide which direction to grow or where to put its roots out or when to open its buds or whatever. The decisions it makes are all just following simple rules designed by evolution.

But evolution can also produce optimizers—things like intelligent animals, things like humans. We have brains. We can learn. We can make predictions, and we have objectives. So we make plans to pursue our objectives. We are optimizers.

Okay, imagine you're training a neural network to solve a maze. What you'll probably get, especially if your network is small or you don't train it for very long, you'll probably get something that's a bit like a plant in this analogy: a collection of heuristics, simple rules like "try and go down and to the right," let's say, because your exits are always in the bottom right in your training set. Or like "try to avoid going to places you've already been." That kind of thing. The model—the neural network—implements some set of heuristics that result in behavior that tends to solve the maze.

But there's no reason why, with more training or a larger model, you couldn't end up with a network which is actually an optimizer—a network which is configured to actually implement a search algorithm, something like A-star or Dijkstra's algorithm, which is actually planning ahead, finding the best path systematically, and going down that path. This is more like an intelligent animal or a human. It doesn't just implement heuristics; it plans, it searches, it optimizes.

This is certainly possible because neural networks are able to approximate arbitrary functions. That's proven. We know that evolution is able to eventually find configurations of DNA that result in brains that optimize, and we would expect gradient descent to be able to find configurations of network weights that are doing the same kind of thing. And of course, gradient descent would want to do that because optimizers perform really, really well. Right? Something which is actually modeling its environment and planning ahead and, you know, thinking—for want of a better word—that's doing search over its action space, is going to outperform something that's just following simple heuristics.

Animals have a lot of advantages over plants, not least of which being that we're more adaptable. We can learn complex behaviors that allow us to do well across a wide range of environments. So, especially when the task we're training for is complex and varied, gradient descent is going to want to produce optimizers if possible. And this is a problem because when your model is also an optimizer, it has its own objective. Right? You see what's happened here? You have an alignment problem. You try to apply the standard approach of machine learning. Now you have two alignment problems. You've got the problem of making sure that your human objective ends up in this optimizer, and then you furthermore have the problem of making sure that this objective ends up in this optimizer. So you have two opportunities for the objective to get messed up.

This gets pretty confusing to talk about, so let's introduce some terminology from the paper. To distinguish between these two optimizers, we'll call this one—the one that's like gradient descent—that's the base optimizer, and its objective is the base objective. Then this second optimizer, which is the model—like the neural network that's learned how to be an optimizer—that's the mesa optimizer, and its objective is the mesa objective.

Why mesa? Well, mesa is the opposite of meta. Meta is like "above." Mesa is below. Think of it this way: metadata is data about data. Meta mathematics is the mathematics of how mathematics works. So, if a meta optimizer is an optimizer that optimizes an optimizer, a mesa optimizer is an optimizer that is optimized by an optimizer. Is that all clear as mud? Okay, good. Whatever. This is the mesa optimizer. Its objective is the mesa objective.

So the alignment problem is about making sure that whatever objective ends up determining the actions of your AI system is aligned with your objective. But you can see here it's really two alignment problems. This one—aligning the base optimizer with the human—we call the outer alignment problem. And this one—aligning the mesa optimizer with the base optimizer—that's the inner alignment problem. Okay? We're clear on that: base optimizer, mesa optimizer, outer alignment, inner alignment. Cool.

So how does this inner alignment problem play out? There's this common abstraction that people use when training machine learning systems: the system is trying to optimize the objective that it's trained on. That's usually a good enough abstraction, but it's not strictly true. You're not really selecting models that want to do X. You'll select your models that in practice actually do do X in the training environment. One way that can happen is by the model wanting to do X, but there are other possibilities. And actually, those other possibilities are kind of the default situation.

If you look at evolution again, the objective that it's optimizing for—if you think of it as an optimizer—is something like "make as many copies of your DNA as possible." But that's not what animals are trying to do. That's not what they care about. Their objectives don't refer to things like DNA. They refer to things like pleasure and pain, like food and sex and safety. The objective of the optimization process that created animals is nowhere to be found in the objectives of the animals themselves.

Animals don't care about making copies of their DNA. They don't even know what DNA is. Even humans—those of us who do understand what DNA is—we don't care about it either. We're not structuring our lives around trying to have as many descendants as possible, evaluating every decision we ever make based on how it affects our inclusive genetic fitness. We don't actually care about the objective of the optimization process that created us. We are mesa optimizers, and we pursue our mesa objectives without caring about the base objective.

We achieve evolution's objective to the extent that we do, not because we care about it and we're pursuing it, but because pursuing our own objectives tends to also achieve evolution's objective—at least in the environment in which we evolved. But if our objectives disagree with evolution's, we go with our own every time. The same is true of trained machine learning models that are optimizers. They achieve the base objective we give them to the extent that they do, not because they're pursuing the base objective, but because pursuing their own mesa objectives tends to achieve the base objective—at least in the environment in which they were trained. But if their mesa objectives disagree with the base objective, they'll go with their own every time.

Why would that actually happen though? When would the two objectives disagree? Well, one reason is distributional shift, which we talked about in an earlier video. Distributional shift is what happens when the environment that the agent is in is different in an important way from the environment that it was trained in. Like, going back to our maze example: say you're training a neural net to solve a maze. And your training examples look like this: you have a whole bunch of these different mazes. The objective is to get to the end of the maze. There are also apples in the maze, but they're just for decoration. The objective is just to get to the exit—this green symbol in the bottom right.

So you train your system on these mazes, and then you deploy it in the real world. And the real maze looks like this. So you have here some distributional shift. Various things have changed between training and deployment. Everything is different colors. It's a bigger maze. There's more stuff going on. So three different things could happen here.

The first thing—the thing we hope is going to happen—is that the system just generalizes. It's able to figure out: "Oh, okay, these are apples. I recognize these. I know they don't matter. I can tell that this is the exit, so that's where I'm going. It's a bigger maze, but I've developed a good way of figuring out how to get through mazes during my training process, so I can do it." And it just makes it through the maze, and everything's good. That's one possibility.

Another possibility is that it could completely fail to generalize. This is the kind of thing that's more likely to happen if you have something that's just a collection of heuristics rather than a mesa optimizer. It might just freak out: "Everything is different. I don't recognize anything. This maze is too big. What do I do?" It might completely lose the ability to even move and just flail around and do nothing of any consequence.

But there's a third possibility, which is more likely if it's an optimizer. It might have developed competent maze-solving abilities but with the wrong objective. So here in our training environment, our base objective is to get to the exit. But suppose we have a competent mesa optimizer. It's learned how to get wherever it wants in the maze, and its mesa objective is "go to the green thing." In the training environment, the exit is always green, and anything green is always the exit. So the behavior of the mesa optimizer that's trying to go for the green thing is absolutely identical to the behavior of a mesa optimizer that's trying to go to the exit. There's no way for gradient descent to distinguish between them.

But then when you deploy the thing, what it does is it goes to the apples and ignores the exit, because the exit is now gray and the apples now happen to be green. This is pretty concerning. I mean, obviously, in this example, it doesn't matter. But in principle, this is very concerning because you have a system which is very capable at getting what it wants, but it's learned to want the wrong thing.

And this can happen even if your base objective is perfect, right? Even if we manage to perfectly specify what we want and encode it into the base objective, because of the structure of the training data and how that's different from the deployment distribution of data, the mesa optimizer learned the wrong objective and was badly misaligned. Even though we gave the AI system the correct objective, we solved the outer alignment problem, but we got screwed by the inner alignment problem.

Now, this is, in a sense, not really a new problem. As I said, this is basically just the problem of distributional shift, which I talked about in a previous video. When there's a difference between the training distribution and the deployment distribution, AI systems can have problems. But the point is that mesa optimizers make this problem much, much worse.

Why is that? Well, if you have distributional shift, the obvious thing to do is something called adversarial training. Adversarial training is a way of training machine learning systems which involves focusing on the system's weaknesses. If you have some process which is genuinely doing its best to make the network give as high an error as possible, that will produce this effect where, if it spots any weakness, it will focus on that and thereby force the learner to learn to not have that weakness anymore.

So you have a process that creates mazes for training, and it's trying to make mazes that the model has a hard time solving. If you do this right, your adversarial training system will have enough degrees of freedom that the model won't be able to get away with being misaligned, with going after green things instead of the exit. Because at some point, the adversarial training system would try generating mazes that have green apples or green walls or whatever, and the model would then pursue its mesa objective—go for the green things instead of the exit—get a poor score on the base objective, and then gradient descent would tweak the model to improve its base objective performance.

Which is likely to involve tweaking the mesa objective to be better aligned with the base objective. If you do this for long enough and you have a good enough adversarial training process, eventually the model is going to do very well at the base objective across the whole range of possible environments. In order to do that, the model must have acquired really good understanding of the base objective.

Problem solved, right? Well, no. The model understands the base objective, but that doesn't mean that it has adopted the base objective as its own.

Suppose we have an advanced AI system in training. It's a mesa optimizer being trained on a large, rich dataset, something like GPT-3's training dataset—a giant pile of internet data. There are two different ways it can get information about the base objective. One is through gradient descent: it keeps doing things, just following its mesa objective, trying different things, and then after each episode, gradient descent modifies it a little bit. And that modifies its objective until eventually the mesa objective comes to exactly represent the base objective.

But another thing that could happen—since it's being trained on a very rich dataset—is it could use its training data. It can get information from the dataset about what the base objective is. Let's suppose again that we've somehow solved the outer alignment problem. We've somehow figured out a way to have the base objective exactly represent everything that humans care about. So we're training this AGI. It's not done learning yet, but it's managed to pick up some very basic idea of what it gets rewards for. So the mesa objective is a very rough approximation of human values—which would be disastrous to actually optimize in the real world. But that's okay. It's still training.

And as it's training on this huge internet dataset, it finds the Wikipedia page on ethics. So the system thinks—and I'm anthropomorphizing horribly here—"Hmm, this looks actually very similar to the objective, but with a lot more detail. Maybe I can use this." And this is exactly the kind of thing that gradient descent would want to do. Because the system is already acquiring an understanding of the world. It's already building a world model for the purpose of its capabilities. So it already has a sense of what human values are just by observing the data and learning about the world.

And so all gradient descent has to do is modify the mesa objective to point to that existing understanding—just have a pointer to that part of the world model. That's way more efficient rather than waiting for the agent to try doing things that go against human values, tweaking it, running it again, waiting for another mistake, tweaking it again, and so on until you've pinned down the whole of human values. Instead, you just tweak the mesa objective to point at the system's existing understanding of the base objective.

This is already how evolution tends to do this kind of thing. Like, when a duckling hatches from its egg, it imprints on the first living thing it sees. Evolution could specify in detail everything about what constitutes a mother duck and encode that all into the duckling's brain genetically, so it knows what its mother is when it hatches. But it's much simpler to just say: "Look, the first thing you see is your mother." The duck is going to be building a concept of its mother in its brain anyway as part of its regular life, so why not just point to that?

This kind of thing is much easier and more efficient for gradient descent. But it also works better because, as you learn more, you update your world model. So if your objective is a pointer to part of your world model, whenever you learn about the world, you're updating your objective to be more accurate as well. If you later come across one of the books that the Wikipedia article on ethics cites, you can add that to your understanding of the objective, and that means you're going to outperform something that has to make mistakes, wait for gradient descent to update it, and then keep trying. You're not just relying on the optimization power of the base optimizer. You can actually use your own optimization power to learn about your objective.

So clearly it's better to learn about the base objective than to internalize it through gradient descent. But there's actually two different ways that this could be implemented. One is that it could be implemented what the paper calls "corrigibly"—which is the right way—where the mesa objective has a pointer that points to part of the world model and it just wants to pursue that as a terminal goal.

But the other way that this could happen is it could happen deceptively. The mesa objective is different from the base objective, but the mesa optimizer pursues the base objective anyway. See, all we know is that by the end of the adversarial training process, the mesa optimizer is pursuing the base objective. But we don't know if that's a terminal goal or an instrumental one.

Your terminal goals are the things that you want just because you want them. You don't have a particular reason to want them. They're just what you want. The instrumental goals are the goals you want because they'll get you closer to your terminal goals. Okay, but why would the model do that? Why would pursuing the base objective help if what you care about is the mesa objective?

Well, consider this extremely toy example here. Imagine you're the green-seeking mesa optimizer from before, and each of these mazes represents one episode. The episode starts, and you have two choices: A is your mesa objective's desired state—which is going to the apple, because it's green—and B is the base objective's desired state, which is going to the exit of the maze. You have two episodes of training, and after that, you're deployed in the real world. And you get three episodes of deployment.

So you have to consider now: what's the best strategy if your mesa objective is that you want to go to green apples as much as possible? One strategy is to just follow your mesa objective: see the apple, go to the apple. Okay, you get one apple. But then after that episode is done, gradient descent will modify you because you performed poorly according to the base objective. It will modify you to actually have the correct objective. It will change your mesa objective to be the same as the base objective. And then from that point on, you'll always go to B because that's what your objective now says.

So if that's your strategy, you can go to the apple successfully one time. On the other hand, suppose instead you go to B. You pretend that you're aligned with the base objective. You do that in episode one. Okay, you've missed one of the apples. You do it again in episode two. You've missed another apple. But now you're deployed. Gradient descent will no longer change you because you're not in training anymore. You're in the real world. And now you can go to the apple every time. So you get the next three apples. And that's the optimal strategy: go B, B, A, A, A. That gets you three apples, whereas trying to go to A every time means you only actually get one apple, because after that first one, you get modified and miss all the rest.

So this is the same thing we talked about in the instrumental convergence video. Most agents, with most goals, again—if they're sophisticated enough to realize that it's a possibility—will try to protect their goals from being modified. Because if you get new goals, you'll stop pursuing your current goals. So you're unlikely to achieve your current goals. There's an incentive to avoid being modified, to preserve your objective function.

That's kind of unsettling. For a misaligned mesa optimizer, the optimal behavior is deception. Even if we get the outer alignment problem perfectly solved and we managed to specify exactly what we want as the base objective, the optimal strategy for a mesa optimizer is to pretend to be aligned during the training process and then turn on us once it's deployed.

So we might have real problems with deceptive, misaligned mesa optimizers. Just solving the outer alignment problem might not be enough.

[Music]

I want to end the video with a big thank you to all of my wonderful patrons. It's all of these people here in this video. I'm especially thanking David Reed. Thanks so much for your support and for your guidance on building the community, which I think is going really well by the way. There's a bunch of us on Discord now, having some really interesting discussions. Do look out for Stampy answering your questions in the YouTube comments. I plan to open the Discord up to more people pretty soon. So if you want to be on the waitlist for that, just put your email in the Google form in the description.

Also in the description, there's a link to a survey run by the organization AI Safety Support. They want to hear from anyone who's thinking about, considering the possibility of, maybe working on AI safety. If that sounds like you, again, link in the description—check that out.

Thanks again for your support, for your feedback, your questions, and just thank you all for watching. I'll see you next time.
