# educational_content/lessons/narrative-test.yaml
# Based on the Introduction lesson content, converted to narrative format
slug: narrative-test
title: "Introduction to AI Safety"

sections:
  # Section 1: Opening video - A.I. Humanity's Final Invention
  - type: video
    source: video_transcripts/fa8k8IQ1_X0_A.I._‐_Humanity's_Final_Invention.md
    segments:
      - type: text
        content: |
          # Welcome to AI Safety

          We begin by examining the potential of AI and the risks and opportunities
          that the characteristics of this technology present to humanity.

          Watch this video from Kurzgesagt to understand why artificial intelligence
          might be humanity's most important invention.

      - type: video-excerpt
        from: "0:00"
        to: "5:00"

      - type: text
        content: |
          **Reflection:**

          The video describes how humans dominate Earth because of our general
          intelligence. It also explains the difference between narrow AI and AGI.

      - type: chat
        instructions: |
          TLDR of what the user just watched:
          Humans dominate Earth because general intelligence enabled cumulative knowledge.
          Modern AI evolved from narrow tools into opaque "black box" learning systems.
          AGI matters because digital minds can run faster, scale, and be copied—potentially
          outcompeting humans and concentrating power.

          Discussion topics to explore:
          - What is intelligence as "problem-solving ability" and why is it a source of power?
          - Why are neural networks called "black boxes"?
          - What's the difference between narrow AI (like ChatGPT) and AGI?

          Start by asking what stood out or surprised them. Use Socratic questioning to
          check their understanding of these concepts. Don't lecture—help them articulate
          their own thinking.
        showUserPreviousContent: true
        showTutorPreviousContent: true

      - type: video-excerpt
        from: "5:00"
        to: "10:00"

      - type: text
        content: |
          The video introduces the concept of an **intelligence explosion** - a rapid,
          recursive cycle of AI self-improvement that could outpace human oversight.

      - type: chat
        instructions: |
          The user just watched the second half of the video about AI risks and intelligence explosion.

          Discussion topics:
          - How could an "intelligence explosion" happen through recursive self-improvement?
          - Why might controlling superintelligent AI be difficult?
          - What does it mean that "the fate of humanity could depend on machine superintelligence"?

          Check if they understand why speed of improvement matters. Ask them to explain
          the intelligence explosion concept in their own words.
        showUserPreviousContent: true
        showTutorPreviousContent: true

  # Section 2: Wikipedia article on existential risk
  - type: article
    source: articles/wikipedia-existential-risk-from-ai.md
    segments:
      - type: text
        content: |
          # Understanding Existential Risk from AI

          Now let's read an overview of the main concepts regarding AI as a source
          of existential threat: what capabilities of this technology are considered
          most concerning, and why the task of eliminating AI risks differs from
          similar tasks for other technologies.

      - type: article-excerpt
        from: "**Existential risk from artificial intelligence**"
        to: "improve their fundamental architecture."

      - type: text
        content: |
          **Key concepts so far:**

          - AI x-risk refers to the possibility that AGI could cause human extinction
          - The "Gorilla Problem": just as gorillas depend on human goodwill, humans
            might depend on AI's goodwill
          - Many leading AI researchers take this risk seriously

      - type: chat
        instructions: |
          The user just read the introduction to AI existential risk from Wikipedia.

          Key concepts covered:
          - AI x-risk hypothesis
          - The gorilla analogy (Stuart Russell)
          - Expert surveys showing concern
          - The alignment problem

          Discussion topics:
          - What is the "Gorilla Problem" and why is it a useful analogy?
          - Why do many AI researchers believe there's a significant chance of catastrophe?
          - What are the two main sources of concern (control and alignment)?

          Ask what they found surprising or new. Check if they can explain the gorilla
          analogy in their own words—it's a key concept.
        showUserPreviousContent: true
        showTutorPreviousContent: true

      - type: article-excerpt
        from: "One of the earliest authors"
        to: "strong public buy-in."

      - type: text
        content: |
          This history shows that concerns about AI risk aren't new - thinkers from
          Alan Turing to Stephen Hawking have considered these questions.

      - type: article-excerpt
        from: "### General Intelligence"
        to: "evasion of human control"

      - type: text
        content: |
          **The path from AGI to superintelligence:**

          Notice Bostrom's list of advantages AI has over human brains: speed,
          scalability, duplicability. These aren't science fiction - they're
          inherent properties of software.

      - type: chat
        instructions: |
          The user just read about AGI and superintelligence capabilities.

          Key points:
          - AGI = human-level across most tasks
          - Superintelligence = vastly exceeds humans in all domains
          - AI advantages: speed, scalability, duplicability, editability
          - Timelines are uncertain but potentially soon

          Discussion topics:
          - Why might the transition from AGI to superintelligence be rapid?
          - Which of the AI advantages over human brains seems most significant?
          - How does duplicability change the dynamics of AI development?

          Help them understand why these technical properties have such large implications.
        showUserPreviousContent: true
        showTutorPreviousContent: true

  # Section 3: Summary
  - type: text
    content: |
      # Summary

      **Key takeaways from this lesson:**

      1. **Intelligence is power** - Humans dominate Earth because of general intelligence
      2. **AI is different** - Digital minds can run faster, scale, and be copied
      3. **The alignment problem** - Ensuring AI goals match human values is extremely difficult
      4. **Expert concern** - Many leading researchers believe AI poses existential risk
      5. **Timelines are uncertain** - AGI could arrive within years or decades

      In the next lesson, we'll explore common objections to AI safety concerns
      and how researchers respond to them.
